## 为什么要有多线程
尽管在平时的开发中，我们很少会直接编写多线程代码，但是，**常用的框架和容器无一例外的都会用到多线程**，比如，Dubbo、Tomcat均使用多线程来处理请求。业务代码一般运行在这些框架或容器中，因此，也就间接的会用到多线程。只有对多线程的使用和原理有透彻的了解，我们才能编写出线程安全且高性能的业务代码。

进程和线程并非从计算机诞生就存在。随着计算机硬件的发展，操作系统经历了单进程、多进程、多线程这几个阶段，才逐渐发展为现在的并发模型。
* 单进程
  * 进程是对程序运行过程中所涉及的数据（比如创建的对象、变量）、代码、资源（比如打开的文件）、执行信息（比如执行到哪行代码了）的封装，起到方便管理的作用。进程跟程序之间的关系，有点类似类跟对象之间关系。针对同一个类，JVM可以创建多个对象。同理，针对同一个程序，操作系统也可以创建多个进程。
  * 早期的计算机只支持单进程，一次只能执行一个程序。一个程序执行完之后，才会轮到下一个程序来执行。每个程序在执行的过程中，都独占计算机资源（比如CPU、内存）。早期的计算机主要用于执行涉及科学计算的批处理程序。这类程序有两大特点。其一：科学计算一般都是CPU密集型的，早期的计算机都是单核的，CPU资源是瓶颈，并发执行多个程序并不能提高吞吐量（吞吐量指单位时间内执行完的程序个数）。其二：对于批处理程序来说，我们一般只注重吞吐量，而不注重实时性。因此，对于早期计算机来说，这种单进程的运行模式就已经足够满足开发需求了。
* 多进程
  * 随着计算机的硬件的发展，特别是随着个人计算机的发展和推广，计算机的应用越来越丰富，不再只是用于工业级的科学计算。对于个人计算机来说，因其应用的特殊性（比如听歌的同时打字），人们对实时性的要求变的更高。
  * 因为限于计算机的发展，早期的计算机都是单核的，所以，计算机内部采用“并发”的方式，来实现用户眼里的“并行”需求。也就是说，在粗时间粒度上，两个程序看似并行运行。在细时间粒度上，两个程序实则交替执行。
  * 一个进程在执行的过程中，会被频繁的暂停执行和重启执行。暂停执行时，操作系统需要帮忙记录下这个进程暂停时的环境信息。重启执行时，操作系统需要恢复这个进程执行的环境信息。我们把这里的环境信息叫做上下文（Context）。两个进程之间切换执行，就会导致上下文切换（Context Switching）。
  * 操作系统用进程表（Process Table）来记录所有进程的执行信息。进程表中每一个表项叫做进程控制块，简称PCB（Process Control Block）。一个PCB对应记录一个进程的执行信息。
* 多线程
  * 多进程已经很好地满足了多个程序并发执行的需求。线程的引入完全是在设计、性能、易用性上的进一步优化。
  * **在设计方面，**线程的引入相当于对进程所负责的功能做了拆分。拆分之后，进程只负责线程共享资源的管理（比如虚拟内存中的代码段、数据段，以及打开的文件等）。线程负责代码的执行。原来由进程负责的部分数据现在由线程负责，比如栈（也就是函数调用栈）、程序计数器、寄存器值。
  * 在引入线程前，操作系统按照进程来分配CPU执行时间。在引入线程后，操作系统按照线程来分配CPU执行时间。因此，进程切换替代为了线程切换。当然，线程的切换跟进程切换一样，也会导致上下文的切换。
  * **在性能方面，**随着多核计算机的发展，多线程可以让一个程序并行运行在多个CPU上，单个程序执行的效率可以更高。实际上，程序中的逻辑可以粗略的分为两类：需要CPU参与执行的逻辑和不需要CPU参与执行的逻辑（比如读写IO）。实际上，提高程序执行效率的关键，是让两类逻辑并行执行（注意这里是并行而非并发），也就是在IO读写的同时，CPU也在执行指令。
  * 如果操作系统只支持多进程，那么，只有两个程序之间有可能并行执行，程序内部包含的两类逻辑之间并不能并行执行。在引入多线程之后，不仅程序间可以并行执行，程序内也可以并行执行。也就是说，多进程相当于粗粒度的并行，多线程相当于细粒度的并行。
  * **在易用性方面，**引入多线程之后，每个线程负责执行一个逻辑，多个逻辑之间的调度执行，由操作系统来完成。如果没有多线程，那么，多个逻辑之间的调度执行，需要程序员自己去维护。因此，引入多线程之后，程序的开发难度降低。

PCB中包含的信息具体如下所示
* 进程ID：每个进程会有一个ID。
* 进程状态：NEW、READY、RUNNING、WAITING、TERMINATED等。
* 程序计数器（Program Counter），也叫做PC计数器：用来记录接下来要执行的代码所在的内存地址。实际上，PC计数器中保存的值就是CS和IP这两个寄存器的值。
* 寄存器值：CPU在执行过程中会使用寄存器来暂存计算结果，因此，在暂停进程时，操作系统需要将当下各个寄存器的值保存在PCB中，以便恢复执行时恢复寄存器的值。
* 调度信息：比如进程的优先级等。
* 文件列表：记录已经打开的文件的信息，比如读取到哪个文件的哪个位置了。
* 其他信息，比如统计信息（进程运行了多长时间了之类信息）。

调度策略
* 对于支持多线程的操作系统，多个线程共同竞争CPU资源，因此，操作系统就必须设计一定的算法，来调度多个线程轮流执行。基础的调度算法有很多种，比如先来先服务、最短作业优先、高优先级优先、多优先级队列、轮转调度（Round Robin）等等。操作系统使用的调度算法一般会比较复杂，特别是针对多核计算机，往往会组合各种基础调度算法。
* 轮转调度算法的原理非常简单。所有就绪线程（状态为READY，待会会讲到线程状态）会放入到一个队列中，操作系统每次从队首取一个线程，分配时间片执行此线程，当时间片用完之后，将这个线程暂停，放入队列的尾部，然后从队首再取新的线程继续执行，以此类推。当然，除了时间片用完之外，还存在其他情况也会导致线程切换，比如线程等待I/O读写完成、线程等待获取锁，以及线程主动让出时间片（比如调用Java中的yield()函数）。
* 因为操作系统中的调度算法比较复杂，所以，时间片的大小一般并不是简单固定的。时间片的大小一般处于10ms~100ms这个量级范围，不能太大也不能太小。时间片太大的话，其他线程等待被执行的时间就会过长。时间片太小的话，线程上下文切换耗时跟时间片相当（线程上下文切换的耗时在几ms的量级范围），CPU还没执行几行代码就要进行线程切换，大部分时间都浪费在线程切换上了。
* 操作系统一般会保证一个固定的时间间隔内，所有的就绪线程都要运行一遍，这样才能保证每个线程都不会等待太久。也就是说，如果线程个数较多，那么时间片就相对短一些，如果线程个数较少，那么时间片就会相对长一些。我们经常听说，对于CPU密集的程序来说（并不存在太多IO读写和CPU执行指令并行进行），创建的线程过多会导致程序变慢，原因就是线程多导致时间片短，上下文切换耗时占比增多，从而影响程序的执行效率。
* 除此之外，我们还经常听说，频繁加锁和释放锁也会导致程序变慢。那么，变慢的其中一个原因是加锁和释放锁本身就耗时，变慢的其中另一个原因是加锁会导致其他线程请求锁阻塞，在没有用完时间片的情况下，就切换为其他线程执行。执行代码逻辑的时间减少，大部分时间都浪费在了上下文切换，程序也会变慢。

线程状态
* NEW：创建的线程，在没有调用start()函数前，线程处于NEW状态。
* READY：线程一切就绪，等待操作系统调度。
* RUNNING：线程正在执行
* WAITING：线程在等待I/O读写完成、等待获取锁、等待时钟定时到期（调用sleep()函数）等等，总之，等待其他事件发生之后，线程才能被调度使用CPU，此时，线程的状态就是WAITING状态。
* TERMINATED：线程终止状态。线程终止之后，未必就立即销毁。有些操作系统为了节省线程创建的时间（毕竟要分配内存还得初始化一些变量），会复用处于TERMINATED状态的线程。

## 线程模型
线程模型概述
* 内核线程
  * 由操作系统内核负责多线程调度
  * 因为应用程序运行在用户空间，无法直接操作（创建、使用、销毁等）内核线程，所以，操作系统需要暴露操作线程的系统调用给应用程序使用。因为系统调用比较底层，所以，大部分编程语言都会对其进行了封装，提供更加易用的接口，比如Linux中的pthread、C++中的std::thread等等都是对操作系统系统调用的封装。对于Java这种跨平台的语言来说，为了提供统一的线程操作接口，也必然会将操作系统提供的系统调用，封装为自己的线程类库。
  * 内核线程，也叫做1:1模型。前面的1表示用户空间的一个线程，也就是在应用程序开发者眼里的一个线程，比如通过Java Thread创建的一个线程对象。后面的1表示内核空间的一个线程，也就是真正的线程。1:1模型指的就是一个应用程序中的线程对应一个内核线程。
* 用户线程
  * 我们发现，应用程序需要通过系统调用，才能实现对内核线程的操作，而系统调用又会导致用户态和内核态的上下文切换，因此，上下文切换导致的性能损失是内核线程的一个弊端。为了解决内核线程的弊端，于是，计算机科学家们就发明了用户线程。
  * 类比内核线程，用户线程指的是线程的调度由虚拟机来完成的线程模型，因为虚拟机本质上就是一个应用程序，它运行在用户空间，所以，用户线程也叫做用户空间线程或用户态线程。实际上，我们常听到的协程，也就是用户线程。
  * 实现调度算法来调度线程的程序，我们叫做调度程序。用户线程的调度程序的实现思路，跟内核线程的调度程序的实现思路，基本上是一致的。虚拟机中内嵌一个调度程序。如果运行在虚拟机上的应用程序创建了3个用户线程，那么，当虚拟机获得CPU时间片之后，它会将这个时间片分为3个更小的时间片，分别运行这三个用户线程对应的代码。当然，这只是其中调度算法的一种实现思路。调度算法的实现思路也有可能是，当虚拟机获得时间片之后，它把这个时间片全部用来执行一个用户线程的代码，等到虚拟机再次获得CPU时间片之后，它再把这个时间片全部用来执行另一个用户线程的代码。
  * 实际上，用户线程只是一个华丽的外壳，抛开外壳，从本质上来看，虚拟执行三个线程就相当于轮询执行三段代码（每个线程对应一段代码）。因此，应用程序操作用户线程（创建、使用、销毁等），都是在用户空间完成的，完全不需要操作系统内核的参与。这样就避免了系统调用带来的用户态和内核态的上下文切换。
  * 不过，用户线程也需要有专门的结构来记录上下文信息。虚拟机在执行某个用户线程对应的代码时，如果分配给这个用户线程的时间片用完了，那么，虚拟机就需要保存这个用户线程的上下文，以便之后再次获得时间片之后重新继续执行。除此之外，虚拟机也需要为每个用户线程维护独立的栈。
  * 用户线程也叫做M:1线程模型。其中，M表示M个用户线程，1表示1个内核线程。我们知道，虚拟机本质上也是程序，在运行时，操作系统会为其创建进程，并且是单线程（这里的线程指的是内核线程）的进程。
  * 操作系统的线程调度算法调度的对象是内核线程，为内核线程公平地分配时间片。不管虚拟机中创建多少个用户线程，它们都只能共享一个内核线程的CPU时间片。因此，用户线程无法利用多核优势。即便一台计算机上只运行一个虚拟机，虚拟机上的多个用户线程也只能排队使用一个CPU资源，其他CPU资源都在白白浪费。这就是用户线程相对于内核线程的弊端。
  * 除此之外，用户线程在使用上还有另外的限制。在用户线程中，我们无法使用阻塞模式的系统调用，比如read()、write()等阻塞I/O系统调用。在内核线程中，当我们调用read()、write()等阻塞I/O系统调用时，操作系统会让当前线程让出时间片，切换为其他线程执行。对于用户线程来说，当一个用户线程中的代码调用了read()、write()等阻塞I/O系统调用时，对应的内核线程，就会被操作操作系统调度让出时间片，直到I/O读写完成才会放入就绪队列，重新排队等待分配时间片。也就是说，只要有一个用户线程阻塞了，其他用户线程也无法工作了。
  * 解决这个问题的办法是在用户线程中不要使用阻塞模式的系统调用。比如读写I/O，我们可以使用非阻塞的read()、write()系统调用。操作系统一般都会提供这类非阻塞的I/O系统调用，内核线程在执行这类非阻塞的I/O系统调用时，不需要让出时间片，可以继续执行后续的代码。
* 混合线程
  * 我们发现，用户线程虽然可以避免使用内核线程导致的内核态和用户态的上下文切换，但是，**用户线程也存在弊端，比如，一个进程内的用户线程无法利用多核并行运行，以及一个用户线程调用阻塞系统调用会阻塞一个进程中的所有用户线程**。为了解决这些问题，于是，计算机科学家又发明了混合线程。混合线程又叫做M:N线程模型。
  * M:N线程模型表示一个进程中的M个用户线程对应N个内核线程，M一般大于N。如果M等于N，那么，M:N线程模型就退化成了1:1线程模型。如果M小于N，那么，多余的内核线程会浪费掉。如果应用程序创建M个用户线程，那么，虚拟机就会使用操作系统提供的系统调用，创建N个内核线程来服务这M个用户线程。M个用户线程并不会绑定在同一个内核线程上，因此，一个用户线程阻塞并不会导致所有用户线程都阻塞。同时，M个用户线程分散在N个内核线程上，不同的用户线程可以分散在不同的CPU上执行，也就利用到了计算机多核的优势。相对于1:1线程模型和M:1线程模型，M:N线程模型实现起来比较复杂。

JDK 线程实现原理
* Green Thread：实际上，Green Thread就是用户线程模型，也就是M:1线程模型。之所以叫Green Thread，是因为开发这个项目的团队名称叫Green Team，因此，他们把开发的线程库命名为Green Thread。实际上，Green Thread只存在于早期JDK版本中（JDK1.1和JDK1.2），在JDK1.3中便已经废弃，取而代之是Native Thread。
* Native Thread：Native Thread实际上就是内核线程模型，也就是1:1线程模型。Java提供的线程库，只不过是对操作系统提供的操作内核线程的系统调用的二次封装。线程的调度由操作系统来完成，因此，Java线程库的底层实现非常简单。

操作系统中基本的线程状态有5个，分别是：NEW、READY、RUNNING、WAITING、TERMINATED。而Java中定义了6个线程状态，分别是：NEW、RUNNABLE、WAITING、TIMED_WAITING、BLOCKED、TERMINATED。

除此之外，Java线程中定义了10个级别的线程优先级（从1到10）。跟线程状态类似，Java定义的线程优先级跟操作系统定义的线程优先级也并非一一对应，例如，Linux优先级有140个，Window线程优先级有7个。

## JMM
但凡讲到多线程，我们就不得不讲一下Java内存模型。Java内存模型用来解决多线程的三大问题：**可见性问题、有序性问题、原子性问题**。

CPU 缓存导致可见性问题
* CPU执行指令的过程，会涉及大量的内存读写。内存读写太慢，会影响到CPU执行指令的效率。
* 缓存是提高数据读写速度的常用手段，比如，基于磁盘的数据库的读写速度比较慢，我们可以前置一个基于内存的缓存（比如Redis）。计算机为了提高内存读写速度，一般在CPU和内存之间安置一个高速的CPU缓存。

有同学可能会说，即便引入了CPU缓存，但数据还是得从内存中读取，最终还是得写入内存，缓存横插在CPU和内存之间，岂不是多此一举？实际上，缓存起效的条件是数据的访问满足局部原理。其中，局部性原理包含时间局部性原理和空间局部性原理。
* 时间局部性原理：基于时间局部性原理，某个数据一般在一个时间段内会被反复读写，CPU将数据从内存加载到缓存之后，之后的数据读写都只需要在高速缓存中完成，最后再更新到内存中，由此减少了内存读写的次数。
* 空间局部性原理：基于空间局部性原理，内存中相邻的数据一般会紧挨着被访问。因此，CPU从内存中读取数据时， 会一次性读取一个缓存行（cache line）大小的数据。一般来讲，一个缓存行大小为64字节或128字节。

在单线程或者单CPU多线程情况下，这样的内存访问模型（CPU通过缓存来读写内存）不存在任何问题，但是在多CPU多线程情况下，它就有可能会导致多个CPU缓存之间的数据一致性问题。例如，CPU1和CPU2读取内存中的数据a=1到各自的缓存中。CPU1将数据a的值更新为2，更新之后的a值不会立刻写入内存，毕竟，如果每次更新缓存都要同步更新内存，那么缓存就没有存在的意义了。此时，CPU1的缓存和CPU2的缓存中数据值就不一致了。从另一个角度来看，也就相当于，CPU1对共享数据的更新对CPU2不可见，因此，这个问题也叫做可见性问题。

指令重排导致有序性问题：刚刚我们讲了CPU缓存，尽管它可以有效提高指令的执行效率，但是，也带了多线程的可见性问题。接下来，我们再来讲另外一个提高指令执行效率的方法：指令重排序，不过，它也会导致多线程情况下的代码执行问题：有序性问题。

实际上，在代码的编译执行过程中，会发生3种不同类型的重排序：
* 编译优化导致的重排序
* CPU 指令并行执行导致的重排序
  * 为了提高指令的执行效率，现在的处理器大都采用指令级并行技术（Instruction-Level Parallelism，简称ILP），来并行的执行多条指令。CPU从内存中逐一读取并解码指令，然后放入待执行队列。有依赖关系的指令显然是无法并行执行的，只能顺序依次执行。没有依赖关系的指令才能够并行执行。CPU从执行队列中，选择没有依赖关系的几条指令来并行执行，这样就会导致指令执行顺序的重排列。
  * 对于单线程环境来说，指令重排序并不影响代码的最终结果，但是，对于多线程来说，指令重排序就会导致有序性问题，也就是，代码最终的运行结果，跟代码按照书写的顺序来执行得到的结果并不相同。
* 硬件内存模型导致的重排序

并发执行导致的原子性问题：原子操作指的是不可分割的操作，原子操作执行过程中的中间状态不会被访问到。对于一些看似原子操作的非原子操作，多线程并发执行会存在问题，我们把这个问题称为原子性问题。
```java
public class Demo30_2 {
  private  static volatile int count = 0;

  public static void main(String[] args) throws InterruptedException {
    Thread t1 = new Thread(new Runnable() {
      @Override
      public void run() {
        for (int i = 0; i < 10000; ++i) {
          count++;
        }
      }
    });

    Thread t2 = new Thread(new Runnable() {
      @Override
      public void run() {
        for (int i = 0; i < 10000; ++i) {
          count++;
        }
      }
    });

    t1.start();
    t2.start();

    t1.join();
    t2.join();

    System.out.println(count);
  }
}
```

直觉告诉我们最终的count值应该为20000，事实却并非如此，最终的count值并不确定，每次运行结果都不一样，但绝大部分情况下都小于20000。之所以有这样的运行结果，主要是因为count++是非原子操作。count++这条语句经过编译之后对应3条CPU指令 ：首先是读取数据到寄存器，然后在寄存器上执行自增操作，最后是将寄存器中的数据写入内存。如果两个线程并行（多核）或并发（单核多线程）执行count++，那么，count++对应的3条CPU指令有可能会交叉执行，从而产生不可预期的结果。

我们拿单核多线程来举例分析。前面讲到，线程切换会保存和恢复上下文，比如寄存器的值，因此，我们可以等价理解为：每个线程独享一组寄存器。假设线程t1将count的值0读取到寄存器，接着就发生了线程切换，线程t2将count的值0也读取到寄存器，并将寄存器中的count值自增一，然后写入内存，此时内存中的count值变为了1。这时又发生了线程切换。CPU切换为执行线程t1。线程t1的寄存器中的count值仍然为0，线程t1将寄存器中的count值自增一，然后写入内存，这时内存中的count值还是1。两个线程分别对count执行了自增一的操作，预期count值变为2，但最终结果却是1。

当然，对于原子操作，即便是多线程执行，也不会出现问题。我们拿赋值语句（例如count=5）举例。因为CPU不需要将count值读取到寄存器再修改，直接更改内存中的count值即可，所以，count=5这个赋值语句是原子操作。在多线程下并行执行赋值语句，每条赋值语句都不可分割，执行结束之后，才会执行下一条赋值语句，因此，也就不存在像count++那样的交叉执行问题了。

当然，并不是所有的赋值语句都是原子的。我们知道，Java是跨平台的，不管运行在32位计算机上还是64位计算机上，其中的long、double类型都是64位的。对于32位计算机，对long或double类型数据赋值，需要两次内存写操作才能完成，这就相当于执行了两条CPU指令，因此，不是原子操作。同理，读取long或double类型的数据，也需要执行两次内存的访问，因此，也不是原子操作。在多线程环境下，如果线程t1对long类型数据a进行赋值，在执行完高位32位赋值指令之后，切换为线程t2执行，那么，线程t2读取到的数据a的值便既不是旧值，也不是新值，而是一个没有意义的中间值。

**CPU缓存导致了可见性问题，指令重排导致了有序性问题，并发执行导致了原子性问题**。这些问题只在多线程中存在。而多线程本身只是软件层面的技术，因此，解决这些问题理应在软件层面解决，而非在硬件层面解决。由软件层面在必要的时候，**禁止CPU缓存、禁止指令重排、禁止线程并发运行**。

然而，CPU缓存、指令重排、并发执行本身都是为了提高代码的执行效率。如果我们为了保证代码在多线程情况下能正确运行，过度严格的禁用这些优化，那么，这就会影响到代码的执行效率。因此，Java提供了一些解决方案，由程序员按需使用，自行保证多线程下代码的正确运行。Java提供的解决方案被定义为Java内存模型，对应的规范为JSR-133。之所以叫做Java内存模型，是因为这个模型要解决的问题（可见性、有序性、原子性）产生的原因都跟内存数据读写有关。

在JSR-133定义的Java内存模型中，Java定义了一些关键字（比如volatile、synchronized），并对某些关键字进行了功能增强（比如final），以此来限制内存中多线程共享数据的读写方式，最终达到解决可见性、有序性、原子性问题。

我们讲解了多线程存在的3个问题：CPU缓存导致的可见性问题、指令重排导致的有序性问题、并发执行导致的原子性问题。Java内存模型解决多线程的这3个问题，主要依靠3个关键词和1个规则。**3个关键词分别是：volatile、synchronized、final，1个规则是happens-before规则**。

volatile 关键字
* 解决可见性问题：volatile翻译成中文是“易变”的意思。如果写操作是针对volatile修饰的变量进行的，那么，写操作在编译成机器指令时会在后面加上一条特殊的指令：“lock addl #0x0, (%rsp)”，这条指令会将CPU对此变量的修改，立即写入内存并通知其他CPU更新缓存。
* 解决有序性问题：指令重排导致有序性问题。因此，volatile关键字通过禁止指令重排序来解决有序性问题。禁止指令重排序又分为完全禁止指令重排序和部分禁止指令重排序。完全禁止指令重排的意思是：volatile修饰的变量的读写指令不可以跟其前面的读写指令重排，也不可以跟后面的读写指令重排。
  * 指令重排是为了优化代码的执行效率，过于严格的限制指令重排，显然会降低代码的执行效率。因此，Java内存模型将volatile的语义定义为部分禁止指令重排序。部分禁止指令重排规则如下图所示。当对volatile修饰的变量执行写操作时，Java内存模型只禁止位于其前面的读写操作与其进行重排序，不禁止位于其后面的读写操作与其进行指令重排序。当对volatile修饰的变量执行读操作时，Java内存模型只禁止位于其后面的读写操作与其进行重排序，不禁止位于其前面的读写操作与其进行指令重排序。
  * 为了实现部分禁止指令重排规则，Java内存模型定义了4个细粒度的内存屏障（Memory Barrier），也叫做内存栅栏（Memory Fence），它们分别是：StoreStore、StoreLoad、LoadLoad、LoadStore。需要注意的是，这些内存屏障是抽象概念，底层需要依赖CPU提供的内存屏障指令来实现。而Java内存模型之所以定义抽象的内存屏障，是为了屏蔽不同CPU提供的内存屏障指令的差别。
  * 刚刚讲到的StoreStore、StoreLoad、LoadLoad、LoadStore都是抽象的内存屏障，它们的具体实现依赖具体CPU提供的内存屏障指令。对于常用到X86 CPU来说，其不支持读写（前一个操作是读操作，后一个操作是写操作）重排序、读读（前一个操作是读操作，后一个操作是读操作）重排序、写写（前一个操作是写操作，后一个操作也是写操作）重排序，只支持写读（前一个操作是写操作，后一个操作是读操作）重排序。因此，JVM运行在x86 CPU上时，Java内存模型会将StoreStore、LoadLoad、LoadStore翻译为空操作指令，将StoreLoad翻译为“lock addl #0x0, (%rsp)”。也就是说，“lock addl #0x0, (%rsp)”这条指令既能保证volatile变量的可见性，还能禁止指令的重排。
* 使用 volatile 解决部分原子性问题
  * 上节提到两类原子性问题，一类是64位long和double类型数据的读写的原子性问题，另一类是自增语句（例如count++）的原子性问题。volatile可以解决第一类原子性问题，但是无法解决第二类原子性问题。
  * 在32位计算机上，读写64位的long或double类型数据会触发两次内存读写操作。如果我们使用volatile关键词修饰long或double类型变量，那么，编译器将代码编译成机器指令时，会在两次读或写之前添加锁定总线指令，在两次读或写完成之后再释放总线，这样就可以保证64位long或double类型数据在32位计算机上读写的原子性。不过，现在大部分计算机都已经是64位的了，因此，我们也就不需要为long或double类型变量添加volatile关键字。
  * 实际上，自增语句的原子性问题，不是出在CPU缓存上，而是出在寄存器上。上一节我们讲到，自增语句可以分解为3个指令：首先是读取数据到寄存器，然后在寄存器上执行自增操作，最后是将寄存器的数据写入内存。假设线程t1和线程t2分别运行在CPU A和CPU B上，当线程t1和线程t2都将变量值读取到寄存器之后，尽管变量被volatile修饰，线程t1对变量的修改，能够立即写入内存，并且同步给线程t2所使用的CPU缓存，但并不会同步更新线程t2所使用的寄存器，线程t2中的寄存器保存的仍然是老值。线程t2对老值自增一，然后写入内存，就会导致覆盖掉线程t1更新之后的结果。

synchronized 关键字：volatile只能解决long、double读写的原子性问题，对于更大范围的原子性问题的解决，volatile无能为力，我们需要使用synchronized关键字来解决。除了解决原子性问题，synchronized还可以解决可见性、有序性问题。它的解决方式比较简单粗暴，让原本并发执行的代码串行执行，并且，每次加锁和释放锁都会同步CPU缓存和内存中的数据。

final 关键字：原本final只限制变量是否可变，现在Java内存模型对final的语义做了增强，禁止编译器将构造函数中对final变量的写操作，重排序到对象引用之后，这样也就禁止了STEP2和STEP3重排序。Java内存模型之所以单独对final的语义进行增强，是因为被final修饰变量原本是不可变的，但在多线程环境下，一个线程多次访问final变量，可能会访问到两个不同的值（instance在调用构造函数初始化之前的value值和初始化之后的value值），违背了final关键词的语义。因此，Java内存模型对final变量语义进行了增强，保证在一个线程内所有关键字的语义不被违背。

happens-before规则：happens-before规则并不能解决多线程存在的可见性、有序性和原子性问题。happens-before规则的主要作用是：程序员可以依照规则，检查自己编写的代码在多线程下的执行顺序，是否符合自己的预期。happens-before规则有如下所示。
* 单线程规则：在单线程中，前面的操作先于后面的操作执行。
* 锁规则：一个线程释放锁先于另一个线程获取锁。
* 在时间序上，如果对一个volatile变量的写操作，先于后面对这个变量的读操作执行，那么，volatile读操作必定能读到volatile写操作的结果。也就是说，如果x为volatile变量，在t1时刻执行了x=1，在t2时刻执行了y=x，并且t1小于t2，那么，不管编译器或者CPU如何优化指令的执行顺序，y总是等于1。
* 线程启动规则：如果线程A在执行过程中，启动了线程B，那么，之前线程A对共享变量的所有修改对线程B都可见。
* 线程终结规则：如果线程A在执行的过程中，通过Thread.join()等待线程B终止，那么，线程B对共享变量的修改，在线程B终止之后，对线程A可见。
* 线程中断规则：线程A对线程B调用interrupt()方法，先行发生于线程B的代码检测到中断事件的发生。
* 对象终结规则：一个对象的初始化完成，先行发生于调用它的finalize()方法。
* 如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。

在多线程环境下，CPU缓存会带来可见性问题。为了保证写入一个CPU缓存的数据能够立刻写入内存，并同步更新到其他CPU缓存，保证数据的一致性，Java内存模型提供了volatile关键字。不过，**大部分CPU都支持缓存一致性协议**（比如MESI），在硬件层面，CPU缓存数据的一致性问题就已经被解决，为什么还需要volatile关键字呢？

CPU以缓存行为单位来读写数据。因此，MESI协议处理的对象是缓存行。MESI协议提供了4种不同的缓存行状态，它们分别如下所示。
* M（Modified）：表示当前缓存行中的数据已被修改，但并未同步到内存
* E（Exclusive）：表示当前缓存中有这个数据，其他CPU缓存中没有这个数据
* S（Shared）：表示当前缓存中有这个数据，其他CPU缓存中也有这个数据
* I（Invalid）：表示当前缓存中的数据已经失效，说明其他CPU对数据进行了修改，下次CPU读取数据要从内存中读取，并同步更新缓存

> 你可能会说，CPU0读取内存中的数据a时，怎么知道其他CPU缓存有没有已经读取数据a呢？实际上，这就要靠总线广播了，更加形象的叫法是**总线嗅探**。CPU0在读取内存中的数据a之前，会往总线上广播一个读请求，其他CPU缓存获取到CPU0的读请求之后，会检查自己是否已经缓存了数据a，并告知CPU0。

从上述MESI协议的状态转换举例来看，当多个CPU缓存中都有同一数据时，一个CPU对缓存数据进行修改，需要广播invalidate消息，其他CPU收到invalidate消息之后，将对应的缓存行设置为I，然后再发送invalidate ack消息给这个CPU。此时，这个CPU才可以将数据更新写入缓存和内存。也就是说，整个写操作需要做很多工作，非常耗时。**CPU需要等待写操作完成，才能执行其他指令。慢速的写操作会影响CPU的执行效率**。于是，计算机科学家在CPU和CPU缓存之间，增加了一个类似消息中间件的存储结构，叫做Store Buffer，用来异步执行写操作。

CPU将写操作的所有信息存储到Store Buffer之后，就立刻返回执行其他指令了，由Store Buffer来完成剩下的工作，包括发送invalidate消息，接收invalidate ack，将数据写入缓存和内存。引入Store Buffer之后，在读取数据时，CPU会先从Store Buffer中读取，如果读取不到再从缓存中读取。这样就可以保证CPU总是能读取到自己写入的最新值。

Invalidate Queue
* Store Buffer发送给其他CPU invalidate消息之后，需要等待其他CPU设置缓存失效并返回invalidate ack消息，才能执行更新缓存和内存的操作。而其他CPU有可能忙于其他事情，导致来不及设置缓存失效和回复invalidate ack消息，这样数据便会堆积在Store Buffer中。Store Buffer的存储空间很小，当有大量数据堆积在Store Buffer中时，CPU往Store Buffer中的写入操作便会阻塞等待，此时Store Buffer便失去了作用。
* 为了解决这个问题，计算机科学家又引入了一个新的存储结构：Invalidate Queue，专门用来存储invlidate消息和回复invalidate ack消息，并异步执行设置缓存行失效操作。这样就进一步节省了Store Buffer处理写操作的时间，能够让Store Buffer尽快清空。

如果没有Store Buffer和Invalidate Queue，那么，缓存一致性协议是可以保证各个CPU缓存之间的数据一致性的，也就不会存在可见性问题。但是，当引入Store Buffer和Invalidate Queue来异步执行写操作之后，即便使用缓存一致性协议，仍然会存在短暂的可见性问题，即在短暂的时间窗口内，其中一个CPU并不能感知另外一个CPU对共享数据的修改。

## 如何分析一段代码是否线程安全
线程安全、临界区、竞态是我们再学习多线程的过程中，经常遇到的几个概念。

在Java中，线程安全或不安全描述的对象既可以是类，也可以是函数。

对于函数来说，如果两个线程并发执行某个函数，那么，由于线程切换的存在，两个线程的指令之间可以任意交叉执行，如下所示。**如果按照任意顺序来执行两个线程中的指令，最终得到的结果都是相同的、确定的、符合预期的，那么，我们就称这个函数是线程安全的**，否则，我们就称为这个函数是线程不安全的，或者非线程安全的。

对于线程安全类来说，除了要求类中的每个函数都是线程安全的之外，任意两个函数之间也必须是线程安全的。也就是说，两个线程并发执行一个类的任意两个不同的函数，按照任意指令执行顺序，最终得到的结果都是相同的、确定的、符合预期的。

通过线程安全的函数和线程安全的类的定义，我们可以总结得出：如果一个类是线程安全的，那么，所有的函数都是线程安全的。如果一个类的所有的函数都是线程安全的，那么，这并不能推出这个类就一定是线程安全的。

我们把**有可能引起线程不安全的局部代码块，叫做临界区（Critical Section）。我们把两个线程竞争执行临界区的这种状态，叫做竞态（Race Condition）**。两个线程处于竞态执行临界区，就有可能执行出错。

临界区一般包含以下两个特征
* 访问共享资源：共享资源包括类中的成员变量、通过函数参数传递进来的共享对象等。如果某个函数只访问局部变量，而局部变量存储在栈中供线程独享，那么，这个函数就不存在线程安全问题。除此之外，**如果代码只包含对共享资源的读操作，那么，这段代码一般也不会存在线程安全问题**。
* 包含复合操作：**复合操作由多个操作组成，比如先检查再执行、先读取再修改后写入。这些复合操作一般都是非原子操作**。实际上，非双重检测的单例就属于先检查再执行这类复合操作，自增操作就属于先读取再修改后写入这类复合操作。

在平时的开发中，尽管我们很少编写多线程代码，但是，我们编写的代码常常会运行在框架、容器中，而框架、容器往往会使用多线程来并发执行我们编写的代码，因此，学会编写线程安全的代码非常重要。不过，要想写出线程安全的代码，我们首先要学会如何通过分析代码的线程安全性。

临界区和竞态是线程不安全的必要条件，而非充分条件，也就是说，两个线程并发竞争执行访问共享资源并且包含复合操作的代码，并不一定就存在问题。代码存在临界区和竞态，只能说明代码存在线程不安全的风险，我们需要继续深入分析，看两个线程交叉执行临界区，是否真的存在执行结果不确定、不符合预期的情况。

除此之外，如果临界区访问的共享资源有多个，那么，**我们还需要查看指令重排序是否会影响执行结果的正确性，导致线程不安全**。如下代码中的共享资源有两个，一个是ready，另一个是value，指令重排序会导致结果运行错误，最终打印的结果不是2而是1。
```java
public class Demo {
  private static boolean ready = false;
  private static int value = 1;

  public static void main(String[] args) throws InterruptedException {
    Thread t1 = new Thread(new Runnable() {
      @Override
      public void run() {
        while (!ready) {
        }
        System.out.println(value);
      }
    });

    Thread t2 = new Thread(new Runnable() {
      @Override
      public void run() {
        value = 2;
        ready = true;
      }
    });

    t1.start();
    t2.start();
    t1.join();
    t2.join();
  }
}
```

在分析代码是否线程安全时，我们还需要关注，代码中所使用的其他类或函数是否线程安全，是否会引起自己编写的类或函数的线程安全性问题。除此之外，特别需要注意的是，使用线程安全的类或函数编写的代码，也不一定是线程安全的。

互斥和同步
* 对于指令重排序导致的线程不安全问题，我们可以通过使用volatile关键字来禁止指令重排序来解决。不过，这种线程不安全的情况并不常见。
* 最常见的是多线程交叉执行非原子操作导致的线程不安全问题。对应的解决的方法是，通过对临界区加锁，让临界区变为互斥操作。一个线程执行完临界区代码之后，才允许下一个线程执行。

实际上，互斥是多线程要解决的两个核心问题之一，而另一个核心问题是同步。同步指的是多个线程之间如何协同执行，比如一个线程等待另一个线程执行完成之后再执行。实际上，专栏中的多线程模块很大部分都是在讲如何实现高效的互斥和同步。专栏中的多线程模块主要包括5部分：基础理论、互斥锁、无锁编程、同步工具、并发容器、线程管理。其中，
* 互斥锁部分主要讲解实现临界区互斥的手段，提供了各种不同粒度和作用的锁，比如读写锁、原子类、偏向锁、轻量级锁、自旋锁等，最大限度的减小加锁范围、提高代码并发执行程度。
* 无锁编程主要讲解如何不使用锁的情况下保证代码的线程安全性，进一步提高代码的并发性能。
* 同步工具主要包括条件变量、信号量、Latch、Barrier等，用来实现各种线程协作模式。
* 并发容器部分讲解了一些高性能的线程安全的容器（比如ConcurrentHashMap）以及支持线程同步的容器（比如各种阻塞队列），方便程序员直接使用，不用自己从零开始实现。

## synchronized
对于多线程执行非原子操作导致的线程不安全问题，最常用的解决方案便是加锁。Java语言提供了两种类型的锁，一种是synchronized关键字，一种是Lock工具类。在JDK1.5及其以前版本中，synchronized的实现比较简陋，性能没有后起之秀Lock高，但是，在JDK1.6及其之后的版本中，Java对synchronized做了大量优化，**synchronized的基本实现原理跟Lock的基本实现原理趋于一致**，因此，在性能方面，两者也就相差无几了。

如果某个线程执行synchronized语句但没有获取锁，那么，synchronized是如何阻塞当前代码的执行的？以及是如何停止对应的内核线程执行的？当另一个线程释放锁之后，又是如何通知等待锁的线程获取锁的？

两种作用方法
* 作用于方法
* 作用于局部代码块

实际上，synchronized关键字底层使用的锁叫做Monitor锁。但是，我们无法直接创建和使用Monitor锁。Monitor锁是寄生于对象存在的。每个对象都会拥有一个Monitor锁。如果我们想要使用一个新的Monitor锁，我们只需要使用一个新的对象，并在synchronized关键字后，附带声明这个对象即可。我们对方法添加synchronized关键字，就相当于隐式地使用了当前对象（this对象）的Monitor锁。

为了让add()函数和substract()函数之间能并发执行，我们可以采用如下方式，对add()函数和substract()函数分别加不同的锁。add()函数使用obj1对象上的Monitor锁，substract()函数使用obj2对象上的锁，两者互不影响。
```java
public class Counter {
  private int increasedSum = 0;
  private int decreasedSum = 0;

  private Object obj1 = new Object();
  private Object obj2 = new Object();

  public void add(int value) {
    synchronized (obj1) {
      increasedSum += value;
    }
  }

  public void substract(int value) {
    synchronized (obj2) {
      decreasedSum -= value;
    }
  }
}
```

从上述分析，我们发现，不仅一个Wallet对象上的transferTo()函数不能并发执行，所有Wallet对象上的transferTo()函数都不能并发执行。为了实现这样的限制，我们就需要使用类锁来替代对象锁，对transferTo()函数进行加锁。类锁的语法非常简单，如下代码所示，在synchronized关键词后跟随某个类的Class类对象即可。
```java
public class Wallet {
  private int balance;

  public void transferTo(Wallet targetWallet, int amount) {
    synchronized (Wallet.class) {
      if (this.balance >= amount) {
        this.balance -= amount;
        targetWallet.balance += amount;
      }
    }
  }
}
```

除了显示指定使用哪个类的类锁（类的Class类对象的Monitor锁）之外，如果我们对静态方法添加synchronized关键词，那么，对应的静态方法会隐式地使用当前类的类锁。如下代码所示，add()函数使用Counter类的类锁。
```java
public class Counter {
  private static int count = 0;

  public synchronized static void add(int value) {
      count += value;
  }
}
```

锁对应的字节码
* 作用于方法：从字节码中，我们发现，实际上，编译器只不过是在函数的flags中添加了ACC_SYNCHRONIZED标记而已，其他部分跟没有使用synchronized的add()函数的字节码相同。
* 作用于局部代码块：从字节码中，我们发现，synchronized 通过monitorenter和monitorexit来标记作用范围。除此之外，对于以下字节码，我们还有两点需要解释。其一，以下字节码中有两个monitorexit，添加第二个monitorexit的目的是为了在代码抛出异常时仍然能解锁。其二，前面讲到，synchronized可以选择指定使用哪个对象的Monitor锁。具体使用哪个对象的Monitor锁，通过monitorenter前面的几行字节码来指定。

底层实现原理：量级锁就是前面提到的 Monitor 锁，Monitor 锁与对象之间又是如何关联的

JVM有不同的实现版本，因此，Monitor锁也有不同的实现方式。在Hotspot JVM中，Monitor锁对应的实现类为ObjectMonitor类。因为Hotspot JVM是用C++实现的，所以，ObjectMonitor是C++类。ObjectMonitor包含的代码很多，我们只罗列一些与其基本实现原理相关的成员变量，如下所示。
```c++
class ObjectMonitor {
  void * volatile _object; //该Monitor锁所属的对象
  void * volatile _owner; //获取到该Monitor锁的线程
  ObjectWaiter * volatile _cxq; //没有获取到锁的线程暂时加入_cxq
  ObjectWaiter * volatile _EntryList; //存储等待被唤醒的线程
  ObjectWaiter * volatile _WaitSet; //存储调用了wait()的线程
}
```

实际上，不管是后面要讲到的JUC（java.util.concurrent） Lock，还是现在正在讲的Java内置synchronized，它们作为多线程的互斥锁，所包含的基本功能是一致的，主要有以下几点：
1. 多个线程竞争获取锁
  * 多个线程同时请求获取Monitor锁时，它们会通过CAS操作来设置ObjectMonitor中的_owner字段。谁设置成功，谁就获取了这个Monitor锁。这里我们稍微解释一下CAS操作。CAS全称为Compare And Set，也就是我们之前提到的先检查再执行。参与竞争Monitor锁的线程，会先检查_owner是否为null，如果_owner为null，那么，线程将对应的Thread对象的地址赋值给_owner。
  * 前面我们讲到，先检查再执行这类复合操作是非线程安全的。那么，这样就会导致多个线程有可能同时检查到_owner为null，然后都去改变_owner值。为了解决这个问题，JVM采用CPU提供的cmpxchg指令，通过给总线加锁的方式来保证了以上CAS操作的线程安全性。实际上，这就相当于在硬件层面上给以上CAS操作加了锁。
2. 没有获取到锁的线程排队等待锁
  * 当多个线程竞争获取Monitor锁时，成功获取锁的线程就去执行代码了，没有获取到锁的线程会被放入ObjectMonitor的_cxq中等待锁。
3. 锁释放之后会通知排队等待锁的线程去竞争锁
  * 当持有锁的线程释放锁之后，它会从_EntryList中取出一个线程。被取出的线程会再次通过CAS操作去竞争Monitor锁。之所以不是直接让这个线程获取锁而是再去竞争锁，是因为此时有可能存在新来的线程（非_EntryList里的线程）也在竞争锁。
  * 如果_EntryList中没有线程，我们就会先将_cxq中的所有线程全部搬移到_EntryList中，然后再从_EntryList中取线程。那么，为什么我们不直接从_cxq取线程，而是要将_cxq中的线程倒腾到_EntryList中再取呢？实际上，这样做的目的是减少多线程环境下链表存取操作的冲突。_cxq只负责存操作（往链表中添加节点），_EntryList负责取操作（从链表中删除节点），冲突减少，线程安全性处理就变得简单。因为多个线程有可能同时竞争锁失败，同时存入_cxq中，所以，我们需要通过CAS操作来保证往_cxq中添加节点操作的线程安全性。因为只有释放锁的线程才会从_EntryList中取线程，所以，_EntryList的删除节点操作是单线程操作，不存在线程安全问题。
  * _WaitSet并不用于实现synchronized，而是用于实现wait()、notify()线程同步功能。
4. 没有获取锁的线程会阻塞，并且对应的内核线程不再分配时间片
  * Java线程采用1:1线程模型来实现，一个Java线程会对应一个内核线程。应用程序提交给Java线程执行的代码（Runnable接口的run()方法中的代码），会一股脑地交给对应的内核线程来执行。内核线程在执行的过程中，如果遇到synchronized关键字，会执行上述的1）2）3）。如果没有竞争到锁，则内核线程会调用park()函数将自己阻塞，这样CPU就不再分配时间片给它。
5. 阻塞线程获取锁之后取消阻塞，并且对应的内核线程恢复分配时间片
  * 实际上，准确的说法是从等待队列里唤醒的线程会取消阻塞。持有锁的线程在释放锁之后，从_EntryList中取出一个线程时，就会调用unpark()函数，取消对应内核线程的阻塞状态，这样才能让它去执行竞争锁的代码。在Linux操作系统下，unpark()函数的大致实现思路如下代码所示，通过pthread_cond_signal()函数给调用park()函数的线程发送信号。

我们讲解了synchronized底层用到的重量级锁的实现原理，重量级锁要维护等待队列（_cxq、_EntryList），并且还要调用操作系统的系统调用（例如pthread_mutext_lock、pthread_cond_wait、pthread_cond_signal、pthread_mutext_unlock）来阻塞和唤醒线程，涉及到用户态和内核态的切换，因此，加锁、解锁比较耗时。**在JDK1.6版本中，Java对synchronized做了较大的优化，引入了偏向锁、轻量级锁、锁粗化、锁消除等优化手段，进一步提高了加锁、解锁的性能**。

synchronized 引入偏向锁和轻量级锁是基于这样一个推断：**尽管我们需要对存在线程安全问题的代码进行加锁，但是，这只是处于防御的目的，实际上，出现同一时刻多个线程竞争锁的概率很小，甚至一个锁在大部分情况下都只被一个线程使用**。

对于一个synchronized锁，**如果它只被一个线程使用，那么，synchronized锁底层使用偏向锁来实现。如果它被多个线程交叉使用（你用完我再用），不存在竞争使用的情况，那么，synchronized锁底层使用轻量级锁来实现。如果它存在被竞争使用的情况，那么，synchronized锁底层使用重量级锁来实现**。

实际上，synchronized使用的锁只能升级不能降级，也就是，只能从偏向锁，升级为轻量级锁或无锁，再升级为重量级锁。在这个升级链路中，一旦升级为更加严格的锁，就不能再被降级。比如，一旦升级为重量级锁之后，就不能再降级为轻量级锁。除此之外，跟偏向锁有所不同，轻量级锁是会主动解锁的，解锁之后的状态就是无锁状态。偏向锁不存在无锁状态，重量级锁的无锁状态通过其他方式来表示。

弄清楚了偏向锁和轻量级锁，synchronized的另外两个优化（锁消除和锁粗化）相比而言就简单多了。虚拟机在执行JIT编译时，会根据对代码的分析（逃逸分析），去掉某些没有必要的锁。如下示例代码所示。为了保证多线程操作的安全性，StringBuffer中的append()函数在设计实现时加了锁。但是，在下面的代码中，strBuffer是局部变量，不会被多线程共享，更不会在多线程环境下调用它的append()函数。因此，append()函数的锁可以被优化消除。
```java
public class Demo {
  public String concat(String s1, String s2) {
    StringBuffer strBuffer = new StringBuffer();
    strBuffer.append(s1);
    strBuffer.append(s2);
    return strBuffer.toString();
  }
}
```

当讲到synchronized作用于代码块时，我们提到，缩小加锁范围能够提高程序的并发程度，提高多线程环境下程序的执行效率。但是，在有些情况下，虚拟机在执行JIT编译时，会扩大加锁范围，将对多个小范围代码的加锁，合并一个对大范围代码的加锁，这样的做法叫做锁粗化。如下示例代码所示，执行10000次append()函数，会加锁解锁10000次。通过进行锁粗化，编译器将append()函数的锁去掉，移到for循环外面，这样就只需要加锁解锁1次即可，性能显然提高了很多。
```java
public class Demo35_4 {
  private StringBuffer strBuffer = new StringBuffer();

  public void reproduce(String s) {
    for (int i = 0; i < 10000; ++i) {
      strBuffer.append(s);
    }
  }
}
```

## Lock
我们学习了一种互斥锁：Java synchronized内置锁。从本节开始，我们来学习另外一种互斥锁：JUC并发包提供的Lock锁。相对于synchronized锁，Lock锁提供了更加丰富的特性，比如支持**公平锁、可中断锁、非阻塞锁、可超时锁**等。

JUC提供了几种不同的锁，继承和实现层次关系如下图所示。本节重点讲解Lock锁（也就是Lock接口及其实现类ReentrantLock），下下节讲解读写锁（也就是ReadWriteLock接口及其实现类ReentrantReadWriteLock）以及读写锁的升级版StampedLock。

因为在平时的开发中，我们用到的锁都是**可重入锁**，所以，Lock接口只有 一个可重入的实现类ReentrantLock。

可重入锁指的是可以被同一个线程多次加锁的锁。注意，这里说的多次加锁，并不是说解锁之后再次加锁，而是在锁没有解锁前再次加锁。

JUC提供的锁都是可重入锁。实际上，Java synchronized内置锁也是可重入锁。从侧面上，我们也可以得出，可重入是对锁的基本要求。为了实现可重入特性，可重入锁中需要有一个变量来记录重入的次数。每重入一次，变量就增一；每解锁一次（调用unlock()或退出synchronized代码块），变量就减一，直到变量值为0时，才会释放锁并唤醒其他线程。

对于公平锁来说，线程会按照请求锁的先后顺序来获得锁，也就是我们经常说的FIFO。对于非公平锁来说，当多个线程请求锁时，非公平锁无法保证这些线程获取锁的先后顺序，有可能后申请锁的线程先获取到锁。

Java将synchronized设计为只支持非公平锁，而JUC提供的ReentrantLock既支持公平锁，也支持非公平锁。默认情况下，ReentrantLock为非公平锁。如果需要创建公平锁，那么，我们只需要在创建ReentrantLock对象时，将构造函数的参数设置为true即可。

既然实现公平锁并不复杂，而且从直觉上，公平锁比非公平锁更加合理，那么，synchronized为什么只支持非公平锁？主要原因有以下3个方面。
* 历史原因：synchronized 早期开发时病咩有考虑的那么全面
* 需求的原因：绝大部分业务场景都不需要严格规定线程的执行顺序，如果真的需要，我们可以通过条件变量（wait()、notify()等）等同步工具来实现
* 性能的原因：非公平锁的性能比公平锁的性能更好。我们知道，加入等待队列并调用park()函数阻塞线程，涉及到用户态和内核态的切换，这个过程是比较耗时。对于非公平锁来说，新来的线程直接竞争锁，这样就有可能避免加入等待队列并调用费时的park()函数。

对于synchronized锁来说，线程在阻塞等待synchronized锁时是无法响应中断的。而JUC Lock接口提供了lockInterruptibly()函数，支持可响应中断的方式来请求锁。示例代码如下所示。主线程先获取到了锁并一直持有，之后线程t1调用lockInterruptibly()请求锁，因为锁被主线程持有，所以，线程t1阻塞等待。主线程调用interrupt()函数向线程t1发起中断请求，线程t1响应中断请求，退出阻塞等待锁，并打印“I am interrupted”。
```java
public class Demo {
  private static Lock lock = new ReentrantLock();

  public static void main(String[] args) {
    Thread t1 = new Thread(new Runnable() {
      @Override
      public void run() {
        try {
          lock.lockInterruptibly();
        } catch (InterruptedException e) {
          System.out.println("I am interrupted");
          return;
        }
        try {
          System.out.println("I got lock");
        } finally {
          lock.unlock();
        }
      }
    });
    lock.lock();
    t1.start();
    t1.interrupt();
    lock.unlock();
  }
}
```

可中断锁方便关闭正在执行的线程。比如，Nginx服务器采用多线程来执行请求。当我们调用stop命令关闭Nginx服务器时，Nginx服务器可以采用中断的方式，将阻塞等待锁的线程中止，然后，合理的释放资源和妥善处理未执行完成的请求，以实现服务器的优雅关闭。

对于synchronized锁来说，一个线程去请求一个synchronized锁时，如果锁已经被另一个线程获取，那么，这个线程就需要阻塞等待。Lock锁提供了tryLock()函数，支持非阻塞的方式获取锁。如果锁已经被其他线程获取，那么，调用tryLock()函数会直接返回错误码而非阻塞等待。示例代码如下所示。非阻塞锁的实现原理非常简单。竞争锁失败的线程不放入队列排队即可实现非阻塞锁。
```java
public class Demo {
  private Lock lock = new ReentrantLock();

  public void useTryLock() {
    if (lock.tryLock()) {
      try {
        // ...执行业务代码...
      } finally {
        lock.unlock();
      }
    } else {
      // ...没有获取锁，执行其他业务代码...
    }
  }
}
```

除了提供不带参数的tryLock()函数之外，JUC Lock接口还提供给了带时间参数的tryLock()函数，支持非阻塞获取锁的同时设置超时时间。一个线程在请求锁时，如果这个锁被其他线程持有，那么这个线程会阻塞等待一段时间。如果超过了设定的超时时间，线程仍然没有获取到锁，那么tryLock()函数将会返回错误码而不再阻塞等待。示例代码如下所示。从示例代码中，我们还可以发现，tryLock()跟lockInterruptibly()一样，也可以被中断。这样是为了避免tryLock()阻塞过长时间。
```java
public class Demo {
  private Lock lock = new ReentrantLock();

  public void useTryLockWithTimeout() {
    boolean locked = false;
    try {
      locked = lock.tryLock(100, TimeUnit.MILLISECONDS);
    } catch (InterruptedException e) {
      System.out.println("I am interrupted");
    }
    if (locked) {
      try {
        // ...执行业务代码...
      } finally {
        lock.unlock();
      }
    } else {
      // ...没有获取锁，执行其他业务代码...
    }
  }
}
```

在很多对响应时间比较敏感的系统中，比如直接面向用户的系统，从用户体验上说，请求失败给予提示，要远好于响应超时而没有反应。我们拿Tomcat等Web服务器来举例。Tomcat采用线程池的方式多线程执行用户请求。如果某个特殊请求不能并发执行，并且请求执行时间比较长，那么，请求的处理代码就需要加锁。当多个线程同时执行多个特殊请求时，有些线程就有可能因为迟迟无法获取到锁而无法执行请求。一方面，这样会导致用户请求超时，给用户带来不好的体验，另一方面，线程一直等待锁，长期被占用，无法执行其他任务，剩余可以执行用户请求的线程变少，从而导致系统的响应速度变慢。

为了解决以上问题，我们可以使用带超时时间的tryLock()函数来请求锁。如果在设定的超时时间内未获取到锁，那么，线程就中止执行用户请求，返回错误信息给用户。当然，这只是保护措施，毕竟，以上问题只有在无法并发执行的特殊请求集中大量到来时才会发生。

JUC Lock底层主要依赖AQS来实现。实际上，AQS是JUC中非常重要的基础组件。JUC中的很多锁（Lock、ReadWriteLock）和同步工具（Condition、Semaphore、CountDownLatch）都是基于AQS来实现的。

AQS是抽象类AbstractQueueSynchronizer的简称，中文翻译为抽象队列同步器。

不过，在实现思路上，AQS跟ObjectMonitor又有所不同。首先，ObjectMonitor类是在JVM中基于C++来实现的，因为synchronized、wait()、notify()是Java语言提供的内置的语法和函数。AQS类是在JDK中基于Java语言实现的，因为JUC只是JDK中的一个并发工具包而已。其次，ObjectMonitor使用不同的队列来实现锁和同步工具，AQS使用同一个队列来实现锁和同步工具。

AQS类中所包含的成员变量并不多，如下代码所示。这几个成员变量构成了AQS实现锁和同步工具所依赖的核心数据结构。
```java
public abstract class AbstractQueuedSynchronizer
                      extends AbstractOwnableSynchronizer {
    private transient volatile Node head;
    private transient volatile Node tail;
    private volatile int state;
}

public abstract class AbstractOwnableSynchronizer {
    private transient Thread exclusiveOwnerThread;
}
```

如上代码所示，AQS继承自AbstractOwnableSynchronizer类。AbstractOwnableSynchronizer类只包含一个成员变量exclusiveOwnerThread。连带继承来的一个成员变量，AQS中总共包含4个成员变量。接下来，我们依次介绍下它们。

AQS定义了8个模板方法，如下所示。以下8个函数可以分为2组，分别用于AQS的两种工作模式：独占模式和共享模式。其中，前4个函数用于独占模式，后4个函数用于共享模式。Lock锁为排它锁，因此，Lock锁的底层实现只会用到AQS的独占模式。ReadWriteLock锁中的读锁为共享锁，写锁为排它锁，因此，ReadWriteLock锁的底层实现既会用到AQS的独占模式，又会用到AQS的共享模式。Semaphore、CountdownLatch这些同步工具只会用到AQS的共享模式。
```java
public final void acquire(int arg) { ... }
public final void acquireInterruptibly(int arg)
                  throws InterruptedException { ... }
public final boolean tryAcquireNanos(int arg, long nanosTimeout)
                  throws InterruptedException { ... }
public final boolean release(int arg) { ... }

public final void acquireShared(int arg) { ... }
public final void acquireSharedInterruptibly(int arg)
                  throws InterruptedException { ... }
public final boolean tryAcquireSharedNanos(int arg, long nanosTimeout)
                  throws InterruptedException { ... }
public final boolean releaseShared(int arg) { ... }
```

AQS提供了4个抽象方法，如下所示。前两个抽象方法用于独占模式的4个模板方法，后两个抽象方法用于共享模式的4个模板方法。在标准的模板方法设计模式的代码实现中，抽象方法需要使用abstract关键字来定义，以强制子类去实现它。但是，以下抽象方法并没有使用abstract关键字来定义，而是给出了默认的实现，即抛出UnsupportOperationException异常。这样做是为了减少开发量，即我们不需要在子类中实现所有的抽象方法，用到哪个就实现哪个即可。
```java
protected boolean tryAcquire(int arg) {
    throw new UnsupportedOperationException();
}

protected boolean tryRelease(int arg) {
    throw new UnsupportedOperationException();
}

protected int tryAcquireShared(int arg) {
    throw new UnsupportedOperationException();
}

protected boolean tryReleaseShared(int arg) {
    throw new UnsupportedOperationException();
}
```

接下来，我们结合ReentrantLock来看下AQS如何使用。ReentrantLock既支持非公平锁，又支持公平锁，其部分代码如下所示。ReentrantLock定义了两个继承自AQS的子类：NonfairSync和FairSync，分别用来实现非公平锁和公平锁。因为NonfairSync和FairSync释放锁的逻辑是一样的，所以，NonfairSync和FairSync又抽象出了一个公共的父类Sync。注意，为了更清晰的展示原理，在不改变代码逻辑的情况下，我对源码做了少许调整。本节中的其他源码亦是如此。
```java
public class ReentrantLock implements Lock {
    private final Sync sync;

    abstract static class Sync extends AbstractQueuedSynchronizer { ... }
    static final class NonfairSync extends Sync { ... }
    static final class FairSync extends Sync { ... }

    public ReentrantLock() {
        sync = new NonfairSync();
    }

    public ReentrantLock(boolean fair) {
        sync = fair ? new FairSync() : new NonfairSync();
    }

    public void lock() {
        sync.acquire(1);
    }

    public void unlock() {
        sync.release(1);
    }
    //...省略其他方法...
}
```

JUC提供的锁有三类：**普通互斥锁**（Lock和ReentrantLock）、**读写锁**（ReadWriteLock和ReentrantReadWriteLock）、**StampedLock**。

为了提高多线程环境下代码执行的并发度，两个读操作是可以并发执行的，但是，读操作和写操作不能并发执行，同理，写操作和写操作也不能并发执行。为了满足这样特殊的加锁需求，JUC提供了读写锁（ReadWriteLock接口和ReentrantReadWriteLock类）。

ReadWriteLock接口的定义如下所示。跟Lock和ReentrantLock的关系类似，ReadWriteLock也只有一个可重入的实现类ReentrantReadWriteLock。
```java
public interface ReadWriteLock {
    Lock readLock();
    Lock writeLock();
}
```

ReadWriteLock接口中只包含两个函数，其中，readLock()函数返回读锁。读锁用来给读操作加锁。writeLock()函数返回写锁。写锁用来给写操作加锁。**读锁是一种共享锁，可以被多个线程同时获取。写锁是排它锁，同时只能被一个线程获取**。除此之外，读锁和写锁之间也是排它的。因此，读写锁一般用于读多写少的场景。读写锁的使用示例代码如下所示。两个线程允许并发执行get()函数。
```java
public class Demo {
  private List<String> list = new LinkedList<>();
  private ReadWriteLock rwLock = new ReentrantReadWriteLock();
  private Lock rLock = rwLock.readLock(); //读锁
  private Lock wLock = rwLock.writeLock(); //写锁

  public void add(int idx, String elem) {
    wLock.lock(); //加写锁
    try {
      list.add(idx, elem);
    } finally {
      wLock.unlock(); //释放写锁
    }
  }

  public String get(int idx) {
    rLock.lock(); //加读锁
    try {
      return list.get(idx);
    } finally {
      rLock.unlock(); //释放读锁
    }
  }
}
```

ReentrantReadWriteLock既支持公平锁又支持非公平锁。跟ReentrantLock的公平锁和非公平锁的构建方法一样，ReentrantReadWriteLock默认为非公平锁。如果要创建公平锁，我们只需要在创建ReentrantReadWriteLock对象时，将构造函数的参数设置为true即可。

绝大部分锁都是可重入锁，读写锁也不例外。一个线程获取读锁之后，在读锁释放前，还可以再次获取读锁。同理，一个线程获取写锁之后，在写锁释放前，还可以再次获取写锁。但是，一个线程在获取读锁之后，在读锁释放前，是否还能再获取写锁？还有一个线程在获取写锁之后，在写锁释放前，是否还能再获取读锁呢？

读写锁不支持锁升级，也就是，一个线程获取读锁之后，在读锁释放前，不可以再获取写锁。这是因为在一个线程获取读锁时，有可能同时还有其他线程也获取了读锁。将其中一个线程的读锁升级为写锁，就有可能违背读写锁中读锁和写锁互斥的要求。

读写锁支持锁降级，也就是，**一个线程在获取写锁之后，在写锁释放前，可以再获取读锁。当写锁释放之后，线程持有的锁从写锁降级为读锁**。

WriteLock 写锁是排它锁，而读锁是共享锁，实现原理相对来说更加复杂。

StampedLock是对ReadWriteLock的进一步优化。在读锁和写锁的基础之上，StampedLock又提供了乐观读锁。实际上，乐观读锁并没有加任何锁。在读多写少的应用场景中，大部分读操作都不会被写操作干扰，因此，我们甚至可以将读锁也省略掉。只有验证读操作真正有被写操作干扰的情况下，线程再加读锁重新执行读操作。我们举一个例子解释一下。代码如下所示。
```java
public class Demo {
  private StampedLock slock = new StampedLock();
  private List<String> list = new LinkedList<>();

  public void add(int idx, String elem) {
    long stamp = slock.writeLock(); //加写锁
    try {
      list.add(idx, elem);
    } finally {
      slock.unlockWrite(stamp); //释放写锁
    }
  }

  public String get(int idx) {
    long stamp = slock.tryOptimisticRead(); //加乐观读锁
    String res = list.get(idx);
    if (slock.validate(stamp)) { //没有被写操作干扰
      return res;
    }

    // 有被写操作干扰，重新使用读锁，重新执行读操作
    stamp = slock.readLock(); //加读锁
    try {
      return list.get(idx);
    } finally {
      slock.unlockRead(stamp); //释放读锁
    }
  }
}
```

在上述代码中，tryOptimisticRead()获取的是乐观读锁，返回一个时间戳stamp。因为乐观读锁并非真正加锁，所以，乐观读锁并不需要解锁。在执行完读操作之后，我们只需要验证stamp是否有被更改过，如果stamp有被更改过，那么，就说明执行读操作期间，writeLock()函数有被执行，也就说明有对共享资源的写操作发生（也就是执行了add()函数），此时，之前得到的结果需要作废，我们使用读锁来重新获取数据。

## 死锁、活锁、饥饿
锁的使用可以解决多线程安全问题，但同时也会带来性能问题（加锁、解锁耗时）。实际上，除了性能问题之外，锁的使用还会导致其他问题，比如死锁、活锁、饥饿。

我们把死锁、活锁、饥饿这三个问题，统称为活跃性问题。实际上，活跃性问题是一个常见的问题，多线程、多进程、多机（分布式系统）请求共享资源（不一定是锁），都有可能出现活跃性问题。

死锁指的是多个线程互相等待对方持有的资源而导致线程无法继续执行的问题。我们举例来解释一下，如果线程t1执行Demo对象中的f()函数，线程t2执行同一个Demo对象中的g()函数，两个线程就有可能会发生死锁。线程t1持有lock1锁，同时请求lock2锁，线程t2持有lock2锁，同时请求lock1锁。两个线程将会一直阻塞等待对方持有的锁，无法继续执行。

对于上面这个例子，线程持有锁和等待锁的情况非常清晰，我们一眼就能看出代码是否存在死锁，但是，在很多情况下，死锁的检测并非如此简单。当多个线程竞争多个锁时，如果线程持有锁以及等待锁的情况非常复杂，那么，**计算机就需要一定的算法来检测死锁**。

死锁检测算法有很多，这里我们介绍一种比较简单的。我们将线程竞争锁的场景，抽象为有向图这种数据结构。线程抽象为图中的顶点。如果线程A等待线程B持有的锁，那么，我们就在线程A和线程B对应的节点之间添加一条有向边。在有向图构建好之后，**我们只需要使用DFS算法，检测有向图中是否存在环即可**。如果有向图中存在环，那么，这就说明多个线程之间存在锁的循环等待，即存在死锁。相反，如果有向图中不存在环，那么这就说明不存在死锁。

我们知道，大部分数据库都实现了死锁检测和解决。当数据库检测到死锁时，会终止循环等待锁的线程中的其中一个线程，让其释放所持有的锁，这样循环等待就被打破，其他线程便可以获取到锁，顺利往下执行。不过，死锁检测本身并不属于Java语言层面的工作，甚至JUC也并没有提供相应的解决方案。如果我们希望程序能自动进行死锁检测和解决，那么，我们需要自己编写相应的代码。

尽管在语言层面无法实现死锁的自动检测和解决，但是，当发生死锁时，我们仍然可以通过类似jstack这类工具，将线程的运行状态等重要信息打印出来，以便分析死锁发生的原因，并对代码做及时的调整。实际上，尽量避免编写引发死锁的代码，才是解决死锁问题的根本方法。那么，如何避免死锁呢？常用的方法有以下两个：**统一线程请求锁的顺序和避免线程持有锁并等待锁**。
* 避免死锁方式一：统一线程请求锁的顺序；如果两个线程采用相同的加锁顺序，那么，就可以避免发生死锁。如下代码所示，两个线程分别同时执行f()函数和g()函数，只有一个线程成功获取lock1锁，获取lock1锁的线程，进而又会成功获取lock2锁，因此，不会发生死锁。
* 避免死锁方式二：避免线程持有锁并等待锁；从死锁的定义，我们可以发现，线程持有一个锁的同时等待另一个锁是产生死锁的重要原因。**如果我们避免线程持有一个锁的同时等待另一个锁，便可以有效的解决死锁问题**。具体的做法是：**如果一个线程请求一个锁时，这个锁已经被其他线程持有，那么这个线程不再阻塞等待这个锁，而是释放掉所持有的所有锁，让其他线程先执行**。上述解决思路对应的代码实现如下所示。需要注意的时，synchronized无法像Lock那样支持非阻塞的加锁方式，因此，这种死锁的避免方式只能使用Lock来实现。

实际上，避免死锁的方式二（避免持有锁并等待锁）是存在问题的。如下图所示，在极端情况下，两个线程循环反复执行以下逻辑：线程t1获取lock1锁，线程t2获取到lock2锁。线程t1尝试获取lock2锁失败之后，释放掉lock1锁。与此同时，线程t2尝试获取lock1锁失败之后，释放掉lock2锁。然后，线程t1和t2再重复执行上述逻辑。**这就导致线程t1和t2一直循环加锁、尝试加锁、释放锁。我们把这种情况叫做活锁**。

死锁和活锁的区别有两点。**第一点，处于死锁状态的两个线程均处于阻塞状态，不消耗CPU资源。相反，处于活锁状态的两个线程，仍然在不停的执行加锁、尝试加锁、释放锁等代码逻辑，消耗CPU资源**。比起死锁，活锁的性能损耗更大。第二点，死锁发生的概率很低。在高并发情况下，两个线程频繁竞争执行临界区，才有可能发生死锁。不过，相对于死锁，活锁发生的概率更低。发生活锁，除了具备死锁发生的条件之外，还需要两个线程的执行过程非常同步，才能保证for循环一直不退出。

对于死锁和活锁的区别，我们举一个生活中例子，再形象地解释一下。在一个比较窄但允许两人同行的马路上，两个人相对而行，如果两个人相撞，那么，死锁的处理思路是：两个人僵持不动，谁都无法往前走。活锁的处理思路是：两个人都很客气的让路给对方，但是，两人同时移动到另一侧，又继续相撞，再移动回来又继续相撞，一直这样持续下去，那么就会发生活锁。从这个例子上，我们可以看出，活锁发生的概率是非常非常低的，两人的移动必须一直保持完全同步才可以，不然，很快就可以解锁。

尽管活锁出现的概率比较低，但一旦出现活锁，便会导致比较严重的问题：线程持续做无用功，业务代码无法执行，白白浪费CPU资源。因此，我们需要想办法解决活锁问题。实际上，解决的办法也很简单，我们只需要让线程在执行的过程中，暂停随机的一小段时间，打破两个线程的持续同步即可。对于代码实现方法来说，我们既可以在tryLock()之后添加sleep()语句，也可以将tryLock()函数改为带超时时间的tryLock()函数。示例代码如下所示。我们只给出了f()函数修改之后的代码，对于g()函数，修改方法类似，留给你自己实现。
```java
  public void f() {
    Random r = new Random();
    for(;;) {
      lock1.lock();
      try {
        boolean locked = false;
        try {
          locked = lock2.tryLock(r.nextLong()%10, TimeUnit.MILLISECONDS);
        } catch (InterruptedException e) {
          // log error and return
        }
        if (locked) { //两个锁都获取了
          try {
            //...业务逻辑...
            return;
          } finally {
            lock2.unlock();
          }
        } // else 没有获取lock2锁，执行finally，释放lock1锁
      } finally {
        lock1.unlock();
      }
    }
  }
```

**死锁和活锁是两个或两个以上线程的整体状态，饥饿是一个线程的状态**。死锁和活锁都会导致线程处于饥饿状态，迟迟获取不到锁。不过，导致线程饥饿的情况还有很多。在前面的章节中，我们也介绍过一些发生线程饥饿的情况。比如，对于非公平锁，因为新来的线程不需要排队，就可以插队直接竞争锁，所以，如果一直都有新来的线程竞争锁，那么，**排队的线程就有可能一直没有机会获取到锁，就会出现饥饿现象**。实际上，**自旋+CAS也算是非公平锁，源源不断的线程执行自旋+CAS，有可能会导致有些线程一直无法成功执行CAS而持续自旋，从而出现饥饿的现象**。除此之外，对于读写锁，如果等待队列的队首线程在等待写锁，即便其他线程持有读锁，新来请求读锁的线程也会直接排队，而非直接获取读锁，这样做就是为了避免等待写锁的线程迟迟获取不到写锁而饥饿。

实际上，死锁、活锁、饥饿这些活跃性问题一般都发生在高并发的极端情况下，比如流量突增。这也是活跃性问题很难测试、很难发现、很难重现的主要原因。活跃性问题一旦发生，就会导致非常严重的结果。比如，Tomcat等Web服务器采用线程池来多线程执行用户请求，当多个线程发生死锁、活锁、饥饿等活跃性问题时，这些线程处理的请求将会超时，并且这些线程迟迟无法响应其他请求，当发生活跃性问题的线程越来越多时，线程池中的可用线程就越来越少，大量用户请求需要排队等待线程执行，最终导致大量请求超时。

不过，话又说回来，在平时的业务开发中，我们很少会编写多线程代码，也很少会用到线程级的锁。比如，对于前面讲到的transfer()函数，在真实的业务开发中，数据一般存储在数据库中，transfer()函数直接操作数据库中的account表中的记录，实现两个账户中金额的加减。当transfer()函数操作account表时，其他进程也有可能正在操作Account表中相同的记录。**而单单使用线程级的锁，并不能解决进程间的互斥问题**。针对这种情况，我们一般会使用**数据库锁**（可以看做是进程级的锁）来解决。当执行转账业务时，transfer()函数使用数据库锁，先锁定Account表中的转账所涉及的两行记录，以免其他进程或线程操作这两行记录。

实际上，**相比线程级的锁，进程级的锁更容易引发死锁**。这是因为两个进程对应的程序有可能是两个不同的程序员开发的，甚至有可能是两个不同的团队开发的。一个团队并不不知道另一个团队所开发的程序，到底都用了哪些锁，以及如何使用的这些锁，因此，协调好统一的加锁顺序是比较困难的。这也是我在过往的开发中经常遇到的死锁的情况。

## CAS
CAS 指的是先检查后更新这类复合操作，英文翻译有多种：Compare And Set、Compare And Swap 或 Check And Set。

X86 提供的 CAS 指令为 cmpxchg 指令。

在单核计算机上，cmpxchg 指令是原子操作。尽管 cmpxchg 指令包含很多细分操作，看似是非原子的复合操作，但是，指令是 CPU 执行的最小单元，指令执行的过程不可中断。当多个线程共用一个 CPU 来交替执行指令时，只有当前线程执行完正在执行的指令（比如 cmpxchg 指令）之后，操作系统才可以调度 CPU 执行其他线程，其他线程是看不到 cmpxchg 指令执行的中间状态的。因此，cmxchg 在单核计算机上是原子操作。

不过，在多核计算机上，cmpxchg 指令是非原子操作。在多核计算机上，多个线程可以并行运行在多个 CPU 上，也就是说，多个线程可以并行执行 cmpxchg 指令，同时对同一个内存变量进行 CAS 操作，因此，cmpxchg 就不再是原子操作了。在多核计算机中，为了保证 cmpxchg 指令的原子性，我们需要在 cmpxchg 指令前加 LOCK 前缀，如下所示。
```shell
LOCK cmpxchg [目标操作数], [源操作数]
```

前面我们讲过很多锁，比如偏向锁、轻量级锁、自旋锁等等，这里我们再介绍另外两种锁：悲观锁和乐观锁，它们属于抽象的概念，并不是具体实现。乐观锁指的是我们乐观的认为代码不大可能存在资源竞争，大部分情况都不需要加锁。悲观锁指的是我们悲观的认为代码很有可能会出现资源竞争，绝大部分情况都需要加锁。**synchronized 或 Lock 可以用来实现悲观锁，自旋+CAS 可以用来实现乐观锁**。

悲观锁和乐观锁各有利弊。基于 synchornized 或 Lock 实现的悲观锁，等待资源而阻塞线程会导致内核态到用户态的上下文切换，带来性能损耗，但是，处于阻塞状态的线程不会被分配CPU时间片，不会浪费CPU资源。基于自旋+CAS实现的乐观锁，循环执行CAS，不需要阻塞线程，没有内核态到用户态的上下文切换带来的性能损耗，但是，线程一直处于运行状态，白白浪费CPU资源。因此，如果多线程竞争资源不激烈，那么，使用乐观锁来竞争资源更合适，如果多线程竞争资源比较激烈，那么，使用悲观锁来竞争资源更合适。

## 原子类
自旋+CAS可以替代锁用于资源竞争不激烈的场景。不过，相对于锁来说，自旋+CAS的代码实现比较复杂。我们需要先创建Unsafe对象，然后获取待更新变量的偏移位置，最后调用Unsafe对象的CAS方法来更新变量。为了方便开发，JUC提供了各种原子类，封装了对各种类型数据的自旋+CAS操作。

原子类主要依赖自旋+CAS来实现。原子类中的每个操作都是原子操作。在多线程环境下，执行原子类中的操作不会出现线程安全问题。根据处理的数据类型，原子类可以粗略地分为4类：基本类型原子类、引用类型原子类、数组类型原子类、对象属性原子类。

相对于 AtomicReference，AtomicStampedReference 增加了版本戳，主要是用来解决 CAS 的 ABA 问题。对于什么是 ABA 问题，我们举例解释一下。

实际上，引起执行结果出错的原因，就是CAS的ABA问题。线程1执行removeAtHead()函数，设置oldHead=A、nextNode=B，在执行CAS之前，线程2将head从A变为B、C，最后又变为A。尽管head仍为A，但链表的整体结构发生了变化。随后，当线程1执行CAS时，检查当前的head跟oldHead相等，仍然是A，错以为期间没有其他线程执行过addAtHead()和removeAtHead()函数，于是，成功执行CAS。

## 累加器
对于如下代码，如何将其改造成线程安全的
```java
public class Counter {
  private long sum;

  public long get() {
    return sum;
  }

  public void add(long value) {
    sum += value;
  }
}
```

为了让以上代码线程安全，我们只需要对add()函数进行处理。
* 第一种线程安全的实现方式是：对add()函数加锁，但是加锁会影响程序本身的性能。
* 第二种线程安全的实现方式是：使用自旋+CAS的方式，这样可以避免加锁，在低并发的情况下，这种实现方式的性能远优于加锁，但是，从零实现自旋+CAS，需要用到Unsafe类，风险比较大且编程复杂。
* 第三种线程安全的实现方式是：直接使用封装了自旋+CAS的原子类，相对于第二种实现方式，编程实现简单了许多。

这三种实现方式对应的代码如下所示。
```java
// 线程安全实现方式一：加锁
public void add_lock(long value) {
  synchronized (this) {
    sum += value;
  }
}

// 线程安全实现方式二：自旋+CAS
private static final Unsafe unsafe = Unsafe.getUnsafe();
private static final long sumOffset;
static {
  try {
    sumOffset = unsafe.objectFieldOffset
      (Counter.class.getDeclaredField("sum"));
  } catch (Exception ex) { throw new Error(ex); }
}

public void add_cas(long value) {
  boolean succeeded = false;
  while (!succeeded) {
    long oldValue = sum;
    succeeded = unsafe.compareAndSwapLong(
        this, sumOffset, oldValue, oldValue+value);
  }
}

// 线程安全实现方式三：原子类
private AtomicLong atomicSum = new AtomicLong(); //替代sum成员变量
public void add_atomic(int value) {
  atomicSum.addAndGet(value);
}
```

实际上，除了以上三种解决方法之外，针对累加这种特殊的业务场景，JUC还提供了专门的LongAdder累加器，它比AtomicLong原子类性能更高。在高并发的情况下，多线程同时执行add()函数，AtomicLong会因为大量线程不断自旋而性能下降，LongAdder却可以持续保持高性能。那么，如此高性能，LongAdder是如何做到的呢？

LongAdder的使用方法非常简单，但其底层实现原理却比较复杂。为了实现高性能累加，LongAdder的底层实现原理涉及**数据分片、哈希优化、去伪共享、非精确求和**等各种优化手段。接下来，我们就一一讲解一下这些优化手段。

## ThreadLocal
对于无锁编程，我们已经讲解了**CAS、原子类、累加器**，本节，我们讲解无锁编程中的最后一个知识点：**ThreadLocal**。我们知道，**共享变量是代码存在线程安全的根本原因之一**。在某些特殊的业务场景下，我们可以使用ThreadLocal线程局部变量替代共享变量，以实现在不需要加锁的情况下达到线程安全。

在Java中，我们可以将变量粗略的划分为两类：类的成员变量和函数内局部变量。**对于类的成员变量，当多个线程使用同一个对象时，对象中的成员变量就是共享变量，其作用域范围为多个线程均可见。多个线程竞争访问成员变量，就有可能存在线程安全问题**。对于函数内局部变量，每个线程在执行函数时，会在自己的栈上存储私有的局部变量，因此，函数内局部变量的作用域范围为单线程内可见。不仅如此，函数内局部变量仅限函数内可见，不同的函数之间不可以共享局部变量。

如果我们希望多个函数共享局部变量，那么，我们需要通过参数传递的方式来实现。
```java
public class UserController {
  private static final Logger logger =  LoggerFactory.getLogger(UserController.class);
  private UserService userService = new UserService();

  public long login(String username, String password) {
    // 创建traceId
    String traceId = "[" + System.currentTimeMillis() + "]";
    // 所有的日志都带有traceId
    logger.info(traceId + " login username=" + username);
    //...省略校验逻辑...
    return userService.login(username, password, traceId);//传递traceId
  }
}
```

上述代码存在的问题很明显，我们需要在每个函数中都定义traceId参数，导致非业务代码和业务代码耦合在一起。为了解决这个问题，JUC便提供了ThreadLocal，其作用域范围介于类的成员变量和函数内局部变量之间，它既是线程私有的，又可以多函数共享，这样既可以避免线程安全问题，又能避免变量在函数之间不停传递。

## 条件变量
互斥和同步是多线程要解决的两个核心问题。互斥依靠互斥锁来解决。同步依靠同步工具来解决，其中包括条件变量、信号量、CountDownLatch、CyclicBarrier等。

自旋并不会让线程进入阻塞状态。如果队列一直为空，那么线程将一直执行while循环，白白浪费CPU资源，甚至会让CPU使用率达到100%。为了减少对CPU资源的浪费，我们可以在while循环中调用sleep()函数，让线程睡眠一小段时间。如果在dequeue()函数执行sleep()函数的过程中队列变为非空，那么，dequeue()函数需要等待sleep()函数执行结束之后，才能返回数据。这就会导致程序响应不及时，性能下降。

那么，有没有什么方法既可以解决自旋浪费CPU资源问题，又能解决睡眠导致的响应不及时问题呢？答案是有的，那就是使用本节要讲的条件变量。条件变量是多线程中用来实现等待通知机制的常用方法。

粗略地讲，锁可以分为两种，一种是Java提供的synchronized内置锁，另一种是JUC提供的Lock锁。同理，条件变量也有两种，一种是Java提供的内置条件变量，使用Object类上的wait()、notify()等来实现，一种是JUC提供的条件变量，使用Condition接口上的await()、signal()等来实现。两种条件变量的使用方式和实现原理基本一致。

## 信号量、锁寸器和栅栏
信号量是并发编程中的一个重要概念。信号量用来限制临界区和共享资源的并发访问。互斥锁限制临界区和共享资源同时只能被一个线程访问。信号量限制临界区和共享资源同时只能被N个线程访问。因此，**信号量也可以看做是一种共享锁**，其底层也是基于AQS实现的。

CountDownLatch中文名称叫作锁存器，CountDownLatch的作用有点类似Thread类中的join()函数，用于一个线程等待其他多个线程的事件发生。对于join()函数来说，这里的事件指的是线程结束。对于CountDownLatch来说，这里的事件可以根据业务来定义。除此之外，使用join()需要知道被等待的线程是谁，而使用CountDownLatch则不需要。因此，CountDownLatch相对于join()函数来说，更加通用。

CyclicBarrier的中文名为栅栏，非常形象地解释了CyclicBarrier的作用，用于多个线程互相等待，互相等待的线程都就位之后，再同时开始执行。我们举个例子解释一下，如下代码所示。我们创建了一个parties为10的CyclicBarrier对象，用于10个线程之间互相等待。尽管这10个线程启动（执行start()函数）的时间不同，但每个线程启动之后，都会调用await()函数，将parties减一，然后检查parties是否为0。如果parties不为0，则当前线程阻塞等待。如果parties为0，则当前线程唤醒所有调用了await()函数的线程。

## 并发容器
为了方便开发，Java提供了很多容器，比如ArrayList、HashMap、TreeSet等，底层对数组、哈希表、红黑树等常用的数据结构进行了封装。在开发时，我们直接使用这些现成的容器即可，不需要重新从零去开发。对于这些封装了常用数据结构的容器，在多线程环境下，我们如何来保证它们的线程安全性呢？

依靠程序员自己去编写代码来维护容器的线程安全性，比如对操作加锁，一来耗费开发时间，二来性能没有保证。为了解决这个问题，JUC针对常用的容器对应开发了高性能的并发容器。在多线程编程中，我们直接使用这些现成的并发容器即可。

Java 容器分为 5 类
* List：ArrayList、 LinkedList 、 Vector（废弃）
* Stack：Stack（废弃）
* Queue：ArrayDeque、 LinkedList、 PriorityQueue
* Set：HashSet、 LinkedHashSet、TreeSet
* Map：HashMap、 LinkedHashMap、 TreeMap、HashTable（废弃）

为了更符合程序员的开发习惯，JCF将非线程安全容器和线程安全容器（也就是并发容器）分开来设计。在非多线程环境下，我们使用没有加锁的非线程安全容器，性能更高。例如，替代Vector，JCF设计了非线程安全的ArrayList。在多线程环境下，我们使用Collections工具类提供的并发方法（如synchronizedList()），将非线程安全容器（如List）转换为线程安全容器（如SynchronizedList）。

Java并发容器的底层实现原理非常简单，跟Vector、Stack、HashTable类似，都是通过对函数加锁来解决线程安全问题。我们拿SynchronizedList举例讲解，其源码如下所示。所有的函数都使用synchronized加了锁。实际上，我们可以使用ReentrantReadWriteLock替代synchronized来提高代码的并发性能。
```java
public static <T> List<T> synchronizedList(List<T> list) {
    return (list instanceof RandomAccess ?
            new SynchronizedRandomAccessList<>(list) :
            new SynchronizedList<>(list));
}

static class SynchronizedList<E>
    extends SynchronizedCollection<E> implements List<E> {
    final List<E> list;

    SynchronizedList(List<E> list) {
        super(list);
        this.list = list;
    }

    SynchronizedList(List<E> list, Object mutex) {
        super(list, mutex);
        this.list = list;
    }

    public boolean equals(Object o) {
        if (this == o)
            return true;
        synchronized (mutex) {return list.equals(o);}
    }
    public int hashCode() {
        synchronized (mutex) {return list.hashCode();}
    }
    public E get(int index) {
        synchronized (mutex) {return list.get(index);}
    }
    public E set(int index, E element) {
        synchronized (mutex) {return list.set(index, element);}
    }
    public void add(int index, E element) {
        synchronized (mutex) {list.add(index, element);}
    }
    public E remove(int index) {
        synchronized (mutex) {return list.remove(index);}
    }
    //...省略其他方法...
}
```

在上述并发容器的代码实现中，每个函数都加了锁，锁粒度大导致并发性能不高。于是，JUC便实现了一套更高性能的并发容器。JUC并发容器之所以比Java并发容器性能更高，是因为JUC利用分段加锁、写时复制、无锁编程等技术对并发容器做了全新的实现，并非像Java并发容器那样只是对原有Java容器简单加锁。当然，从代码实现复杂度上，JUC并发容器的代码实现要比Java并发容器的代码实现要复杂得多。

## 写时复制
写时复制是一个比较通用的技术，可以应用于很多技术场景中。当应用于并发容器中时，写时复制指的是，当对容器进行写操作（这里的写可以理解为“增、删、改”）时，为了避免读写操作同时进行而导致的线程安全问题，我们将原始容器中的数据复制一份放入新创建的容器，然后对新创建的容器进行写操作中，而读操作继续在原始容器上进行。这样读写操作之间便不会存在数据访问冲突，也就不存在线程安全问题。当写操作执行完成之后，新创建的容器替代原始容器，原始容器便废弃。

## 阻塞等待
阻塞并发队列具有两个特点，第一个是线程安全，也就是名称中“并发”的含义，第二个是支持读阻塞或写阻塞，也就是名称中“阻塞”的含义。读阻塞指的是，当从队列中读取数据时，如果队列已空，那么读操作阻塞，直到队列有新数据写入，读操作才成功返回。写阻塞指的是，当往队列中写入数据时，如果队列已满，那么写操作阻塞，直到队列重新腾出空位置，写入操作才成功返回。

JUC提供的阻塞并发队列有很多，比如ArrayBlockingQueue、LinkedBlockingQueue、LinkedBlockingDeque、PriorityBlockingQueue、DelayQueue、SynchronousQueue、LinkedTransferQueue。接下来，我们讲解一下这些阻塞并发容器的用法和实现原理。

## 分段加锁
HashMap是在开发中经常用到的容器，但是，它不是线程安全的，只能应用于单线程环境下。在多线程环境下，Java提供了线程安全的HashTable、SynchronizedMap，但是，两者因为采用粗粒度锁来实现，并发性能不佳。于是，JUC便开发了ConcurrentHashMap，利用分段加锁等技术来提高并发性能。

实际上，HashTable和SynchronziedMap在本质上是一样的，都是采用简单粗暴的方式（即所有的函数都进行加锁）来解决线程安全问题，因此，并发性能欠佳。Java之所以废弃HashTable，引入SynchronizedMap，主要是为了让JCF框架的类结构更加清晰，将线程安全容器和非线程安全容器分离，使线程安全容器通过统一的方式（Collections的synchronizedXXX()方法）来创建。

对于ConcurrentHashMap，我们又可以分为JDK7版本的ConcurrentHashMap和JDK8版本的ConcurrentHashMap。这两个版本的ConcurrentHashMap的实现方式有比较大的区别，比如，JDK8版本的ConcurrentHashMap的分段加锁粒度更小、并发度更高，扩容方式有所不同，size()函数实现更加高效等等。在本节中，我们仅对JDK8版本的ConcurrentHashMap的实现原理做讲解。对于JDK7版本的ConcurrentHashMap的实现原理，留给你自己来分析。

实际上，ConcurrentHashMap提高并发度的核心方法就是分段加锁。在HashTable或SynchronziedMap中，table数组上只有一把锁，所有的读写操作都争抢这一把锁。而在ConcurrentHashMap中，table数组被分段加锁，如果table数组的大小为n，那么就对应存在n把锁。table数组中的每一个链表独享一把锁，不同链表之间的操作可以多线程并行执行，互不影响，以此来提高ConcurrentHashMap的并发性能。

## 线程状态
线程在执行synchronized阻塞等待锁时，对应的线程状态为BLOCKED，而线程在执行Lock锁的lock()函数阻塞等待锁时，对应的线程状态为WAITING。同样是阻塞等待锁，为什么对应的线程状态却是不同的呢？带着这个问题，我们来开始本节的学习。

不同操作系统定义的线程状态略有不同，拿 Linux 举例，其定义的线程状态如下所示
* NEW：新创建的线程在没有调用 start 函数前，处于 NEW 状态
* READY：线程一切就绪，等待操作系统调度，也就是等待 CPU 时间片
* RUNNING：线程正在使用 CPU 时间片执行程序
* WAITING：：线程在等待I/O读写完成、等待获取锁、等待时钟定时到期（调用sleep()函数）等等，总之，等待其他事件发生之后，线程才能被调度使用CPU，此时，线程的状态就是WAITING状态。也就是说，只要线程不占用CPU，并且不等待CPU（非READY），那么线程就处于WAITING状态。
* TERMINATED：线程终止状态

尽管Java线程是基于内核线程模型实现的，即一个Java用户线程对应一个操作系统内核线程，但是，Java线程没有直接使用操作系统定义的线程状态，一方面是因为Java是跨平台语言，不同操作系统定义的线程状态不同，另一方面是因为应用层关注的线程状态，跟操作系统关注的线程状态是不同的，Java线程定义的状态能更清晰表示程序在应用层的执行情况。Java定义的线程状态如下所示。
* NEW：Java线程中NEW状态的含义，跟操作系统线程中NEW状态的含义相同。
* RUNNABLE：在应用层，我们不需要关注程序是正在等待CPU时间片，还是正在使用CPU时间这些操作系统才需要关注的细节，我们只需要知道程序正在执行就可以了，因此，Java并没有区分READY和RUNNING这两种线程状态，这两种线程状态统称为RUNNABLE。实际上，除了包含READY和RUNNING之外，RUNNABLE还包含操作系统线程状态中的部分WAITING状态。待会我们再专门解释这一点。
* WAITING：这里的WAITING状态跟操作系统线程中的WAITING状态不同。只有执行一些跟线程有关的特殊函数时，线程才会进入WAITING状态。这些特殊函数就包含Object.wait()、Thread.join()、Unsafe.park()。
* TIMED_WAITING：跟上面的WAITING状态类似，也是只有当执行一些跟线程有关的特殊函数时，线程才会进入TIMED_WAITING状态。这些特殊函数就包括Object.wait(long timeout)、Thread.join(long timeout)、Unsafe.parkNanos(long timeout)、Thread.sleep(long timeout)，这些函数均带有超时时间。
* BLOCKED：只有两种情况会使线程进入BLOCKED状态，一种情况是线程执行synchronized语句，阻塞等待获取锁，另一种情况是线程执行Object.wait()后被notify()或notifyAll()唤醒，再次阻塞等待获取锁。
* TERMINATED：Java线程中TERMINATED状态的含义，跟操作系统线程中TERMINATED状态的含义相同。

## 线程中断
如何安全的提前终止线程，避免突然终止业务逻辑而导致的数据不一致，资源得不到回收等问题。

基于标志终止线程：自己定义标志变量

基于中断终止线程：直接使用线程内部提供的中断标志位即可

基于中断异常终止线程：当我们通过interrupt()向某个线程发起中断请求时，如果这个线程正在执行阻塞函数，那么，它将无法响应中断请求（比如Thread.sleep()），也就无法及时终止线程。对于这个问题，该如何解决呢？实际上，大部分阻塞函数在设计实现时都已经考虑到了这个问题。当阻塞函数接受到中断请求之后，会停止执行并抛出InterruptedException异常，我们可以基于中断异常来终止线程，

我们知道，操作系统中也有中断，那么，本节所讲的Java中断和操作系统中断有什么区别和联系呢？

从上述对Java中断的讲解，我们可以发现，Java中断完全由Java语言独立实现，并不依赖操作系统中断。Java中断和操作系统中断是两回事。Java中断用来中断线程，操作系统中断用来中断CPU。不过，从功能上来讲，它们都是用来打断某个正在执行的任务，并且在实现原理上也大同小异。

前面已经讲解过Java中断的实现原理了，现在我们再来看下操作系统中断的实现原理。CPU在执行指令的过程中，每当一个CPU周期执行完成之后，就会去中断寄存器中检查是否有中断请求，如果有中断请求，那么，CPU根据中断请求编号，在事先设置好的中断向量表中，查找对应的中断处理程序入口地址，然后，跳转去执行对应的中断处理程序。一般来讲，常用的操作系统中断有：I/O中断（响应鼠标、键盘、磁盘等I/O设备的输入）、时钟中断、异常（比如缺页异常）、系统调用中断等等。有关操作系统中断更详细的讲解，你可以查阅操作系统相关的书籍。

## 线程池
在很多框架和系统（比如Tomcat、Dubbo RPC等）中，线程的创建和管理基本上都是由线程池负责的。在使用这些框架和系统时，我们常常需要设置线程池参数，比如线程池大小。线程池参数设置的是否合理直接影响了硬件资源的利用率和系统的运行性能。线程池到底开多大才合适呢？

线程池是池化技术的一种，常见的池化技术还有数据库连接池、对象池等，池化技术用来避免资源的频繁创建和销毁，以此提高资源的复用率。

当我们需要使用线程池执行任务时，我们只需要将待执行的任务，封装成Runnable对象，传递给execute()函数即可。execute()函数全权负责任务的执行。实际上，线程池在创建时并不会事先把所有的线程创建好。线程池中的线程是在有任务需要执行时才创建。当任务到来时，线程池可能处于不同的状态，进而对应不同的处理方式，如下所示。
1. 检查核心线程池是否已满，如果未满，则创建核心线程执行任务。
2. 如果核心线程池已满，那么再检查等待队列是否已满，如果等待队列未满，则将任务放入等待队列。
3. 如果等待队列已满，则再检查非核心线程池是否已满，如果未满，则创建非核心线程执行任务。
4. 如果核心线程池、非核心线程池、等待队列都满，则按照拒绝策略对任务进行处理。

线程池大小应该设置为多少是开发和面试中经常遇到的一个问题。**对于CPU密集型程序，对应的线程池不需要太大，跟可用CPU核数相当或稍大即可。这样便可以充分的利用CPU资源。对于I/O密集型程序，因为程序的大部分时间都在执行I/O操作，所以，CPU利用率很低。为了提高CPU的利用率，我们可以将线程池适当开大点，以便多个线程轮流使用CPU**。那么，既非CPU密集又非I/O密集的程序，对应的线程池大小又该如何设置呢？有没有具体的计算机公式可以给出线程池大小的确切值呢？

从理论上来讲，确实存在这样的计算公式。假设通过监控统计，我们得知线程池所执行的任务的平均CPU耗时为cpu_time毫秒，平均I/O耗时（确切的讲应该为非CPU耗时，但是，大部分非CPU耗时一般都花费在I/O上，因此，这里就直接使用I/O耗时代指非CPU耗时）为io_time毫秒，那么，线程池大小的计算公式如下所示。以下是针对单核CPU的计算公式，如果CPU核有N个，那么，我们只需要在计算结果上乘以N，便是最终线程池的大小。除此之外，以下公式计算得出的线程池大小指的是CPU利用率100%时对应的线程池大小。
```shell
pool_size = (cpu_time + io_time) / cpu_time
```

实际上，上述计算公式的合理性还有两个前提，一是没有瓶颈操作，即各个操作不会随着线程的增加而性能降低；二是没有瓶颈资源，即各个资源的数量满足所有线程的需求。

我们拿Redis举例解释瓶颈操作。尽管Redis执行命令这一任务是I/O密集型的，根据上述公式，理应将线程池开大点才能充分利用CPU资源，但是，在Redis执行命令的过程中，I/O操作是瓶颈操作。尽管我们可以将线程池开的很大，让CPU利用率高达100%，但命令的执行都阻塞在I/O操作上，整体的执行效率并不会提高。这时，我们就要重点关注I/O的利用率，而非CPU的利用率。如果单线程执行命令就可以让I/O负荷达到100%，我们又何必使用多线程呢？

我们拿数据库连接举例解释瓶颈资源。如果任务的执行依赖数据库，数据库连接通过数据库连接池来管理，假设数据库连接池的大小为N，那么，当线程数大于N时，数据库连接就成了瓶颈资源，多出来的线程需要等待数据库连接，整体的执行效率也不会提高。对于存在瓶颈资源的任务来说，在计算或者估计线程池大小时，我们不能再以CPU利用率100%为目标，而是以充分利用瓶颈资源为目标。也就是说，线程池大小应该设置为跟数据库连接池大小相当才算合理。

## 线程执行框架
实际上，上一节中讲到的ThreadPoolExecutor、Executors隶属于线程执行框架。线程执行框架提供了一系列类，封装了线程创建、关闭、执行、管理等代码逻辑，这样做一方面实现了业务逻辑与非业务逻辑的解耦，另一方面方便代码复用，开发者不再需要编写创建线程、启动线程等代码逻辑。如何获取在一个线程中获取另一个线程的运行结果？