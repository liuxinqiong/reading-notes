虽然以 Java 为切入点，但还是有很多共性值得学习的。

## 从代码执行看程序本质
CPU只认识机器指令（也叫做CPU指令、机器码），使用高级语言（Python、C++、Java等）编写的代码，需要编译（编译的意思实际上就是“翻译”）为机器指令之后，才能被CPU执行。不同编程语言的编译过程是不同的。根据编译过程的不同，我们将编程语言分为三类。
* **编译型语言**：对于类似C++这样的编译型语言，代码会事先被编译成机器指令（可执行文件），然后再一股脑儿交给CPU来执行。在执行时，CPU面对的是已经编译好的机器指令，直接逐条执行即可，执行效率比较高。但因为每种类型的CPU（比如Intel、ARM等）支持的CPU指令集不同，并且程序还有可能调用特定操作系统提供的API，所以，编译之后的可执行文件只能在特定的操作系统和机器上执行，换一种操作系统或机器，编译之后的可执行文件就无法执行了。
* **解释型语言**：对于类似Python这样的解释型语言，代码并不会被事先编译成机器指令，而是在执行的过程中，由Python虚拟机（也叫做解释器）逐条取出程序中的代码，然后编译成机器指令，交由CPU执行，执行完成之后，再取出下一条代码，重复上述的编译、执行过程。这种一边编译一边执行的过程，叫做解释执行。相对于使用编译型语言编写的代码，使用解释型语言编写的代码的可移植性更好。程序在执行的过程中，虚拟机可以根据当前所在机器的CPU类型和操作系统类型，翻译成不同的CPU指令。这样，同一份代码就可以运行在不同类型的机器和不同类型的操作系统上。这就是常听到的“一次编写，多处运行”。
* **混合型语言**：Java语言比较特殊，它属于混合型语言，既包含编译执行也包含解释执行。Java编译器会先将代码（.java文件）编译成字节码（.class文件）而非机器码。字节码是Java代码和机器码之间的一种中间状态，它既跟平台无关，又可以快速地被翻译成机器码。编译之后的字节码在执行时，仍然是解释执行的，也就是字节码被逐行读出，然后翻译成机器码，再交给CPU执行。只不过，从字节码到机器码的翻译过程，比从高级语言到机器码的翻译过程，耗时要少。这样既保证了Java代码的可移植性（同一份代码可以运行在不同的CPU和操作系统上），又避免了解释执行效率低的问题。实际上，在解释执行的过程中，Java虚拟机会将**热点字节码**（反复多次执行的代码，类似缓存中的热点数据），编译成机器码缓存起来，以供反复执行，这样就避免了热点字节码需要反复编译，进一步节省了解释执行的时间。这就是著名的**JIT编译**（Just In Time Compile，即时编译）

CPU、操作系统、虚拟机在程序的执行过程中，都扮演了什么角色。
* CPU：用来执行编译好的机器指令
* 操作系统：管理硬件资源和调度程序的执行。打个比如，CPU等硬件就好比车间中的机器，工人就好比操作系统，一个个程序就像一个个待执行的任务。工人（操作系统）调度机器（CPU等硬件）来执行各个任务（程序）。除此之外，操作系统还担当了**类库的作用**。对于通用的功能代码，比如读写硬盘等，没必要在每个程序中都从零编写一遍。操作系统将这些通用的功能代码，封装成API（专业名称叫做系统调用），供我们在编写应用程序时直接调用。也就是说，在应用程序的执行过程中，CPU可能会跳转去执行操作系统中的某段代码。
* 虚拟机：简单理解就是万能的中间层，CPU执行虚拟机代码将应用程序的字节码翻译成CPU指令，放到固定的内存位置，再通过修改IP寄存器（IP寄存器存储CPU将要执行的指令位置），引导CPU执行这块内存中存储的CPU指令。

关于虚拟机的作用，可以对比一下C++、Python、Java这三种语言的编译命令和执行命令
```shell
// C++
$ g++ helloword.cpp -o helloworld
$ ./helloword

// Python
$ python helloworld.py

// Java
$ javac HelloWorld.java
$ java HelloWorld
```

我们可以发现，C++编译之后的代码直接就可以执行，而Python和Java代码的执行，需要依赖其他程序（即虚拟机），表现在命令行中就是执行命令前面有python、java字样。

站在操作系统和CPU的角度，Java程序编译之后的字节码跟虚拟机合并在一起，才算是一个完整的程序，才相当于C++编译之后的可执行文件。CPU在执行程序员编写的代码的同时，也在执行虚拟机代码，并且是先执行虚拟机代码，然后才引导执行程序员编写的代码。

CPU指令、汇编语言、字节码
* CPU 指令：一条CPU指令包含的信息主要有：操作码、地址、数据三种，分别指明所要执行的操作、数据来源或去向、数据本身。一组CPU指令的集合，叫做指令集。常见的指令集有X86、X86-64、ARM、MIPS等。不同的CPU支持的指令集可能不同（Intel CPU支持X86指令集，ARM CPU支持ARM指令集）。
* 汇编语言：汇编语言由一组汇编指令构成。汇编指令跟CPU指令一一对应，但汇编指令采用字符串而非二进制数来表示指令，因此，其可读性好很多。实际上，CPU指令和汇编指令之间的关系，就类似IP地址和域名之间的关系。IP地址和域名一一对应，域名的可读性比IP地址好。程序员使用汇编语言编写的代码，需要经过编译，翻译成机器码才能被CPU执行。C/C++语言的编译过程，实际上也包含汇编这一过程。编译器会先将C/C++代码编译成汇编代码，然后再汇编成机器码。
* 字节码：Java语言是跨平台的，程序员编写的代码，在不需要任何修改的情况下，就可以运行在不同的平台（不同的操作系统和CPU）上。字节码诞生的目的是，克服解释型语言解释执行速度慢的缺点（字节码是介于高级语言和机器码之间的形态，比高级语言解释执行更快）。

> 之所以Java语言能做到跨平台（操作系统和CPU），最根本原因是有虚拟机的存在。Java代码跟平台无关，字节码跟平台无关，在编译执行过程中，**总要有一个环节跟平台有关**，不然，跟平台有关的、最终可以被CPU执行的机器码从何而来呢。俗话说的好，哪有什么岁月静好，只是有人帮你负重前行。跟平台有关的环节就是将字节码解释执行的环节，而这个环节的主导者就是虚拟机。

class文件为什么叫字节码？跟字节（byte）有什么关系呢？从上述字节码中，我们可以发现，有些字节码指令包含操作码和操作数两部分，而有些只包含操作码这一部分。因为操作码长度为一个字节，所以，这种指令格式被叫做字节码。从另一个角度，我们也可以得知，**字节码的操作码类型不超过256个（2^8）**。相对于CPU指令和汇编指令，字节码指令少很多。这是因为字节码指令相对于CPU指令来说，抽象程度更高。一条字节码指令完成的逻辑，比一条CPU指令完成的逻辑，更加复杂。

不管是使用哪种类型的编程语言（编译型、解释型、混合型）编写的代码，也不管经历什么样的编译、解释过程，最终交由CPU执行的都是机器码。

讲到CPU，我们就不得不讲一下寄存器。
* 内存的读写速度比起CPU指令的执行速度要慢很多，CPU在执行指令时，如果依赖内存来存储计算过程中的中间数据，那么，CPU将总是在等待读写内存操作的完成，势必会影响CPU整体的计算速度。为了解决这个问题，于是，计算机科学家便发明了寄存器。
* 寄存器读写速度非常快，能够跟CPU指令的执行速度相匹配。所以，内存中的数据会先读取到寄存器中再参与计算。但问题是数据在计算前需要先从内存读取到寄存器，计算之后存储在寄存器中的结果需要再写入内存，因此，寄存器的存在并没有避免掉内存的读写，那么，使用寄存器是不是多此一举呢？实际上，尽管最初数据来源于内存，最后计算结果也要写入内存，**但中间的计算过程涉及到一些临时结果的存取，都可以在寄存器中完成**，不需要跟非常慢速的内存进行交互。顺便说一句，计算机为了提高CPU读写内存的速度，还引入了**L1、L2、L3这三级缓存**。

为了做到能让CPU高速访问，寄存器的硬件设计比较特殊（**高成本、高能耗**），且相对于内存来说与CPU距离更近（寄存器直接跟CPU集成在一起），这些也决定了寄存器的个数不会很多。不同的CPU包含的寄存器会有所不同。常见的寄存器有以下几类。

**通用寄存器**：AX，BX，CX，DX，一般用来存储普通数据。AX，BX，CX，DX这四种通用寄存器的用途有所区别，比如AX是累加器

**指针寄存器**：BP，SP，SI，DI，IP，BP（Base Pointer Register）和SP（Stack Pointer Register）是用于存储栈空间地址的寄存器，SP存储栈顶地址，BP比较特殊，一般存储栈中一个栈帧的栈底地址。SI（Source Index Register）源地址寄存器和DI（Destination Index Register）目的地址寄存器，分别用来存储读取和写入数据的内存地址。IP（Instruction Pointer Register）指令指针寄存器用来存储下一条将要执行的指令的内存地址的一部分。

**段寄存器**：CS，DS，SS，程序由一组指令和一堆数据组成。指令存储在一块被称为代码段的内存中，由CPU逐一读取执行。数据存储在一块被称为数据段的内存中。指令执行的过程中，CPU会操作（读取或写入）这块内存中的数据。

CS（Code Segment Register）代码段地址寄存器存储了代码段的起始地址。上文中讲到，IP寄存器中存储的是下一条将要执行的指令的内存地址的一部分。CS和IP两个寄存器中存储的内容如下计算，才能得到一个真正的物理内存地址。
```
物理内存地址 = 段地址（如CS） * 16 + 偏移地址（如IP）
```

我们拿8086 CPU（早期的16位的X86 CPU）举例解释。8086 CPU具有20位地址总线，支持1MB内存的寻址能力。然而16位的IP寄存器只能存储64K（2^16）个内存地址，一个字节占一个地址，因此，16位的IP寄存器只能支持64KB大小内存的寻址。为了扩大寻址能力，满足X86 CPU 1MB内存的寻址能力，计算机使用段地址和偏移地址相结合的方式来确定一个物理内存地址。通常，我们把CS寄存器和IP寄存器统称为**PC寄存器**，实际上，PC寄存器是一个抽象概念，并不真正存在这种寄存器。
```
最大段地址=2^16-1
最大偏移地址=2^16-1
最大寻址=(2^16-1)*16 + (2^16-1) = 2^20+2^16-16-1 > 2^20=1M
```

拓展：为什么 32 位系统最大寻址能力是 4GB 呢。
1. 2^32 换算成 GB 是 0.5GB
2. 那为什么是 4GB，因为上面的结果是 0.5GB 个地址，**一个地址对应的内存单元的大小通常是一个字节**，因此 0.5*8 = 4GB
3. 由于一些地址需要用于系统的特殊用途，比如硬件设备的映射、BIOS 等系统保留区域等，实际上可供用户程序直接使用的内存空间会小于理论最大值。

DS（Data Segment Register）数据段地址寄存器存储了数据段的起始地址，它跟DI或SI结合才能确定一个数据段中的内存地址。SS（Stack Segment Register）栈寄存器存储的是栈的起始地址，它跟SP结合确定栈顶的内存地址，跟BP结合确定栈中某个中间位置的内存地址。

**指令寄存器**：IR（Instruction Register）指令寄存器用来存放当前正在执行的指令。指令为一串二进制码，指令译码器需要从指令中解析出操作码和操作地址或操作数。所以，指令需要暂存在指令寄存器中等待译码处理。

**标志寄存器**：FR（Flag Register）标志寄存器，也叫做程序状态字寄存器（Program Status Word，PSW）。在这个寄存器中，每一个二进制位记录一类状态。比如cmp指令的运算结果会存储在ZF零标志位或CF进位标志位中。当cmp指令执行完成之后，CPU读取标志寄存器中的ZF位或CF位，便可以得到cmp指令的执行结果。

> 注意，以上讲解的是16位的寄存器，32位的寄存器名称在对应的16位寄存器名称前加E（例如EAX，EBP，ESP，EIP），64位的寄存器名称在对应的16位寄存器名称前加R（例如RAX，RBP，RSP，RIP）。

CPU指令执行的具体流程
1. 对于编译型语言，操作系统会把编译好的机器码，加载到内存中的代码段，将代码中变量等数据加载到内存中的数据段，并且设置好各个寄存器的初始值，如DS、CS等。IP寄存器中存储代码段中第一条指令的内存地址相对于CS的偏移地址。
2. CPU根据PC寄存器（CS寄存器和IP寄存器的总称）存储的内存地址，从对应的内存单元中取出一条CPU指令，放到IR指令寄存器中，然后将IP寄存器中的地址加上offset，从而得到下一条指令的内存地址。对于16位CPU，一条指令的长度为2字节，一个字节占一个地址，因此，offset为2。同理，对于32位和64位CPU，一条指令的长度分别为4字节和8字节，因此，offset分别为4和8。
3. 一条指令执行完成之后，再通过PC寄存器中的地址，取下一条指令继续执行。循环往复，直到所有的指令都执行完成。
4. 对于解释型或混合型语言，操作系统将虚拟机本身的机器码，加载到内存中的代码段。CPU执行虚拟机代码，将程序编写的代码解释为机器码，并放入某块内存中，然后将PC寄存器的地址设置为这块内存的首地址，于是，**CPU就被虚拟机引导去执行程序员编写的代码了**。

## 从 CPU 角度看基础语法
对于绝大部分编程语言来说，基本语法无外乎这样几种：变量、类型、数组、运算（赋值、算术、逻辑、比较等）、跳转（条件、循环）、函数，而其他语法（比如类、容器、异常等）在CPU眼里只不过是语法糖。

### 变量
内存被划分为一个个的内存单元（一个内存单元为1个字节大小）。每个内存单元都对应一个内存地址，方便CPU根据内存地址来读取和操作内存单元中的数据。

对于高级语言来说，内存地址可读性比较差，所以，就发明了变量这种语法。变量可以看看作是内存地址的别名。内存地址和变量的关系，跟IP地址和域名的关系类似。编译器在将代码编译成机器码时，会将代码中的变量替换为内存地址。

不同的变量有不同的作用域（也可以理解为生命周期）。不同作用域的变量，分配在代码段中的不同区域。不同的区域有不同的内存管理方式。不同语言对数据段的分区方式会有所不同，但又大同小异，**常见的分区有栈、堆、常量池等**。

笼统来讲
* 栈一般存储作用域为“函数内”的数据，如函数内的局部变量、函数参数等，它们只在函数内参与计算，函数结束之后，就不再使用了，同时，所占用的内存就可以释放，以供其他变量重复使用。
* 堆一般存储作用域不局限于“函数内”的数据，如对象，只有在程序员主动释放（如C/C++语言）或虚拟机判定为不再使用（如Java语言）时，对象对应的内存才会被释放。
* 常量池一般存储常量等，常量的生命周期跟程序的生命周期一样，只有在程序执行结束之后，对应的内存才会被释放。

对数据段进行分区，是为了方便管理不同生命周期的变量。而之所以不同的变量要设置不同的生命周期，是为了能够有效的利用内存空间，方便在变量生命周期结束之后，对应的内存能够快速地被回收，以供重复使用。

### 数组
使用数组，我们可以定义一块连续的内存空间。

使用 Java 举例
```java
public class Test {
  public void demo() {
    int[] a = new int[10];
    a[3] = 92;
    System.out.println(a[3]);
  }
}
```

在上述示例代码中，a是一个局部变量，存储在栈上。“int[] a = new int[10]”这条语句表示，在堆上申请一块能够存储下10个int类型数据的连续内存空间，并将这块内存空间的首地址存储在变量a所对应的内存单元中。

当通过下标来访问数组中的元素时，如语句“a[3]=92”，编译器将这条语句分解为多条CPU指令，先通过变量a中存储的首地址和如下寻址公式，计算出下标为3的元素所在的内存地址，然后将92写入到这个内存地址对应的内存单元。
```
a[i]的内存地址=
 a中存储的值（也就是数组的首地址）+ i*4(4表示4字节，也就是数据类型的长度)
```

在Java语言中，new申请的数组存储在堆上，首地址赋值给栈上的变量。而在有些语言中，比如C语言，“数组”语法更加灵活，数组既可以申请在堆上，也可以申请在栈上。
```c
int a[100]； //数组在栈中，可以直接类似a[2]=92;这样使用了
int a[100] = malloc(sizeof(int)*100); //数组在堆中
```

实际上，如果你了解JavaScript语言，你还会发现，JavaScript中的数组还可以存储不同类型的数据，数组中存储的是不同类型的数据，因此，上文中提到的寻址公式就无法工作了，那JavaScript是如何通过下标定位到元素的内存地址的呢？

实际上，不同编程语言中的数组，其在内存中的存储方式并不完全一样，也并非只有在上文中讲到的“在一块连续的内存空间中存储相同类型的数据”这样一种存储方式。

### 类型
在CPU眼里，是没有类型这一概念的。任何类型的数据，在CPU眼里都只是一串二进制码。**一串二进制码是表示为字符串还是整型数又或者浮点数，完全看代码是如何定义这块内存的类型的**。

引入类型的目的是，方便程序员编写正确的代码，避免错误的赋值操作。不同的编程语言具有不同的类型系统。根据变量的类型是否可以动态变化和类型检查发生的时期，我们将类型系统分为静态类型和动态类型。

### 运算
在编程语言中，常见的运算类型有以下5种：
1. 算术运算，比如加、减、乘、除；
2. 关系运算，比如大于、小于、等于；
3. 赋值运算，比如a=5；
4. 逻辑运算，比如&&，||，！；
5. 位运算，比如&，|，~，^，>>, <<；

以上绝大部分运算在CPU中都有对应的指令。不过，不同类型的指令对应的电路逻辑不同，所以，执行花费的时间也不同，比如**位运算会比较快**，乘法运算、除法运算会比较慢。

### 跳转
程序由顺序、选择（或叫分支、条件）、循环三种基本结构构成，其中，选择和循环又统称为跳转。

### 函数
编写函数是代码模块化的一种有效手段。几乎所有的编程语言都会提供函数这种语法。函数的底层实现，相对于前面讲到的几种语法的底层实现，要复杂一些。函数底层实现依赖一个非常重要的东西：栈。就是我们前面讲到的，**用来保存局部变量、参数等的内存区域**。因为这块内存的访问方式是先进后出，符合栈这种数据结构的特点，所以，被称为栈。

为什么函数底层实现需要用到栈呢？

每个函数都是一个相对封闭的代码块，其运行需要依赖一些局部数据，比如局部变量等。这些数据会存储在内存中。当函数A调用另一个函数B时，CPU会跳转去执行函数B的代码。函数B的执行又会涉及一些局部变量等，这些数据也会存储在内存中（紧挨着函数A的内存块）。以此类推，当函数B调用另一个函数C时，CPU又会跳转去执行函数C的代码。函数C的内存块会紧邻函数B的内存块。

当函数C执行完成之后，函数C中的局部变量等数据不再被使用，对应的内存块也可以释放，并且，CPU返回执行函数B的代码。函数B对应的内存块又开始被使用。同理，函数B执行完成之后，其对应的内存块也会被释放，CPU返回执行函数A的代码。函数A对应的内存块又开始被使用。

我们可以发现，在函数调用的过程中，同一时间只有一个函数的内存块在被使用，并且内存块被释放的顺序为“先创建者后释放”。内存中数据的操作方式符合栈的特点：“只在一端操作、先进后出”。所以，**编译器把函数调用所使用的整块内存，组织成栈这种数据结构，称为函数调用栈。我们把其中每个函数对应的内存块叫做栈帧**。

当通过函数调用，程序运行到一个新的函数时，**编译器会在栈中创建一个栈帧（实际上就是申请一个内存块）**，存储这个函数的局部变量等数据。当这个函数执行完毕返回上层函数时，栈顶栈帧出栈（也就是释放内存块），此时，新的栈顶栈帧为返回后的函数对应的栈帧。

了解了函数调用栈的大体结构之后，我们思考以下几个问题：
* 除了保存局部变量之外，栈帧中还保存哪些其他数据？栈帧中依次保存：前一个栈帧的帧底地址，参数，局部变量，返回地址。保存前一个栈帧的帧底地址的目的是，方便当前函数执行完成之后，rbp指针重新指向前一个栈帧的栈底。保存返回地址的目的是，方便当前函数执行完成之后，返回到上层函数继续执行。
* 上一节讲到的SP寄存器和BP寄存器具体用在哪里？SP寄存器存储栈顶地址，方便将新数据压入栈。BP寄存器存储的是当前栈帧帧底地址，方便基于这个地址的偏移来访问参数、局部变量。
* CPU在执行完某个函数之后，如何知道应该回到上层函数的哪处再继续执行？在通过callq指令调用函数时，callq指令会将当前的rip寄存器中的内容（callq指令的下一条指令的内存地址，即返回地址）存储在栈帧的最顶端。当被调用的函数执行完之后，被调用函数的栈帧释放，最后调用retq指令（相当于popq %rip），将返回地址重新赋值给rip，CPU就可以从函数中callq指令的下一条指令继续执行了。

## 为什么java引用与 c 中指针更安全
Java中的数据类型可以分为两类：基本类型和引用类型。这两种类型的数据在内存中是如何存储的。

参数传递：值传递 vs 引用传递
* 教科书版的答案是：值传递。
* 但实际编码中的感受恰恰相反，给人比较直接的感觉是，Java既支持值传递也支持引用传递，实际上，如果读者只了解Java语言，确实很难理解为什么Java中的参数传递是值传递，这是**因为值传递和引用传递的区分来源于C++语言**，而且，“引用传递”其中的“引用”的含义，也并不是指传递的函数参数的类型是引用类型。
* 在C++中，我们可以通过“&”引用语法获取到一个变量的地址，也可以将一个变量的地址传递给函数，这样，在函数内对变量修改，函数结束之后，修改不会失效。

看如下代码
```c++
void change(int &va) {
  va = 2;
}

int main(void) {
  int a = 1;
  change(a);
  printf("a=%d\n", a); //输出2
}
```

在上述代码中，在调用change()函数时，编译器会将a对应的内存单元的地址传递给va。编译器将chang()函数中的"va=2"翻译为：将2存储到va所存储的地址对应的内存单元中，而不是存储到va对应的内存单元。这样就实现了修改变量a。

C++这种传递参数的方式叫做引用传递，也就是尽管我们在调用函数时，感觉传递是变量本身，**但实际上传递的却是变量的地址**。你可能会说，Java中的函数参数可以是引用类型，也可以传递对象的地址啊。从对象的角度来看，传递的确实是对象的地址，但从引用变量本身的角度来看，**传递是引用变量的值（也就是对象的地址）而非引用变量的地址**。所以，从引用变量本身来看，即便传递的参数是引用类型，仍然是值传递，而非引用传递。

这点不好理解，我们再举例解释一下。代码如下所示。
```java
public class Demo3_6 {
  public static void main(String[] args) {
    Student stu = new Student(1, 1);
    fb(stu);
    System.out.println(stu.age); //输出1，stu存储的地址未改变
  }

  private static void fb(Student vstu) {
    vstu = new Student(2, 2);
  }
}
```

从上述代码中，我们可以发现，即便传递的参数是引用类型的，我们也只能改变所引用的对象的属性，而不能改变引用变量本身的值（也就是无法指向新的对象），所以，Java的参数不管是基本类型，还是引用类型，从变量本身来看，传递的都是变量的值，因此，都是值传递的。

等号 vs equals()方法
* 不管是基本类型数据，还是引用类型数据，等号比较的都是变量对应的内存单元中存储的值是否相等。对于引用变量来说，等号就是判定两个引用变量中存储的地址是否相同，也就是判定两个引用变量所引用的对象是否是同一个。
* 如果我们在Java类中没有重写equals()方法，那么使用equals()方法判等，就跟使用等号判等一样了。这是继承自 Object 类的方法
* JDK中的大部分类都重写了equals()方法。比如，Integer类和String类，重写之后的equals()判定的不再是对象的“址”（内存地址）是否相等，而是对象的“值”（关键属性值）是否相等。比如，Integer类的equals()方法判断的是其int值是否相等；String类的equals()方法判断的是其char数组存储的是否是相同的字符。

Java摒弃了C/C++中的指针语法，取而代之，引入了引用语法。尽管指针和引用存储的都是被指或被引用的内存块的地址，但引用却比指针在使用上更加安全。
* 为了方便编写偏底层的代码（如驱动、操作系统），C/C++赋予指针灵活的操作内存的能力。C/C++允许指针越界访问，允许指针做加减运算，允许指针嵌套（指针的指针），甚至允许指针将一块内存重新解读为任意类型。
* 正是因为C/C++中的指针使用起来非常灵活，对内存的访问几乎没有什么限制，所以，对程序员编写代码的能力要求比较高，稍有不慎就会引入bug，误操作不应该操作的内存区域，造成非常巨大的外延性破坏，从而引起安全问题。
* Java语言设计的初衷就是简单易用，因此，权衡安全性和灵活性，Java语言摒弃了灵活的指针，设计了更加安全的指针，即引用。尽管指针和引用存储的都是某段内存的地址，但是，在用法上，**引用是有限制的指针**，没有了那么多酷炫的骚操作，只能引用对象或数组，并且不能进行加减运算，强制类型转化也只能发生在有继承关系的类之间。虽然这样限制了引用使用的灵活性，但增加了引用使用的安全性。

## 既然 java 一切皆对象，那为何保留基本类型
详细讲解一下Java中的基本类型以及对应的包装类。

Java中的基本类型有8种，并且可以分为三类，具体如下所示。
* 整型类型：byte（字节型）、short（短整型）、int（整型）、long（长整型）
* 浮点类型：float（单精度）、double（双精度）
* 字符类型：char
* 布尔类型：boolean

> C/C++支持无符号类型，而Java不支持无符号类型。

boolean类型只有true和false两个值，理论上只需要1个二进制位就可以表示。我们知道，数据存储的位置是通过内存地址来标识的，内存地址以字节为单位，一个字节一个地址。单个二进制位很难存储和访问，考虑到字节对齐，在JVM具体实现boolean类型时，大都采用1个字节来存储，用0表示false，用1表示true。尽管在存储空间上有些浪费，但操作起来更加简单。

基本类型转换有两种：自动类型转换和强制类型转换。自动类型转化也叫做隐式类型转换，强制类型转换也叫做显式类型转化。一般来讲，从数据范围小的类型向数据范围大的类型转换，可以触发自动类型转换；从数据范围大的类型向数据范围小的类型转换，需要强制类型转换。不过，boolean类型比较特殊，它不支持任何类型转换。

强制类型转换有可能导致数据的截断（将高位字节舍弃）或精度的丢失，需要程序员自己保证转换后的结果符合自己的业务要求。在真实的项目开发中，精度丢失是可以接受的，如下代码所示，数据从float类型转换为int类型，相当于实现了取整操作。但大部分截断是不被允许的，只有程序员事先确定数据落在另一个类型可表示的范围内时，这种转换才是有意义的。

引用类型也可以互相转换。不过，这种转换仅限于有继承关系的类之间。引用类型之间的转换有两种类型：向上转换（Upcasting）和向下转换（Downcasting）。向上转换的意思是将对象的类型转换为父类或接口类型。向上转换总是被允许的，属于自动类型转换。向下转换的意思是将对象的类型转换为子类类型。向下转换需要显式指明，属于强制类型转换。

需要特别注意的是，对于向下转换，因为转换为子类之后，有可能会调用子类存在而父类不存在的属性和方法，所以，程序员需要保证转换的对象本身就是子类类型的。

对于基本类型，Java提供了对应的包装类（Wrapper Class），如下所示。
* byte	Byte
* short	Short
* int	Integer
* long	Long
* float	Float
* double	Double
* char	Character
* boolean	Boolean

基本类型和包装类之间可以转换，这种转换可以显式执行，也可以隐式执行。
```java
// 基本类型转换为包装类
int i = 5;
Integer iobj1 = new Integer(i);
Integer iobj2 = Integer.valueOf(i);

// 包装类转换为基本类型
i = iobj1.intValue();

Integer iobj = 12; //自动装箱，底层是 valueof
int i = iobj; //自动拆箱，底层是 intValue
```

自动装箱和自动拆箱的触发场景有以下几种。
* 将基本类型数据赋值给包装类变量（包括参数传递）时，触发自动装箱。
* 将包装类对象赋值给基本类型变量（包括参数传递）时，触发自动拆箱。
* 当包装类对象参与算术运算时，触发自动拆箱操作。
* 当包装类对象参与关系运算（<、>）时，触发自动拆箱操作。
* 当包装类对象参与关系运算（==），并且另一方是基本类型数据时，触发拆箱操作。

尽管自动装箱和自动拆箱给我们的开发带来了很多便利，但不恰当的使用，也会导致性能问题。如下代码所示。
```java
public class Demo4_3 {
  public static void main(String[] args) {
    Integer count = 0;
    for (int i = 0; i < 10000; ++i) {
      count += 1;
    }
  }
}
```

因为Integer等包装类是不可变类，执行这条语句会先触发自动拆箱，执行加法操作，然后再触发自动装箱，生成新的Integer类对象，最后将Integer类对象赋值给count变量。也就是说，执行上述代码，要执行10000次自动装箱和拆箱，并且生成10000个Integer对象，这就会导致代码的内存消耗大和执行速度慢。因此，在平时的开发中，在基本类型和包装类都适用的情况下，我们尽量使用基本类型数据。

常量词技术
```java
Integer a = 12;
Integer b = 12;
Integer c = new Integer(12);
System.out.println("a==12: " + (a==12)); //输出true
System.out.println("a==b: " + (a==b)); //输出true
System.out.println("a==c: " + (a==c)); //输出false
```

这是因为Integer包装类使用了常量池技术。IntegerCache类会缓存值为-128到127之间的Integer对象。当我们通过自动装箱，也就是调用valueOf()来创建Integer对象时，如果要创建的Integer对象的值在-128和127之间，就会从IntegerCache中直接返回，否则才会真正调用new方法创建。

为什么IntegerCache只缓存-128到127之间的整型值，而不缓存其他整型值呢？

在IntegerCache的代码实现中，当这个类被加载的时候，缓存的Integer对象会被集中一次性创建好。毕竟整型值太多了，我们不可能在IntegerCache类中预先创建好所有的整型值对象，这样既占用太多内存，又使得加载IntegerCache类的时间过长，更也没有这样做的必要。所以，JVM只选择缓存对大部分应用来说常用的整型值，也就是一个字节范围内的整型值（-128~127）。

在平时的开发中，对于下面这样三种创建整型对象的方式，我们优先使用后两种。这是因为，第一种创建方式并不会使用到IntegerCache缓存，而后面两种创建方法可以利用IntegerCache缓存，返回共享对象，以达到节省内存的目的。
```java
Integer a = new Integer(123);
Integer a = 123;
Integer a = Integer.valueOf(123);
```

> 除了Integer类型之外，其他部分包装类也使用了常量池技术。其中，Long、Short、Byte利用常量池技术来缓存值在-128到127之间对象。Character利用常量池技术缓存值在0到127之间的对象（因为Character的值没有负数）。Float、Double表示浮点数，无法利用常量池技术。Boolean只有两个值，不需要使用常量池技术。

基本类型VS包装类
* 包装类是引用类型，**对象的引用和对象本身是分开存储的，而对于基本类型数据，变量对应的内存块直接存储数据本身**。因此，基本类型数据在读写效率方面，要比包装类高效。除此之外，在64位JVM上，在开启引用压缩的情况下，一个Integer对象占用16个字节的内存空间（关于这一点，我们在第9节详细讲解），而一个int类型数据只占用4字节的内存空间，前者对空间的占用是后者的4倍。也就是说，**不管是读写效率，还是存储效率，基本类型都比包装类高效。这就是Java保留基本类型的原因**。尽管Java最初的设计理念是一切皆对象，这样可以统一对变量的处理逻辑，但为了性能做了妥协，毕竟基本类型数据在开发中使用太频繁了。
* 不过，Java真的想要做到一切皆对象，也是有可能的。它可以只提供包装类给开发者，而不提供基本类型。编译器在底层将包装类转换为基本类型再进行处理。这样就相当于包装类是基本类型的语法糖。既兼顾了符合一切皆对象的设计理念，又兼顾了性能。实际上，像Groovy, Scala等语言也正是这么做的。而Java之所以没有这么做，我猜测，很可能是历史的原因，毕竟Java发明于上个世纪90年代，当时没有考虑那么全面，而之后大家已经习惯了使用基本类型，如果再将其废弃，那么影响过于大。
* 包装类也有优势，它提供了更加丰富的方法，可以更加方便地实现复杂功能。
* 在项目开发中，首选基本类型，毕竟基本类型在性能方面更好。当然，也有一些特殊情况，比较适合使用包装类。比如映射数据库的Entity、映射接口请求的DTO，在数据库或请求中的字段值为null时，我们需要将其映射为Entity或DTO中的null值。还有，我们在初始化变量时，需要将其设置为没有业务意义的值，如果某个变量的默认值0是有业务意义的值，这个时候，我们就推荐使用包装类，使用其默认值null来表示没有业务意义的值。

## >>>和>>的区别？(原码/反码/补码、算术位移/逻辑位移）
人类习惯用十进制来计数，逢十进一，这跟人类有十根手指有很大关系。而计算机采用二进制来计数，逢二进一，这跟计算机的硬件电路实现有很大关系。

十进制和二进制
* 十进制转二进制：除二取余的逆序
* 二进制转十进制：将数组中的每一位二进制数与其对应的权值相乘，得到的结果加起来，就是最终的十进制整数

如何在计算机中表示整数：补码
* 正数表示二进制是很简单的
* 负数在计算机中如何表示呢？计算机并没有专门的硬件来存储数字的正负号，它只能识别0、1这样的二进制数。所以，计算机使用一串二进制数的最高位作为符号位，其余位作为数值位。**符号位为0表示正数，符号位为1表示负数**。以上二进制表示法叫做**原码表示法**。在这种表示法中，对于长度为1个字节的byte类型的数据来说，取值范围是-127（1111 1111十六进制为0xFF）到127（0111 1111十六进制为0x7F）。比较特殊的是，0有两种表示方法+0（0000 0000）和-0（1000 0000）。
* 原码表示法简单直接，容易理解，但计算机并不是用原码来存储整数的，其原因是如果使用原码表示整数，那么，尽管加法运算比较简单，但减法运算就会比较复杂，需要设计有别于加法的新的电路来实现。
  * 对于加法运算，例如3+5，表示成二进制原码（假设数据类型是byte，长度为1字节）为：0000 0011 + 0000 0101，我们只要像十进制加法运算一样，从低位向高位逐位相加逢二进一即可。
  * 但对于减法运算，例如5-3，如果想要复用加法电路，我们可以将其转化为：5+(-3)，表示成二进制原码为：0000 0101+1000 0011，如果不区分符号位，继续按照加法的逻辑来运算，得到的结果为：1000 1000，也就是-8，显然，结果是不对。如果我们区分符号位，那么，计算逻辑就复杂了很多，需要实现新的电路来实现正数+负数这种运算，也就是减法运算。
* 于是引入一种新的整数的二进制表示法：**补码表示法**。利用补码，减法可以转换为加法，利用同一套电路来实现。**正数的补码跟原码相同。负数的补码是在原码的基础上先求反码，然后再+1。反码的意思是在原码的基础上，符号位不变，数值位按位取反。**
* 补码跟原码虽然有一定的关系，但它们是两套不同的二进制编码方式。在补码表示法中，有两个比较特殊的地方。其一是，**0不再像原码那样有+0和-0的区分**，-0没有对应的补码；其二是，对于长度为n（n个二进制位）的数据类型，**最高位为1、数值位全为0的二进制数为-2^(n-1)的补码，此补码没有对应的原码**。例如，对于byte类型，1000 0000为-128的补码， 没有对应的原码。实际上，我们也可以理解为，把-0的补码挪去表示-128了。所以，对于长度为n（n个二进制位）的数据类型，原码的表示范围是[-2^(n-1)+1, 2^(n-1)-1]，而补码的表示范围是[-2^(n-1), 2^(n-1)-1]。补码表示范围比原码表示范围大1。

如何用补码实现加减法
* 因为正数的补码跟原码相同，所以，加法的运算逻辑不变，仍然是按位求和，逢二进一。
* 对于减法，例如5-3，表示为加法就相当于：5+(-3)，用补码表示就是：0000 0101+1111 1101（假设数据类型为byte，长度为1个字节）。对于补码的加法，**计算机不单独区分符号位和数值位，所有的二进制位一把梭，一律按照加法的运算逻辑来运算**，如下图所示，得到的结果为：1 0000 0010，**最前面的1溢出，被截断丢弃**，所以最终结果为：0000 0010，也是补码表示，对应的整数值为2。
* 对于长度为1个字节的byte类型，在补码表示法中，-0的补码（1000 0000）挪做表示-128。这种安排并不是随意的，而是因为这样做，正好满足刚刚讲的补码的减法运算规则。例如5-128，表示成加法为：5+(-128)，用补码表示为：0000 0101+1000 0000，逐位相加，最终结果为：1000 0101，正好为-123的补码。

证明补码运算的正确性
* 补码的加法运算（两个正数相加）的正确性不言而喻，因为两加数都为正数，正数的补码跟原码相同，加法操作就是普通的按位求和，逢二进一。
* 重点来看补码的减法运算（两个正数相减）的正确性

两个非常重要的前置知识点，我们拿长度为1个字节的byte类型的数据来举例讲解。
* 如果两数a、b相加的结果c超过127，也就是，c包含9位二进制位，最高位是1。那么，c的高位溢出，只保留低8位的值。这个操作就相当于拿c跟2^8求模。
* 一个负数的补码和这个数的绝对值的原码按位相加（不区分处理符号位），得到的结果为2^8。比如，-10的绝对值的原码为0000 1010，-10的补码为1111 0110，不区分处理符号位，两数相加为：1 0000 0000，正好为2^8。也就是说，如果x是一个负数，假设其补码为y，那么-x+y=2^8，那么y=2^8+x。也就是说，负数x的补码为2^8+x。

补码在溢出和自动类型转换中的应用

溢出的判断不能根据相加后最大最小值去比较，因为会截断，因此最终结果总是在能表示的范围区间的，只是说结果不对
```java
// 错误写法
public int sum(int a, int b) {
  if (a+b > Integer.MAX_VALUE) { //Integer.MAX_VALUE=2147483647
    throw new RuntimeException("Overflow");
  }
  return a+b;
}

// 正确写法
public int sum(int a, int b) {
  boolean downOverflow = a<0 && b<0 && a<Integer.MIN_VALUE-b; 
  boolean upOverflow = a>0 && b>0 && a>Integer.MAX_VALUE-b;
  if (downOverflow || upOverflow) {
    throw new RuntimeException("Overflow");
  }
  return a+b;
}
```

自动类型转换
* 当byte类型的数据赋值给short类型变量时，就会触发自动类型转换。byte类型的数据对应的二进制数，会拷贝到short类型变量的低字节中，那么short类型的变量的高字节怎么补全呢？
* 如果byte类型的数据是正数，那么高字节用0补全，如果byte类型的数据为负数，那么高字节用1补全。这样操作的正确性得益于整数在计算机中是用补码来表示。我们拿-5举例，-5对应的补码为1111 1011，当赋值给short类型的变量时，为了保证值不变，我们在高字节处补1，结果就变成了1111 1111 1111 1011，此补码对应的原码为1000 0000 0000 0101，转换成十进制数为-5。
* 一个从补码反推原码的小技巧：**补码的补码就是原码**。如果感兴趣的话，读者可以自己证明一下其正确性。

计算机如何操作二进制位：位运算
* 逻辑位运算有与（&）、或（|）、异或（^）、取反（！）：两个数执行位运算，也就是两个数的补码执行位运算
* 移位：移位操作分为算术位移和逻辑位移。这两种运算操作的对象也是数据的补码
  * 逻辑位移不区分符号位，它将数据的补码整体往左或往右移动，并在**后面或前面补全0**
  * **算术左移跟逻辑左移操作相同，而算术右移针对正数和负数的处理逻辑不同。对正数进行算法右移，补码在右移之后前面补0；对负数进行算术右移，补码在右移之后前面补1**。
  * 不管逻辑位移还是算术位移，超出范围的二进制位都会被舍弃。

> 算术左移相当于乘以2，我们常常利用位运算来替代乘2运算，以提高运算速度。不过，当数据被左移之后，超过了可以表示的数据范围时（比如byte整型值范围为-128~127），就有可能导致数据从负数变成正数，或从正数变成负数。算术右移相当于除以2。对正数不停进行算术右移，最终值将会变为0，但是对负数不停进行算术右移，其值永远都不会变为0，其最终值将会变为-1。

## 浮点数
浮点数是计算机中用来表示实数的数据类型。**整数在计算机中有固定的表示方法（或者叫存储格式）：补码**，同样，**浮点数也有固定的表示方法，并且形成了一份规范文件，叫做IEEE754标准**。绝大多数计算机都参照这个标准来存储浮点数。实际上，**浮点数在计算机中的存储格式类似科学记数法**。

在数学计算中，特别是在表示一些物理量时，比如星球之间的距离，为了方便表示这些超级大数，我们一般采用科学记数法。我们将数据表示成**x * 10^y**的形式，x叫做尾数或有效数字，y叫做指数、幂或者阶数。例如，我们将12350000表示为1.235 * 10^7。为了方便书写，我们一般会将科学记数法的表示形式，简化为**xEy**的形式。例如，我们把1.235 * 10^7简写为1.235E7。

对于一个实数，我们可以将其分为两部分：整数部分和小数部分。我们将整数部分和小数部分分别转换为二进制数，然后中间用点号（.）连接，就得到了实数对应的二进制表示。

整数转化成二进制表示，采用是除2求余的方法，如何将小数部分转换成二进制表示。对于小数的十进制表示，点号后的每一位都对应一个权值，依次为1/10，1/10^2，1/10^3...以此类推。同理，对于小数的二进制表示，点号后的每一位也对应一个权值，依次为1/2，1/2^2，1/2^3...以此类推。

类比十进制的乘10运算，假设某个小数w表示成二进制数之后为：0.xyz（x、y、z为0或1），将其乘2就相当于点号后移一位，变为：x.yz。对x.yz取整，我们就得到了x的值，即从0.xyz中分离出了第一位小数x。以此类推，每次乘以2，然后取整，这样就能依次得到小数点后的每一位。这种将小数转换为二进制表示的方法叫做乘2取整法。

不过，上述实数的二进制表示包含负号和点号，只适合人类阅读，但无法直接存储到计算机中。那么，怎么将类似-1100.011这样的二进制格式，转换为适合计算机存储的二进制格式呢？换句话说，如果我们用4字节去存储实数，那么，如何将类似-1100.011这样的二进制串的所有信息，都保存在这4个字节中呢？

实数的存储格式：定点数
* 借鉴整数符号的处理方法，我们将一串二进制位的最高位作为符号位来表示正负数。符号位为0表示正数，符号位为1表示负数。这样如何存储符号的问题解决了
* 如何存储点号，也就是，当把二进制位存储到4个字节中时，如何区分哪几位是整数部分，哪几位是小数部分。比较简单的方法是将实数表示为定点数，即固定整数和小数所占二进制位的个数，比如最高位为符号位，接下来的20位表示整数，最后11位表示小数。
* 定点数最大的问题是有时会浪费一些存储空间。比如，当我们表示一个只包含整数部分，不包含小数部分的实数时，也就是说，小数部分的二进制位都为0，即便整数部分都要溢出了，但也不能挪用小数部分的二进制位。同理，当我们要表示一个高精度的实数，并且它只包含小数不包含整数时，也就是整数部分的二进制位都是0，即便小数部分因为存储空间不够都要被截断了，但我们也不能挪用整数部分的二进制位。

实数的存储格式：浮点数
* 整数和小数的二进制位的个数是不固定的。这样不仅可以充分利用存储空间，还能够表示更大的数据范围和更高的小数精度。
* 计算机中的浮点数一般分为4字节单精度浮点数和8字节双精度浮点数，对应到Java语言中就是float类型和double类型。
* 根据IEEE754标准的规定，浮点数的二进制表示格式为(-1)^sM2^E，实际上就是二进制的科学记数法。(-1)^s表示符号，s为0时表示正数，s为1时表示负数。M是有效数字或尾数，E是指数、幂或阶数。
* 为了统一表示方式，IEEE754标准规定，M的整数位必须是1，即M必须大于等于1且小于2。这样-12.375就只能表示为(-1)^11.1000112^3，对应的S为1，M为1.00011，E为3。
* IEEE754标准规定，对于4字节单精度浮点数，最高位存储s，中间8位存储指数E，叫做指数域，最后23位存储有效数字，叫做有效数字域。对于8字节的双精度浮点数，最高位存储s，中间11位存储指数E，最后52位存储有效数字。
* 因为IEEE754标准规定，M的整数位总是为1，所以，在存储M时，我们可以不用存储整数位1，只存储小数位即可。也就是说，我们可以用23个二进制位存储24位有效数字。
* 指数E如何存储。E是一个整数，它既可以是负数（比如0.000011用科学记数法表示为1.1*2^(-5)，E为-5），也可以是正数。整数的存储方法有多种，上一节讲到原码和补码。用原码存储，8个二进制位可以表示的范围是[-127, 127]，用补码存储，8位二进制位可以表示的范围是[-128, 127]。不过，IEEE754并没有沿用原码或补码来存储E（当然，使用原码或补码也是可以的）。IEEE754限定指数E的范围是[-126, 127]，IEEE754将指数域解释为无符号类型，也就是，指数域中没有符号位，8个二进制位全是数值位，那么，指数域可表示数据范围是[0, 255]（0000 0000 ~ 1111 1111）。不过，这样，负指数就无法存储在指数域了。为了解决这个问题，IEEE标准将指数E统一加127之后，再存储到指数域，同理，当从指数域中取出指数时，也要对应的减去127。指数E的返回[-126, 127]加127之后，就变成了[1, 254]，正好落在指数域可表示的范围[0, 255]内。
* 那么指数域中的0和255岂不是没用到。如果IEEE754将指数E范围扩大一点，限定为[-127, 128]，那么这个范围加127之后，就变成了[0, 255]，这不就正好跟指数域可表示范围相吻合，就不浪费0和255两个值了吗？实际上，IEEEE754之所以把指数E的数据范围限定为[-126, 127]，是因为指数域中的0和255这两个值还有其他特殊用途。
  * 指数域为0（0000 0000）用来辅助表示浮点数0：IEEE规定有效数字M的整数位总是为1，并且，在存储有效数字M到有效数字域中时，其整数位1会被省略。反过来，从有效数字域中读取的二进制位的前面加1，才得到真正的有效数字。尽管这种存储方式节省了一位二进制位，但是也带来了新的问题，那就是无法表示0.0这个浮点数。因为0.0的有效数字是0，而有效数字域能表示的最小数为000....00000（23个0），将其翻译为有效数字时，需要在其前面加1，于是就变成了1000...00000（1个1，23个0），不再是原来的0。为了解决这个问题，IEEE754标准规定，当指数域为0时，从有效数字域中读取的二进制位不需要在前面加1。这样，指数域为0并且有效数字域为0，就表示浮点数0。
  * 指数域为255（1111 1111）用来辅助表示无穷大或NaN：当指数域为255，有效数字域为0时，表示正无穷大（s为0）或负无穷大（s为1）。当指数域为255，有效数字域的二进制位不全为0时，表示这是一个无意义数NaN（Not a Number）。在Java中的Float类中定义3个静态常量来表示正负无穷大和NaN，

浮点数的表示范围和精度：4字节的单精度浮点数来举例
* 浮点数的表示范围：在IEEE754标准规定的浮点数的存储结构中，有效数字域占23个二进制位，因此，有效数字M的最大值是1.111...11（总共有24个1）。指数的范围是[-126, 127]，因此，指数的最大值是127。综合起来，浮点数可以表示的最大值是1.111...11 * 2^127，最小值是-1.111...11*2^127。转化成十进制数约等于3.4E38和-3.4E38。浮点数可以表示的范围是非常大的。而同样占用4个字节存储空间的int类型的表示范围只有-2^31 ~ 2^31-1，也就是-1073741824 ~ 1073741823。那么，同样是4字节长度，为什么浮点数就可以表示这么大的数据范围呢？之所以浮点数能表示这么大的数据范围，是因为它有选择地表示这个范围内的一小部分的数，而非全部的数。一来，实数是无限多的，全部表示本身就是不可能的事，二来，根据排列组合知识，32个二进制位可以最多表示2^32个不同的数。根据鸽巢原理，用2^32个数来表示无限多的实数，必然会有重叠。由此我们可以得到：不同的实数，在用浮点数表示时，在计算机中存储的可能是相同的浮点数。
* 浮点数的精度问题：当某个实数表示成二进制科学计数法，并且其有效数字位数超过24位时，就会做精度舍弃，类似四舍五入的方法舍弃多出来的有效数字（注意不是直接截断舍弃，具体舍入的算法比较复杂，我们就不展开讲解了）。由此就会产生精度问题。某个实数存入计算机中，再次被取出时，有可能就不是之前存入的实数值了。这里需要注意一下，不仅仅只有小数会有精度问题，整数也有精度问题。总之，只要有效数字个数超过24个，就会存在精度问题。

浮点数的替代品BigDecimal
* 浮点数的表示存在误差，因此浮点数的计算也存在误差，不过，这个误差非常小，大部分对精度不是特别敏感的系统，用浮点数来表示实数就足够了。
* 对精度比较敏感的金融系统，代码中充斥着各种浮点数的表示和计算，一丢丢误差累积下来就会产生比较大的误差，因此，金融系统一般采用BigDecimal来表示实数。BigDecimal将整数部分和小数部分分开存储，小数部分也当做整数来存储，这样就能精确表示像0.1这样数据了。
* 注意递进BigDecimal中的是字符串“0.1”，而非float类型数据0.1f，否则BigDecimal将无法表示精确的0.1，这是原因0.1f这个字面量存储在计算机中时已经就是不准确的了，再传递给BigDecimal也就不准确了。
* BigDecimal还提供了相应的方法，进行精确的加减乘除操作，示例代码如下所示。注意，对于无法整除的除法操作，我们需要指明舍入（Rounding）方法，否则，程序将抛出ArithmeticException异常。
* 浮点数的关系操作（判等、大于、小于等）是比较复杂的，需要引入误差，示例如下所示。而BigDecimal完全不存在这个问题，有现成的方法可以使用。

浮点数的精度取舍方法
* 在金融系统里面，代码中浮点计算的结果，应该尽量多保留几位小数，在存入数据库或者展示给用户时，再按照业务需要做舍入。比如在计算过程中，浮点数保留8位小数，存入数据库中时保留5位小数，展示给用户时保留2位小数。
* 常用的舍入算法是四舍五入法，但是，它的累积误差比较大。如果我们通过四舍五入保留1位小数，那么，0.01舍，会产生-0.01的误差，而0.09入，会抵消0.01产生的-0.01的误差。同样，0.02舍和0.08入，0.03舍和0.07入，0.04舍和0.06入，累积下来，都可以实现正负误差相抵。而0.05入，产生+0.05的误差，无人抵消。所以，在数据分布比较均匀的情况下，对于求和操作，10次舍入就会产生一个+0.05的误差。对于金融系统来说，浮点计算非常频繁，100万次舍入操作，就会产生5万的误差。累积误差就比较大了。
* 金融系统经常用到的舍入方法是四舍六入五成双，也叫做银行家舍入算法，此舍入算法是对四舍五入方法的改进。舍去位的数值小于5时，直接舍去；舍去位的数值大于5时，直接进位；当舍去位的数值等于5时，根据5前一位数的奇偶性来判断是否需要进位。如果前一位数是偶数，则5进位，如果前一位数是奇数，则5舍去。
* 当然银行家算法也不是适应用所有的情况，有时候我们还会根据业务需求选择其他舍入方法，比如分息向上取整，罚息向下取整，以保证客户不亏。
* 在开发中，我们要避免依赖数据库的舍入算法。Mysql中Decimal和Oracle中的Number经常用来存储高精度数据，比如Decimal(7,3)和Number(7, 3)，其中，7表示全部的数据位数，3表示小数点之后的数据位数。如果存储的数据超出了字段可表示的精度，Mysql会四舍五入，Oracle会直接截断。为了避免产生不可预测的结果，在存入数据库之前，最好按业务对精度的要求，提前做精度舍入，以免触发数据库的精度舍入。从数据库取出数据时，实数也要用BigDecimal来映射，避免映射为浮点数而导致的精确性问题。

两个很有趣的问题
1. 4字节单精度浮点数能否准确表示int能表示的所有整数呢？如果不能，那么哪个范围内的整数可以准确表示？
  * float 不能准确表示 int 能表示的所有整数，因为 float 有效数字部分实际上只有24位，但 int 可以用31位存储，转为 float 将会出现精度缺失。float 只能够表示 [-2^24, 2^24 - 1] 范围内的整数，这个范围需要借助指数域完成
  * 扩展：javascript 中使用 64 位浮点数同时存储整数和小数，因此能表示的最大整数不是 2^63-1，而是 2^53-1
2. 浮点数可以表示的最小的正数是多少？
  * 浮点数能够表示的最小正数应该是：1.0 * 2^-126，因为有效位数默认整数部分为1，23位全部存储0时最小，然后指数部分表示的最小值为 -126。

## 字符编码
字符、字符集和字符编码
* 字符（Character）可以理解为书面表达中可能用到的符号，包括各种文字、数字、标点、图形符号、控制符号（如回车换行）等。
* 字符集（Character Set）是一组字符的集合。不同语言会有不同的字符集，字符集不仅包含字符，还包含每个字符的编号。这里的编号只是方便索引，跟字符编码不是一回事。
* 字符编码（Character Encoding）是指计算机存储字符编号的格式。大部分情况下，在设计字符集时，都会同步设计字符编码。一般来说，一个字符集会对应一种字符编码，比如，GB2312字符集对应GB2312字符编码。不过，也有例外，同一个字符集也可以对应多种不同的字符编码，比如，Unicode字符集对应UTF-8、UTF-16、UTF-32三种不同的字符编码。

比较常用的字符集有ASCII、GB2312、GBK、GB18030、Unicode。其中，前三个字符集对应的字符编码跟字符集同名。Unicode字符集对应的字符编码有三种，分别是UTF-8、UTF-16、UTF-32。

ASCII字符集和字符编码
* ASCII字符集只包含128个字符，对应的编号如下图所示。因为只有128个字符，所以，ASCII字符集的字符编码很简单，使用1个字节中的低7位来存储编号，最高位默认为0。
* ASCII字符集中的字符分为两类：不可显示字符和可显示字符。编号0 ~ 31和127对应的字符为不可显示字符，编号32 ~ 126对应的字符为可显示字符。不可显示字符也叫做控制字符。当在一个字符串中包含一些控制字符时，控制字符并不会显示在计算机屏幕上，而是用来控制输出格式。比如常用的控制字符有回车符（ASCII码值为13）、换行符（ASCII码值为10）。
* 在字符串中存储可显示字符比较简单，但如何存储非可显示字符呢？实际上，我们可以使用\xxx这种格式来表示非可显示字符，其中，xxx为非可显示字符的ASCII码的八进制表示。当然，对于可显示字符，我们也可以用这种方式来表示。
* 实际上，对于部分常用的非可显示字符，我们还可以使用对应的转义字符来表示。比如\r表示回车，\n表示换行，\t表示tab，\0表示null。
* 我们也可以直接使用ASCII码值来表示字符。如下代码所示。字符a的ASCII码值为97，它存储在计算机中的二进制串，跟数值97存储在计算机中的二进制串，是一模一样的，都是0110 0001。对于0110 0001这个二进制串，到底是表示为字符a，还是数值97，实际上**全看编译器如何解读**。
* char类型数据之间还可以进行比较操作，对应的就是，将字符编码转变为无符号数之后进行大小比较。除此之外，char类型数据还可以进行加减操作，对应的就是，将字符编码转变为无符号数之后的加减操作

GB*系列字符集和字符编码
* ASCII只能表示128个字符，对于英文来说可能足够了，但是，对于中文、日文、韩文等，所包含的字符远远不止这些，所以，当计算机传播到世界各地之后，为了适应各地的语言，又相继发布了其他字符集和字符编码。支持中文的字符集和字符编码，大都以GB开头来命名，比如常见的有GB2312、GBK、GB18030。
* GB2312发布于1980年，是第一个中文字符集和字符编码。它采用定长存储方式，每个字符编号在计算机中都用2个字节来存储。尽管2个字节可以表示6万多（2^16）个不同的字符，但因为其特殊的编码方式，GB2312仅收录了6000多个汉字及其他符号，总共7000多个字符。尽管GB2312收录了使用频率超过99%的常用汉字，但对于一些罕用字、人名等，GB2312无法表示，毕竟中国汉字有10万多个，显然，GB2312是不够全面的。于是就出现了GBK。尽管GBK仍然使用2个字节，但因为其使用新的编码方式（对于GB*字符集的编码方式，我们不展开讲解），能表示的字符增多，比GB2312增加了2万多个汉字和符号。
* GB18030兼容GB2312和GBK，并且可表示的字符更多，共收录了7万多个汉字。GB18030采用变长编码方式，不同的字符使用不同长度的字节（1字节、2字节或4字节）来存储。

Unicode字符集和UTF*系列字符编码
* 各个语言都有自己的字符集和字符编码，同一串二进制位在不同的字符集和字符编码中，代表不同的字符。这就导致我们无法在一个文档中使用两种不同的语言（两种不同的字符集和字符编码）。为了解决这个问题，Unicode字符集就出现了。Unicode字符集包含大约100万个字符，涵盖了世界上所有语言的所有字符，每一个字符都对应一个不同的编号。我们一般习惯将字符编号表示为十六进制，并且辅以前缀“U+”，以表示此编号为Unicode字符编号。
* 尽管Unicode字符集中的字符个数超百万，但常用的并不多，为了让常用字符的编号尽可能小（这样计算机在存储字符串时会节省空间，待会会讲），Unicode字符集将编号分为两部分。
  * 编号从U+0 ~ U+FFFF，并且排除U+D800 ~ U+DFFF，分配给使用频率最高的字符，这几乎涵盖了各个语言中的常用字符。至于为什么要排除U+D800 ~ U+DFFF这个范围的编号，我们在讲完UTF-16字符编码后你就明白了。
  * 编号从U+10000 ~ U+10FFFF，大约有100多万个编号，分配给剩下的所有字符。
* Unicode只是一个字符集，包含字符及其编号，但并不包含字符编号在计算机中的存储方式，也就是字符编码。

按照编码的复杂程度，我们来依次讲解Unicode字符集对应的3种字符编码：UTF-32、UTF-16、UTF-8。
* UTF-32：UTF-32是定长编码，使用4个字节来存储Unicode编号。定长的好处就是编码简单，只需要将字符编号直接存入计算机即可。读取时的解码也非常简单，每读取四个字节解码为一个字符。
* UTF-16： UTF-16采用变长编码，U+0 ~ U+FFFF范围（不包含U+D800 ~ U+DFFF）内的编号使用2字节编码，U+10000 ~ U+10FFFF之间的编号采用4字节编码。采用变长编码方式，比起定长的UTF-32编码方式，更加节省存储空间。但是，编解码也复杂了很多。当从一个文本中读取2个字节之后，我们怎么知道这2个字节对应的数值，是U+0 ~ U+FFFF范围的2字节编码，还是U+10000 ~ U+10FFFF范围内的4字节编码的高十六位或低十六位呢？为了解决这个问题，UTF-16将U+0 ~ U+FFFF之间的Unicode编号，直接存储在2个字节中，而对于U+10000 ~ U+10FFFF之间的Unicode编号，采用如下特殊编码方式。
  * STEP1：将U+10000 ~ U+10FFFF范围内的Unicode编号减去10000，得到新的范围：U+00000 ~ U+FFFFF，新的范围内的每个编号，只使用20个二进制位就能表示。
  * STEP2：将20个二进制位中的高10位取出，放到UTF-16的4字节编码中的高16位中，前面多出的6位用110110补全。这样高16位的数据范围就变成了U+D800 ~ U+DBFF。
  * STEP3：将20个二进制位中的低10位取出，放到UTF-16的4字节编码中的低16位中，前面多出的6位用110111补全。这样低16位的数据范围就变成了U+DC00 ~ U+DFFF。
  * 从上述UTF-16的解码过程，我们可以得知，在Unicode字符集中，在U+0 ~ U+FFFF这个范围内，U+D800 ~ U+DFFF这个范围的编号并没有使用，就是为了对2字节编码跟4字节编码的高16位和低16位数据做区分。
* UTF-8：相比UTF-16，UTF-8对不同字符占用存储空间的大小，控制得更加精细，当然，编码也更加复杂。UTF-8同样使用变长编码，其中包括4种类型的编码：1字节编码、2字节编码、3字节编码、4字节编码。不同范围内的编号使用不同的编码。
  * U+0000 ~ U+007F范围内的编号使用1字节编码
  * U+0080 ~ U+07FF范围内的编号使用2字节编码
​  * U+0800 ~ U+FFFF范围内的编号使用3字节编码
  * U+10000 ~ U+10FFFF范围内的编号使用4字节编码
  * 在UTF-8的编码规则中，1字节编码的首字节的前缀为0，2字节编码的首字节的前缀为110，3字节编码的首字节的前缀为1110，4字节编码的首字节的前缀为11110。尾随字节的前缀均为10。
  * 跟UTF-16编码类似，UTF-8这样编码的目的是，明确读取出来的字节，属于哪种类型的编码（1字节编码、2字节编码...）。因为UTF-8的最短编码长度是1字节，在读取二进制文件进行解码时，我们每次读取一个字节，判定是哪种类型的首字节编码。假如是3字节编码的首字节编码，那么我们再顺序往下读取2个尾随字节。

既然UTF-8比UTF-16采用更加复杂的编码，那么，在平时的开发中，使用UTF-8是不是一定比使用UTF-16更加节省存储空间呢？答案是否定的
* 仔细观察编号范围与编码长度，我们可以发现，如果在开发中，我们存储英文字符居多，那么，使用UTF-8更加节省空间，因为为了兼容ASCII码，Unicode中编号0 ~ 127之间的字符跟ASCII码一一对应，也就是说，英文字符的Unicode编号在0 ~ 127之间，使用UTF-8编码只需要1个字节长度，而是使用UTF-16编码则需要2个节长度
* 如果存储非英文字符居多，比如中文，那么使用UTF-16反倒会更加节省空间。因为常用的非英文字符，在UTF-16中编码长度为2字节，而在UTF-8中编码长度为2字节或3字节，并且3字节居多。

为何C/C++中char占1个字节，而Java中char占2个字节?
* 因为C语言出现的比较早，彼时多数计算机还只支持英文系统，而C++又继承了C语言的特性，所以，C/C++在设计char类型时，使其只占用一个字节长度，只能存储ASCII字符，如果要存储非ASCII字符，如中文字符，C/C++选择使用char数组（char[]）。
* Java出现较晚，彼时Unicode已经流行，为了让char类型表示更多的字符，Java设计了占两个字节长度的char类型，存储部分Unicode字符，编号在U+0 ~ U+FFFF之间的Unicode字符会通过UTF-16编码之后存储到char类型变量中。
* 因为Java中的char类型只占2个字节长度，所以，并不能存储所有的Unicode字符。不过，平时经常用到的字符，一般都是Unicode编号处于U+0 ~ U+FFFF之间的字符，为了避免存储空间的浪费，Java让char类型只占2字节长度，只存储常用字符。
* 剩下的U+10000 ~ U+10FFFF范围内的Unicode字符在Java中如何存储呢？类似C/C++存储ASCII码之外字符的做法，Java使用char数组来存储U+10000 ~ U+10FFFFF范围内的字符

跟ASCII码类似，我们也有3种方法将Unicode字符赋值给char类型变量：
* 对于可显示字符，我们可以直接使用字符来赋值给变量。
* 对于所有字符（可显示或不可显示），我们都可以将字符对应的UTF-16编码表示为\uxxxx的形式赋值给变量。其中，xxxx为16进制。
* 对于所有字符，我们都可以将字符对应的Unicode编号赋值给变量。

内编码和外编码
* Java中的char、String的字符编码被称为内编码。内编码指的是字符在内存中的编码格式。对应外编码指的是外部输入输出（比如文件、命令行参数、源码中的字符串常量等）的编码方式。
* 在Java中，内编码为UTF-16字符编码。外编码通过JVM参数-Dfile.endcoding来指定。如果我们在运行程序时没有指定外编码，那么，外编码跟系统编码一致。比如，Linux默认系统编码为UTF-8。在未指定外编码的情况下，运行在Linux系统下的Java程序默认按照UTF-8字符编码来读取文件中内容。

## 字符串：压缩、常量池、不可变
针对字符串，Java提供了String类，封装了字符数组（char[]），并提供了大量操作字符串的方法。

在JDK8中，String底层依赖char数组实现，核心的属性只有value数组和hash。
* value数组用来存储字符串，在JDK8中为char类型。JDK9对其进行了优化，将其改为byte类型
* hash属性用来缓存的hashcode

String 类相关方法
* +运算符：C++语言支持运算符重载，不过，Java语言并不支持运算符重载，一来，运算符重载的设计思想来自函数式编程，并不是纯粹的面向对象设计思想。二来，Java语言设计的主旨之一就是简单，摒弃了C++中的很多复杂语法，比如指针和这里的运算符重载。尽管程序员自己编写的类无法重载运算符，但Java自己提供的String类却实现了加法操作。如下代码所示，两个String对象可以相加。**String类型作为最常用的类型之一，延续了基本类型及其包装类的设计思想，同样支持加法操作，这样使用起来就比较方便和统一**。String对象还可以跟其他任意类型的对象相加，最终的结果为String对象与其他对象的toString()函数的返回值相加
* length()方法的返回值是char类型value数组的长度。不管是英文还是中文，均占用一个char的存储空间。
* valueOf()：Java重载了一组valueOf()方法，可以将int、char、long、float、double、boolean等基本类型数据转化成String类型
* compareTo()：字符比较大小，是将字符对应的UTF-16二进制编码，重新解读为16位的无符号数，再进行比较。字符串比较大小，是从下标0开始，两个字符串中的相同下标位置的字符一一比较，当遇到第一组不相等的字符时，根据这组字符的大小关系，决定两个字符串的大小关系。当然，如果短字符串跟长字符串的前缀完全相同，那么规定短字符串小于长字符串。

String的压缩技术
* String类作为在开发中最常用的类型之一，在性能和使用方便程度上，都理应做到极致。
* 在JDK8以及之前的版本中，String类底层依赖char类型的数组来存储字符串。而上一节讲到，char类型存储的是字符的UTF-16编码，一个字符占2个字节长度，因此，使用char类型来存储英文等ASCII字符，会比较浪费空间。因此，在JDK9中，Java对String类进行了优化，将存储字符串的value数组的类型，**由char类型改为了byte类型**。
* coder属性的值是通过分析字符串来得到。如果在所存储的字符串中，每个字符对应的UTF-16编码值（2字节编码）都小于等于127，那么，这就说明字符串中只包含英文字符。我们就对字符串进行压缩存储，使用1个字节存储1个字符。实际上，String类中的很多操作，比如计算字符串长度的length()，以及根据下标返回字符的charAt(int index)等，都依赖coder属性的值

String的常量池技术
* Integer、Long等基本类型的包装类，使用常量池技术，缓存常用的数值对象，起到节省内存的作用。String作为常用的数据类型，也效仿了基本类型的包装类的做法，设计了常量池，缓存用过的字符串。
* String类型跟Integer等包装类类似，使用new方式创建对象，并不会触发常量池技术，只有在使用字符串常量赋值时，才会触发常量池技术。字符串常量封装成String对象存储在字符串常量池中。如果字符串常量对应的String对象在常量池中存在，那么，JVM直接使用这个已经存在的String对象，否则，JVM在常量池中创建封装了字符串常量的String对象
* 除了使用字符串常量赋值之外，我们还可以使用intern()方法，将分配在堆上的String对象，原模原样在常量池中复制一份。在平时的开发中，什么时候使用intern()方法呢？当我们无法通过字符串常量来给String变量赋值（比如使用现成的API从文件或数据库中读取字符串），但又存在大量重复字符串（比如数据库中有一个“公司”字段，这个字段有大量重复值）时，我们就可以将读取到的String对象，调用intern()方法，复制到常量池中。之后在代码中使用常量池中的String对象，原String对象就被JVM垃圾回收掉。

String的不可变性
* 不可变的意思是：其对象在创建完成之后，所有的属性都不可以再被修改，包括引用类型变量所引用的对象的属性。
* 原因一：因为String类使用了常量池技术，所以，有可能存在很多变量同时引用同一个String对象的情况。如果String对象允许修改，某段代码对String对象进行了修改，其他变量因为引用同一个String对象，获取到的数据值也紧跟着被修改，这样显然不符合大部分的业务开发需求。
* 原因二：字符串和整型数经常用来作为HashMap的键（key）。在平常的开发中，我们经常将对象的某个String类型或整型类型的属性作为key，对象本身作为value，存储在HashMap中。如果之后属性值又改变了，那么，此对象在HashMap中的存储位置，需要作相应的调整，否则就会导致此对象在HashMap中无法被查询到。这显然增加了编码的复杂度。String类中的hash属性。String类定义了自己的hashcode()函数。当将对象存储在HashMap（哈希表）中时，HashMap会调用对象的hashcode()函数来计算哈希值。对于String不可变对象来说，因为hash值在计算得到之后就不会再改变，所以，使用一个属性hash来缓存这个值，避免重复计算。
* 原因三：String的设计思想非常贴近基本类型及其包装类，比如支持+运算符等。因为基本类型及其包装类都是不可变的，所以，String也延续了它们的设计思路，也设计为不可变的。

StringBuilder 类
* 因为String是不可变类，当我们在拼接多个字符串时，执行效率会比较低
* 为了解决这个问题，Java设计了一个新的类StringBuilder，支持修改和动态扩容。这样就避免了for循环创建大量的String对象。

## 对象