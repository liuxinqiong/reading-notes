虽然以 Java 为切入点，但还是有很多共性值得学习的。

## 从代码执行看程序本质
CPU只认识机器指令（也叫做CPU指令、机器码），使用高级语言（Python、C++、Java等）编写的代码，需要编译（编译的意思实际上就是“翻译”）为机器指令之后，才能被CPU执行。不同编程语言的编译过程是不同的。根据编译过程的不同，我们将编程语言分为三类。
* **编译型语言**：对于类似C++这样的编译型语言，代码会事先被编译成机器指令（可执行文件），然后再一股脑儿交给CPU来执行。在执行时，CPU面对的是已经编译好的机器指令，直接逐条执行即可，执行效率比较高。但因为每种类型的CPU（比如Intel、ARM等）支持的CPU指令集不同，并且程序还有可能调用特定操作系统提供的API，所以，编译之后的可执行文件只能在特定的操作系统和机器上执行，换一种操作系统或机器，编译之后的可执行文件就无法执行了。
* **解释型语言**：对于类似Python这样的解释型语言，代码并不会被事先编译成机器指令，而是在执行的过程中，由Python虚拟机（也叫做解释器）逐条取出程序中的代码，然后编译成机器指令，交由CPU执行，执行完成之后，再取出下一条代码，重复上述的编译、执行过程。这种一边编译一边执行的过程，叫做解释执行。相对于使用编译型语言编写的代码，使用解释型语言编写的代码的可移植性更好。程序在执行的过程中，虚拟机可以根据当前所在机器的CPU类型和操作系统类型，翻译成不同的CPU指令。这样，同一份代码就可以运行在不同类型的机器和不同类型的操作系统上。这就是常听到的“一次编写，多处运行”。
* **混合型语言**：Java语言比较特殊，它属于混合型语言，既包含编译执行也包含解释执行。Java编译器会先将代码（.java文件）编译成字节码（.class文件）而非机器码。字节码是Java代码和机器码之间的一种中间状态，它既跟平台无关，又可以快速地被翻译成机器码。编译之后的字节码在执行时，仍然是解释执行的，也就是字节码被逐行读出，然后翻译成机器码，再交给CPU执行。只不过，从字节码到机器码的翻译过程，比从高级语言到机器码的翻译过程，耗时要少。这样既保证了Java代码的可移植性（同一份代码可以运行在不同的CPU和操作系统上），又避免了解释执行效率低的问题。实际上，在解释执行的过程中，Java虚拟机会将**热点字节码**（反复多次执行的代码，类似缓存中的热点数据），编译成机器码缓存起来，以供反复执行，这样就避免了热点字节码需要反复编译，进一步节省了解释执行的时间。这就是著名的**JIT编译**（Just In Time Compile，即时编译）

CPU、操作系统、虚拟机在程序的执行过程中，都扮演了什么角色。
* CPU：用来执行编译好的机器指令
* 操作系统：管理硬件资源和调度程序的执行。打个比如，CPU等硬件就好比车间中的机器，工人就好比操作系统，一个个程序就像一个个待执行的任务。工人（操作系统）调度机器（CPU等硬件）来执行各个任务（程序）。除此之外，操作系统还担当了**类库的作用**。对于通用的功能代码，比如读写硬盘等，没必要在每个程序中都从零编写一遍。操作系统将这些通用的功能代码，封装成API（专业名称叫做系统调用），供我们在编写应用程序时直接调用。也就是说，在应用程序的执行过程中，CPU可能会跳转去执行操作系统中的某段代码。
* 虚拟机：简单理解就是万能的中间层，CPU执行虚拟机代码将应用程序的字节码翻译成CPU指令，放到固定的内存位置，再通过修改IP寄存器（IP寄存器存储CPU将要执行的指令位置），引导CPU执行这块内存中存储的CPU指令。

关于虚拟机的作用，可以对比一下C++、Python、Java这三种语言的编译命令和执行命令
```shell
// C++
$ g++ helloword.cpp -o helloworld
$ ./helloword

// Python
$ python helloworld.py

// Java
$ javac HelloWorld.java
$ java HelloWorld
```

我们可以发现，C++编译之后的代码直接就可以执行，而Python和Java代码的执行，需要依赖其他程序（即虚拟机），表现在命令行中就是执行命令前面有python、java字样。

站在操作系统和CPU的角度，Java程序编译之后的字节码跟虚拟机合并在一起，才算是一个完整的程序，才相当于C++编译之后的可执行文件。CPU在执行程序员编写的代码的同时，也在执行虚拟机代码，并且是先执行虚拟机代码，然后才引导执行程序员编写的代码。

CPU指令、汇编语言、字节码
* CPU 指令：一条CPU指令包含的信息主要有：操作码、地址、数据三种，分别指明所要执行的操作、数据来源或去向、数据本身。一组CPU指令的集合，叫做指令集。常见的指令集有X86、X86-64、ARM、MIPS等。不同的CPU支持的指令集可能不同（Intel CPU支持X86指令集，ARM CPU支持ARM指令集）。
* 汇编语言：汇编语言由一组汇编指令构成。汇编指令跟CPU指令一一对应，但汇编指令采用字符串而非二进制数来表示指令，因此，其可读性好很多。实际上，CPU指令和汇编指令之间的关系，就类似IP地址和域名之间的关系。IP地址和域名一一对应，域名的可读性比IP地址好。程序员使用汇编语言编写的代码，需要经过编译，翻译成机器码才能被CPU执行。C/C++语言的编译过程，实际上也包含汇编这一过程。编译器会先将C/C++代码编译成汇编代码，然后再汇编成机器码。
* 字节码：Java语言是跨平台的，程序员编写的代码，在不需要任何修改的情况下，就可以运行在不同的平台（不同的操作系统和CPU）上。字节码诞生的目的是，克服解释型语言解释执行速度慢的缺点（字节码是介于高级语言和机器码之间的形态，比高级语言解释执行更快）。

> 之所以Java语言能做到跨平台（操作系统和CPU），最根本原因是有虚拟机的存在。Java代码跟平台无关，字节码跟平台无关，在编译执行过程中，**总要有一个环节跟平台有关**，不然，跟平台有关的、最终可以被CPU执行的机器码从何而来呢。俗话说的好，哪有什么岁月静好，只是有人帮你负重前行。跟平台有关的环节就是将字节码解释执行的环节，而这个环节的主导者就是虚拟机。

class文件为什么叫字节码？跟字节（byte）有什么关系呢？从上述字节码中，我们可以发现，有些字节码指令包含操作码和操作数两部分，而有些只包含操作码这一部分。因为操作码长度为一个字节，所以，这种指令格式被叫做字节码。从另一个角度，我们也可以得知，**字节码的操作码类型不超过256个（2^8）**。相对于CPU指令和汇编指令，字节码指令少很多。这是因为字节码指令相对于CPU指令来说，抽象程度更高。一条字节码指令完成的逻辑，比一条CPU指令完成的逻辑，更加复杂。

不管是使用哪种类型的编程语言（编译型、解释型、混合型）编写的代码，也不管经历什么样的编译、解释过程，最终交由CPU执行的都是机器码。

讲到CPU，我们就不得不讲一下寄存器。
* 内存的读写速度比起CPU指令的执行速度要慢很多，CPU在执行指令时，如果依赖内存来存储计算过程中的中间数据，那么，CPU将总是在等待读写内存操作的完成，势必会影响CPU整体的计算速度。为了解决这个问题，于是，计算机科学家便发明了寄存器。
* 寄存器读写速度非常快，能够跟CPU指令的执行速度相匹配。所以，内存中的数据会先读取到寄存器中再参与计算。但问题是数据在计算前需要先从内存读取到寄存器，计算之后存储在寄存器中的结果需要再写入内存，因此，寄存器的存在并没有避免掉内存的读写，那么，使用寄存器是不是多此一举呢？实际上，尽管最初数据来源于内存，最后计算结果也要写入内存，**但中间的计算过程涉及到一些临时结果的存取，都可以在寄存器中完成**，不需要跟非常慢速的内存进行交互。顺便说一句，计算机为了提高CPU读写内存的速度，还引入了**L1、L2、L3这三级缓存**。

为了做到能让CPU高速访问，寄存器的硬件设计比较特殊（**高成本、高能耗**），且相对于内存来说与CPU距离更近（寄存器直接跟CPU集成在一起），这些也决定了寄存器的个数不会很多。不同的CPU包含的寄存器会有所不同。常见的寄存器有以下几类。

**通用寄存器**：AX，BX，CX，DX，一般用来存储普通数据。AX，BX，CX，DX这四种通用寄存器的用途有所区别，比如AX是累加器

**指针寄存器**：BP，SP，SI，DI，IP，BP（Base Pointer Register）和SP（Stack Pointer Register）是用于存储栈空间地址的寄存器，SP存储栈顶地址，BP比较特殊，一般存储栈中一个栈帧的栈底地址。SI（Source Index Register）源地址寄存器和DI（Destination Index Register）目的地址寄存器，分别用来存储读取和写入数据的内存地址。IP（Instruction Pointer Register）指令指针寄存器用来存储下一条将要执行的指令的内存地址的一部分。

**段寄存器**：CS，DS，SS，程序由一组指令和一堆数据组成。指令存储在一块被称为代码段的内存中，由CPU逐一读取执行。数据存储在一块被称为数据段的内存中。指令执行的过程中，CPU会操作（读取或写入）这块内存中的数据。

CS（Code Segment Register）代码段地址寄存器存储了代码段的起始地址。上文中讲到，IP寄存器中存储的是下一条将要执行的指令的内存地址的一部分。CS和IP两个寄存器中存储的内容如下计算，才能得到一个真正的物理内存地址。
```
物理内存地址 = 段地址（如CS） * 16 + 偏移地址（如IP）
```

我们拿8086 CPU（早期的16位的X86 CPU）举例解释。8086 CPU具有20位地址总线，支持1MB内存的寻址能力。然而16位的IP寄存器只能存储64K（2^16）个内存地址，一个字节占一个地址，因此，16位的IP寄存器只能支持64KB大小内存的寻址。为了扩大寻址能力，满足X86 CPU 1MB内存的寻址能力，计算机使用段地址和偏移地址相结合的方式来确定一个物理内存地址。通常，我们把CS寄存器和IP寄存器统称为**PC寄存器**，实际上，PC寄存器是一个抽象概念，并不真正存在这种寄存器。
```
最大段地址=2^16-1
最大偏移地址=2^16-1
最大寻址=(2^16-1)*16 + (2^16-1) = 2^20+2^16-16-1 > 2^20=1M
```

拓展：为什么 32 位系统最大寻址能力是 4GB 呢。
1. 2^32 换算成 GB 是 0.5GB
2. 那为什么是 4GB，因为上面的结果是 0.5GB 个地址，**一个地址对应的内存单元的大小通常是一个字节**，因此 0.5*8 = 4GB
3. 由于一些地址需要用于系统的特殊用途，比如硬件设备的映射、BIOS 等系统保留区域等，实际上可供用户程序直接使用的内存空间会小于理论最大值。

DS（Data Segment Register）数据段地址寄存器存储了数据段的起始地址，它跟DI或SI结合才能确定一个数据段中的内存地址。SS（Stack Segment Register）栈寄存器存储的是栈的起始地址，它跟SP结合确定栈顶的内存地址，跟BP结合确定栈中某个中间位置的内存地址。

**指令寄存器**：IR（Instruction Register）指令寄存器用来存放当前正在执行的指令。指令为一串二进制码，指令译码器需要从指令中解析出操作码和操作地址或操作数。所以，指令需要暂存在指令寄存器中等待译码处理。

**标志寄存器**：FR（Flag Register）标志寄存器，也叫做程序状态字寄存器（Program Status Word，PSW）。在这个寄存器中，每一个二进制位记录一类状态。比如cmp指令的运算结果会存储在ZF零标志位或CF进位标志位中。当cmp指令执行完成之后，CPU读取标志寄存器中的ZF位或CF位，便可以得到cmp指令的执行结果。

> 注意，以上讲解的是16位的寄存器，32位的寄存器名称在对应的16位寄存器名称前加E（例如EAX，EBP，ESP，EIP），64位的寄存器名称在对应的16位寄存器名称前加R（例如RAX，RBP，RSP，RIP）。

CPU指令执行的具体流程
1. 对于编译型语言，操作系统会把编译好的机器码，加载到内存中的代码段，将代码中变量等数据加载到内存中的数据段，并且设置好各个寄存器的初始值，如DS、CS等。IP寄存器中存储代码段中第一条指令的内存地址相对于CS的偏移地址。
2. CPU根据PC寄存器（CS寄存器和IP寄存器的总称）存储的内存地址，从对应的内存单元中取出一条CPU指令，放到IR指令寄存器中，然后将IP寄存器中的地址加上offset，从而得到下一条指令的内存地址。对于16位CPU，一条指令的长度为2字节，一个字节占一个地址，因此，offset为2。同理，对于32位和64位CPU，一条指令的长度分别为4字节和8字节，因此，offset分别为4和8。
3. 一条指令执行完成之后，再通过PC寄存器中的地址，取下一条指令继续执行。循环往复，直到所有的指令都执行完成。
4. 对于解释型或混合型语言，操作系统将虚拟机本身的机器码，加载到内存中的代码段。CPU执行虚拟机代码，将程序编写的代码解释为机器码，并放入某块内存中，然后将PC寄存器的地址设置为这块内存的首地址，于是，**CPU就被虚拟机引导去执行程序员编写的代码了**。

## 从 CPU 角度看基础语法
对于绝大部分编程语言来说，基本语法无外乎这样几种：变量、类型、数组、运算（赋值、算术、逻辑、比较等）、跳转（条件、循环）、函数，而其他语法（比如类、容器、异常等）在CPU眼里只不过是语法糖。

### 变量
内存被划分为一个个的内存单元（一个内存单元为1个字节大小）。每个内存单元都对应一个内存地址，方便CPU根据内存地址来读取和操作内存单元中的数据。

对于高级语言来说，内存地址可读性比较差，所以，就发明了变量这种语法。变量可以看看作是内存地址的别名。内存地址和变量的关系，跟IP地址和域名的关系类似。编译器在将代码编译成机器码时，会将代码中的变量替换为内存地址。

不同的变量有不同的作用域（也可以理解为生命周期）。不同作用域的变量，分配在代码段中的不同区域。不同的区域有不同的内存管理方式。不同语言对数据段的分区方式会有所不同，但又大同小异，**常见的分区有栈、堆、常量池等**。

笼统来讲
* 栈一般存储作用域为“函数内”的数据，如函数内的局部变量、函数参数等，它们只在函数内参与计算，函数结束之后，就不再使用了，同时，所占用的内存就可以释放，以供其他变量重复使用。
* 堆一般存储作用域不局限于“函数内”的数据，如对象，只有在程序员主动释放（如C/C++语言）或虚拟机判定为不再使用（如Java语言）时，对象对应的内存才会被释放。
* 常量池一般存储常量等，常量的生命周期跟程序的生命周期一样，只有在程序执行结束之后，对应的内存才会被释放。

对数据段进行分区，是为了方便管理不同生命周期的变量。而之所以不同的变量要设置不同的生命周期，是为了能够有效的利用内存空间，方便在变量生命周期结束之后，对应的内存能够快速地被回收，以供重复使用。

### 数组
使用数组，我们可以定义一块连续的内存空间。

使用 Java 举例
```java
public class Test {
  public void demo() {
    int[] a = new int[10];
    a[3] = 92;
    System.out.println(a[3]);
  }
}
```

在上述示例代码中，a是一个局部变量，存储在栈上。“int[] a = new int[10]”这条语句表示，在堆上申请一块能够存储下10个int类型数据的连续内存空间，并将这块内存空间的首地址存储在变量a所对应的内存单元中。

当通过下标来访问数组中的元素时，如语句“a[3]=92”，编译器将这条语句分解为多条CPU指令，先通过变量a中存储的首地址和如下寻址公式，计算出下标为3的元素所在的内存地址，然后将92写入到这个内存地址对应的内存单元。
```
a[i]的内存地址=
 a中存储的值（也就是数组的首地址）+ i*4(4表示4字节，也就是数据类型的长度)
```

在Java语言中，new申请的数组存储在堆上，首地址赋值给栈上的变量。而在有些语言中，比如C语言，“数组”语法更加灵活，数组既可以申请在堆上，也可以申请在栈上。
```c
int a[100]； //数组在栈中，可以直接类似a[2]=92;这样使用了
int a[100] = malloc(sizeof(int)*100); //数组在堆中
```

实际上，如果你了解JavaScript语言，你还会发现，JavaScript中的数组还可以存储不同类型的数据，数组中存储的是不同类型的数据，因此，上文中提到的寻址公式就无法工作了，那JavaScript是如何通过下标定位到元素的内存地址的呢？

实际上，不同编程语言中的数组，其在内存中的存储方式并不完全一样，也并非只有在上文中讲到的“在一块连续的内存空间中存储相同类型的数据”这样一种存储方式。

### 类型
在CPU眼里，是没有类型这一概念的。任何类型的数据，在CPU眼里都只是一串二进制码。**一串二进制码是表示为字符串还是整型数又或者浮点数，完全看代码是如何定义这块内存的类型的**。

引入类型的目的是，方便程序员编写正确的代码，避免错误的赋值操作。不同的编程语言具有不同的类型系统。根据变量的类型是否可以动态变化和类型检查发生的时期，我们将类型系统分为静态类型和动态类型。

### 运算
在编程语言中，常见的运算类型有以下5种：
1. 算术运算，比如加、减、乘、除；
2. 关系运算，比如大于、小于、等于；
3. 赋值运算，比如a=5；
4. 逻辑运算，比如&&，||，！；
5. 位运算，比如&，|，~，^，>>, <<；

以上绝大部分运算在CPU中都有对应的指令。不过，不同类型的指令对应的电路逻辑不同，所以，执行花费的时间也不同，比如**位运算会比较快**，乘法运算、除法运算会比较慢。

### 跳转
程序由顺序、选择（或叫分支、条件）、循环三种基本结构构成，其中，选择和循环又统称为跳转。

### 函数
编写函数是代码模块化的一种有效手段。几乎所有的编程语言都会提供函数这种语法。函数的底层实现，相对于前面讲到的几种语法的底层实现，要复杂一些。函数底层实现依赖一个非常重要的东西：栈。就是我们前面讲到的，**用来保存局部变量、参数等的内存区域**。因为这块内存的访问方式是先进后出，符合栈这种数据结构的特点，所以，被称为栈。

为什么函数底层实现需要用到栈呢？

每个函数都是一个相对封闭的代码块，其运行需要依赖一些局部数据，比如局部变量等。这些数据会存储在内存中。当函数A调用另一个函数B时，CPU会跳转去执行函数B的代码。函数B的执行又会涉及一些局部变量等，这些数据也会存储在内存中（紧挨着函数A的内存块）。以此类推，当函数B调用另一个函数C时，CPU又会跳转去执行函数C的代码。函数C的内存块会紧邻函数B的内存块。

当函数C执行完成之后，函数C中的局部变量等数据不再被使用，对应的内存块也可以释放，并且，CPU返回执行函数B的代码。函数B对应的内存块又开始被使用。同理，函数B执行完成之后，其对应的内存块也会被释放，CPU返回执行函数A的代码。函数A对应的内存块又开始被使用。

我们可以发现，在函数调用的过程中，同一时间只有一个函数的内存块在被使用，并且内存块被释放的顺序为“先创建者后释放”。内存中数据的操作方式符合栈的特点：“只在一端操作、先进后出”。所以，**编译器把函数调用所使用的整块内存，组织成栈这种数据结构，称为函数调用栈。我们把其中每个函数对应的内存块叫做栈帧**。

当通过函数调用，程序运行到一个新的函数时，**编译器会在栈中创建一个栈帧（实际上就是申请一个内存块）**，存储这个函数的局部变量等数据。当这个函数执行完毕返回上层函数时，栈顶栈帧出栈（也就是释放内存块），此时，新的栈顶栈帧为返回后的函数对应的栈帧。

了解了函数调用栈的大体结构之后，我们思考以下几个问题：
* 除了保存局部变量之外，栈帧中还保存哪些其他数据？栈帧中依次保存：前一个栈帧的帧底地址，参数，局部变量，返回地址。保存前一个栈帧的帧底地址的目的是，方便当前函数执行完成之后，rbp指针重新指向前一个栈帧的栈底。保存返回地址的目的是，方便当前函数执行完成之后，返回到上层函数继续执行。
* 上一节讲到的SP寄存器和BP寄存器具体用在哪里？SP寄存器存储栈顶地址，方便将新数据压入栈。BP寄存器存储的是当前栈帧帧底地址，方便基于这个地址的偏移来访问参数、局部变量。
* CPU在执行完某个函数之后，如何知道应该回到上层函数的哪处再继续执行？在通过callq指令调用函数时，callq指令会将当前的rip寄存器中的内容（callq指令的下一条指令的内存地址，即返回地址）存储在栈帧的最顶端。当被调用的函数执行完之后，被调用函数的栈帧释放，最后调用retq指令（相当于popq %rip），将返回地址重新赋值给rip，CPU就可以从函数中callq指令的下一条指令继续执行了。

## 为什么java引用与 c 中指针更安全
Java中的数据类型可以分为两类：基本类型和引用类型。这两种类型的数据在内存中是如何存储的。

参数传递：值传递 vs 引用传递
* 教科书版的答案是：值传递。
* 但实际编码中的感受恰恰相反，给人比较直接的感觉是，Java既支持值传递也支持引用传递，实际上，如果读者只了解Java语言，确实很难理解为什么Java中的参数传递是值传递，这是**因为值传递和引用传递的区分来源于C++语言**，而且，“引用传递”其中的“引用”的含义，也并不是指传递的函数参数的类型是引用类型。
* 在C++中，我们可以通过“&”引用语法获取到一个变量的地址，也可以将一个变量的地址传递给函数，这样，在函数内对变量修改，函数结束之后，修改不会失效。

看如下代码
```c++
void change(int &va) {
  va = 2;
}

int main(void) {
  int a = 1;
  change(a);
  printf("a=%d\n", a); //输出2
}
```

在上述代码中，在调用change()函数时，编译器会将a对应的内存单元的地址传递给va。编译器将chang()函数中的"va=2"翻译为：将2存储到va所存储的地址对应的内存单元中，而不是存储到va对应的内存单元。这样就实现了修改变量a。

C++这种传递参数的方式叫做引用传递，也就是尽管我们在调用函数时，感觉传递是变量本身，**但实际上传递的却是变量的地址**。你可能会说，Java中的函数参数可以是引用类型，也可以传递对象的地址啊。从对象的角度来看，传递的确实是对象的地址，但从引用变量本身的角度来看，**传递是引用变量的值（也就是对象的地址）而非引用变量的地址**。所以，从引用变量本身来看，即便传递的参数是引用类型，仍然是值传递，而非引用传递。

这点不好理解，我们再举例解释一下。代码如下所示。
```java
public class Demo3_6 {
  public static void main(String[] args) {
    Student stu = new Student(1, 1);
    fb(stu);
    System.out.println(stu.age); //输出1，stu存储的地址未改变
  }

  private static void fb(Student vstu) {
    vstu = new Student(2, 2);
  }
}
```

从上述代码中，我们可以发现，即便传递的参数是引用类型的，我们也只能改变所引用的对象的属性，而不能改变引用变量本身的值（也就是无法指向新的对象），所以，Java的参数不管是基本类型，还是引用类型，从变量本身来看，传递的都是变量的值，因此，都是值传递的。

等号 vs equals()方法
* 不管是基本类型数据，还是引用类型数据，等号比较的都是变量对应的内存单元中存储的值是否相等。对于引用变量来说，等号就是判定两个引用变量中存储的地址是否相同，也就是判定两个引用变量所引用的对象是否是同一个。
* 如果我们在Java类中没有重写equals()方法，那么使用equals()方法判等，就跟使用等号判等一样了。这是继承自 Object 类的方法
* JDK中的大部分类都重写了equals()方法。比如，Integer类和String类，重写之后的equals()判定的不再是对象的“址”（内存地址）是否相等，而是对象的“值”（关键属性值）是否相等。比如，Integer类的equals()方法判断的是其int值是否相等；String类的equals()方法判断的是其char数组存储的是否是相同的字符。

Java摒弃了C/C++中的指针语法，取而代之，引入了引用语法。尽管指针和引用存储的都是被指或被引用的内存块的地址，但引用却比指针在使用上更加安全。
* 为了方便编写偏底层的代码（如驱动、操作系统），C/C++赋予指针灵活的操作内存的能力。C/C++允许指针越界访问，允许指针做加减运算，允许指针嵌套（指针的指针），甚至允许指针将一块内存重新解读为任意类型。
* 正是因为C/C++中的指针使用起来非常灵活，对内存的访问几乎没有什么限制，所以，对程序员编写代码的能力要求比较高，稍有不慎就会引入bug，误操作不应该操作的内存区域，造成非常巨大的外延性破坏，从而引起安全问题。
* Java语言设计的初衷就是简单易用，因此，权衡安全性和灵活性，Java语言摒弃了灵活的指针，设计了更加安全的指针，即引用。尽管指针和引用存储的都是某段内存的地址，但是，在用法上，**引用是有限制的指针**，没有了那么多酷炫的骚操作，只能引用对象或数组，并且不能进行加减运算，强制类型转化也只能发生在有继承关系的类之间。虽然这样限制了引用使用的灵活性，但增加了引用使用的安全性。

## 既然 java 一切皆对象，那为何保留基本类型
详细讲解一下Java中的基本类型以及对应的包装类。

Java中的基本类型有8种，并且可以分为三类，具体如下所示。
* 整型类型：byte（字节型）、short（短整型）、int（整型）、long（长整型）
* 浮点类型：float（单精度）、double（双精度）
* 字符类型：char
* 布尔类型：boolean

> C/C++支持无符号类型，而Java不支持无符号类型。

boolean类型只有true和false两个值，理论上只需要1个二进制位就可以表示。我们知道，数据存储的位置是通过内存地址来标识的，内存地址以字节为单位，一个字节一个地址。单个二进制位很难存储和访问，考虑到字节对齐，在JVM具体实现boolean类型时，大都采用1个字节来存储，用0表示false，用1表示true。尽管在存储空间上有些浪费，但操作起来更加简单。

基本类型转换有两种：自动类型转换和强制类型转换。自动类型转化也叫做隐式类型转换，强制类型转换也叫做显式类型转化。一般来讲，从数据范围小的类型向数据范围大的类型转换，可以触发自动类型转换；从数据范围大的类型向数据范围小的类型转换，需要强制类型转换。不过，boolean类型比较特殊，它不支持任何类型转换。

强制类型转换有可能导致数据的截断（将高位字节舍弃）或精度的丢失，需要程序员自己保证转换后的结果符合自己的业务要求。在真实的项目开发中，精度丢失是可以接受的，如下代码所示，数据从float类型转换为int类型，相当于实现了取整操作。但大部分截断是不被允许的，只有程序员事先确定数据落在另一个类型可表示的范围内时，这种转换才是有意义的。

引用类型也可以互相转换。不过，这种转换仅限于有继承关系的类之间。引用类型之间的转换有两种类型：向上转换（Upcasting）和向下转换（Downcasting）。向上转换的意思是将对象的类型转换为父类或接口类型。向上转换总是被允许的，属于自动类型转换。向下转换的意思是将对象的类型转换为子类类型。向下转换需要显式指明，属于强制类型转换。

需要特别注意的是，对于向下转换，因为转换为子类之后，有可能会调用子类存在而父类不存在的属性和方法，所以，程序员需要保证转换的对象本身就是子类类型的。

对于基本类型，Java提供了对应的包装类（Wrapper Class），如下所示。
* byte	Byte
* short	Short
* int	Integer
* long	Long
* float	Float
* double	Double
* char	Character
* boolean	Boolean

基本类型和包装类之间可以转换，这种转换可以显式执行，也可以隐式执行。
```java
// 基本类型转换为包装类
int i = 5;
Integer iobj1 = new Integer(i);
Integer iobj2 = Integer.valueOf(i);

// 包装类转换为基本类型
i = iobj1.intValue();

Integer iobj = 12; //自动装箱，底层是 valueof
int i = iobj; //自动拆箱，底层是 intValue
```

自动装箱和自动拆箱的触发场景有以下几种。
* 将基本类型数据赋值给包装类变量（包括参数传递）时，触发自动装箱。
* 将包装类对象赋值给基本类型变量（包括参数传递）时，触发自动拆箱。
* 当包装类对象参与算术运算时，触发自动拆箱操作。
* 当包装类对象参与关系运算（<、>）时，触发自动拆箱操作。
* 当包装类对象参与关系运算（==），并且另一方是基本类型数据时，触发拆箱操作。

尽管自动装箱和自动拆箱给我们的开发带来了很多便利，但不恰当的使用，也会导致性能问题。如下代码所示。
```java
public class Demo4_3 {
  public static void main(String[] args) {
    Integer count = 0;
    for (int i = 0; i < 10000; ++i) {
      count += 1;
    }
  }
}
```

因为Integer等包装类是不可变类，执行这条语句会先触发自动拆箱，执行加法操作，然后再触发自动装箱，生成新的Integer类对象，最后将Integer类对象赋值给count变量。也就是说，执行上述代码，要执行10000次自动装箱和拆箱，并且生成10000个Integer对象，这就会导致代码的内存消耗大和执行速度慢。因此，在平时的开发中，在基本类型和包装类都适用的情况下，我们尽量使用基本类型数据。

常量词技术
```java
Integer a = 12;
Integer b = 12;
Integer c = new Integer(12);
System.out.println("a==12: " + (a==12)); //输出true
System.out.println("a==b: " + (a==b)); //输出true
System.out.println("a==c: " + (a==c)); //输出false
```

这是因为Integer包装类使用了常量池技术。IntegerCache类会缓存值为-128到127之间的Integer对象。当我们通过自动装箱，也就是调用valueOf()来创建Integer对象时，如果要创建的Integer对象的值在-128和127之间，就会从IntegerCache中直接返回，否则才会真正调用new方法创建。

为什么IntegerCache只缓存-128到127之间的整型值，而不缓存其他整型值呢？

在IntegerCache的代码实现中，当这个类被加载的时候，缓存的Integer对象会被集中一次性创建好。毕竟整型值太多了，我们不可能在IntegerCache类中预先创建好所有的整型值对象，这样既占用太多内存，又使得加载IntegerCache类的时间过长，更也没有这样做的必要。所以，JVM只选择缓存对大部分应用来说常用的整型值，也就是一个字节范围内的整型值（-128~127）。

在平时的开发中，对于下面这样三种创建整型对象的方式，我们优先使用后两种。这是因为，第一种创建方式并不会使用到IntegerCache缓存，而后面两种创建方法可以利用IntegerCache缓存，返回共享对象，以达到节省内存的目的。
```java
Integer a = new Integer(123);
Integer a = 123;
Integer a = Integer.valueOf(123);
```

> 除了Integer类型之外，其他部分包装类也使用了常量池技术。其中，Long、Short、Byte利用常量池技术来缓存值在-128到127之间对象。Character利用常量池技术缓存值在0到127之间的对象（因为Character的值没有负数）。Float、Double表示浮点数，无法利用常量池技术。Boolean只有两个值，不需要使用常量池技术。

基本类型VS包装类
* 包装类是引用类型，**对象的引用和对象本身是分开存储的，而对于基本类型数据，变量对应的内存块直接存储数据本身**。因此，基本类型数据在读写效率方面，要比包装类高效。除此之外，在64位JVM上，在开启引用压缩的情况下，一个Integer对象占用16个字节的内存空间（关于这一点，我们在第9节详细讲解），而一个int类型数据只占用4字节的内存空间，前者对空间的占用是后者的4倍。也就是说，**不管是读写效率，还是存储效率，基本类型都比包装类高效。这就是Java保留基本类型的原因**。尽管Java最初的设计理念是一切皆对象，这样可以统一对变量的处理逻辑，但为了性能做了妥协，毕竟基本类型数据在开发中使用太频繁了。
* 不过，Java真的想要做到一切皆对象，也是有可能的。它可以只提供包装类给开发者，而不提供基本类型。编译器在底层将包装类转换为基本类型再进行处理。这样就相当于包装类是基本类型的语法糖。既兼顾了符合一切皆对象的设计理念，又兼顾了性能。实际上，像Groovy, Scala等语言也正是这么做的。而Java之所以没有这么做，我猜测，很可能是历史的原因，毕竟Java发明于上个世纪90年代，当时没有考虑那么全面，而之后大家已经习惯了使用基本类型，如果再将其废弃，那么影响过于大。
* 包装类也有优势，它提供了更加丰富的方法，可以更加方便地实现复杂功能。
* 在项目开发中，首选基本类型，毕竟基本类型在性能方面更好。当然，也有一些特殊情况，比较适合使用包装类。比如映射数据库的Entity、映射接口请求的DTO，在数据库或请求中的字段值为null时，我们需要将其映射为Entity或DTO中的null值。还有，我们在初始化变量时，需要将其设置为没有业务意义的值，如果某个变量的默认值0是有业务意义的值，这个时候，我们就推荐使用包装类，使用其默认值null来表示没有业务意义的值。

## >>>和>>的区别？(原码/反码/补码、算术位移/逻辑位移）
人类习惯用十进制来计数，逢十进一，这跟人类有十根手指有很大关系。而计算机采用二进制来计数，逢二进一，这跟计算机的硬件电路实现有很大关系。

十进制和二进制
* 十进制转二进制：除二取余的逆序
* 二进制转十进制：将数组中的每一位二进制数与其对应的权值相乘，得到的结果加起来，就是最终的十进制整数

如何在计算机中表示整数：补码
* 正数表示二进制是很简单的
* 负数在计算机中如何表示呢？计算机并没有专门的硬件来存储数字的正负号，它只能识别0、1这样的二进制数。所以，计算机使用一串二进制数的最高位作为符号位，其余位作为数值位。**符号位为0表示正数，符号位为1表示负数**。以上二进制表示法叫做**原码表示法**。在这种表示法中，对于长度为1个字节的byte类型的数据来说，取值范围是-127（1111 1111十六进制为0xFF）到127（0111 1111十六进制为0x7F）。比较特殊的是，0有两种表示方法+0（0000 0000）和-0（1000 0000）。
* 原码表示法简单直接，容易理解，但计算机并不是用原码来存储整数的，其原因是如果使用原码表示整数，那么，尽管加法运算比较简单，但减法运算就会比较复杂，需要设计有别于加法的新的电路来实现。
  * 对于加法运算，例如3+5，表示成二进制原码（假设数据类型是byte，长度为1字节）为：0000 0011 + 0000 0101，我们只要像十进制加法运算一样，从低位向高位逐位相加逢二进一即可。
  * 但对于减法运算，例如5-3，如果想要复用加法电路，我们可以将其转化为：5+(-3)，表示成二进制原码为：0000 0101+1000 0011，如果不区分符号位，继续按照加法的逻辑来运算，得到的结果为：1000 1000，也就是-8，显然，结果是不对。如果我们区分符号位，那么，计算逻辑就复杂了很多，需要实现新的电路来实现正数+负数这种运算，也就是减法运算。
* 于是引入一种新的整数的二进制表示法：**补码表示法**。利用补码，减法可以转换为加法，利用同一套电路来实现。**正数的补码跟原码相同。负数的补码是在原码的基础上先求反码，然后再+1。反码的意思是在原码的基础上，符号位不变，数值位按位取反。**
* 补码跟原码虽然有一定的关系，但它们是两套不同的二进制编码方式。在补码表示法中，有两个比较特殊的地方。其一是，**0不再像原码那样有+0和-0的区分**，-0没有对应的补码；其二是，对于长度为n（n个二进制位）的数据类型，**最高位为1、数值位全为0的二进制数为-2^(n-1)的补码，此补码没有对应的原码**。例如，对于byte类型，1000 0000为-128的补码， 没有对应的原码。实际上，我们也可以理解为，把-0的补码挪去表示-128了。所以，对于长度为n（n个二进制位）的数据类型，原码的表示范围是[-2^(n-1)+1, 2^(n-1)-1]，而补码的表示范围是[-2^(n-1), 2^(n-1)-1]。**补码表示范围比原码表示范围大1**。

如何用补码实现加减法
* 因为正数的补码跟原码相同，所以，加法的运算逻辑不变，仍然是按位求和，逢二进一。
* 对于减法，例如5-3，表示为加法就相当于：5+(-3)，用补码表示就是：0000 0101+1111 1101（假设数据类型为byte，长度为1个字节）。对于补码的加法，**计算机不单独区分符号位和数值位，所有的二进制位一把梭，一律按照加法的运算逻辑来运算**，如下图所示，得到的结果为：1 0000 0010，**最前面的1溢出，被截断丢弃**，所以最终结果为：0000 0010，也是补码表示，对应的整数值为2。
* 对于长度为1个字节的byte类型，在补码表示法中，-0的补码（1000 0000）挪做表示-128。这种安排并不是随意的，而是因为这样做，正好满足刚刚讲的补码的减法运算规则。例如5-128，表示成加法为：5+(-128)，用补码表示为：0000 0101+1000 0000，逐位相加，最终结果为：1000 0101，正好为-123的补码。

证明补码运算的正确性
* 补码的加法运算（两个正数相加）的正确性不言而喻，因为两加数都为正数，正数的补码跟原码相同，加法操作就是普通的按位求和，逢二进一。
* 重点来看补码的减法运算（两个正数相减）的正确性

两个非常重要的前置知识点，我们拿长度为1个字节的byte类型的数据来举例讲解。
* 如果两数a、b相加的结果c超过127，也就是，c包含9位二进制位，最高位是1。那么，c的高位溢出，只保留低8位的值。这个操作就相当于拿c跟2^8求模。
* 一个负数的补码和这个数的绝对值的原码按位相加（不区分处理符号位），得到的结果为2^8。比如，-10的绝对值的原码为0000 1010，-10的补码为1111 0110，不区分处理符号位，两数相加为：1 0000 0000，正好为2^8。也就是说，如果x是一个负数，假设其补码为y，那么-x+y=2^8，那么y=2^8+x。也就是说，负数x的补码为2^8+x。

补码在溢出和自动类型转换中的应用

溢出的判断不能根据相加后最大最小值去比较，因为会截断，因此最终结果总是在能表示的范围区间的，只是说结果不对
```java
// 错误写法
public int sum(int a, int b) {
  if (a+b > Integer.MAX_VALUE) { //Integer.MAX_VALUE=2147483647
    throw new RuntimeException("Overflow");
  }
  return a+b;
}

// 正确写法
public int sum(int a, int b) {
  boolean downOverflow = a<0 && b<0 && a<Integer.MIN_VALUE-b;
  boolean upOverflow = a>0 && b>0 && a>Integer.MAX_VALUE-b;
  if (downOverflow || upOverflow) {
    throw new RuntimeException("Overflow");
  }
  return a+b;
}
```

自动类型转换
* 当byte类型的数据赋值给short类型变量时，就会触发自动类型转换。byte类型的数据对应的二进制数，会拷贝到short类型变量的低字节中，那么short类型的变量的高字节怎么补全呢？
* 如果byte类型的数据是正数，那么高字节用0补全，如果byte类型的数据为负数，那么高字节用1补全。这样操作的正确性得益于整数在计算机中是用补码来表示。我们拿-5举例，-5对应的补码为1111 1011，当赋值给short类型的变量时，为了保证值不变，我们在高字节处补1，结果就变成了1111 1111 1111 1011，此补码对应的原码为1000 0000 0000 0101，转换成十进制数为-5。
* 一个从补码反推原码的小技巧：**补码的补码就是原码**。如果感兴趣的话，读者可以自己证明一下其正确性。

计算机如何操作二进制位：位运算
* 逻辑位运算有与（&）、或（|）、异或（^）、取反（!）：两个数执行位运算，也就是两个数的补码执行位运算
* 移位：移位操作分为算术位移和逻辑位移。这两种运算操作的对象也是数据的补码
  * 逻辑位移不区分符号位，它将数据的补码整体往左或往右移动，并在**后面或前面补全0**
  * **算术左移跟逻辑左移操作相同，而算术右移针对正数和负数的处理逻辑不同。对正数进行算法右移，补码在右移之后前面补0；对负数进行算术右移，补码在右移之后前面补1**。
  * 不管逻辑位移还是算术位移，超出范围的二进制位都会被舍弃。

> 算术左移相当于乘以2，我们常常利用位运算来替代乘2运算，以提高运算速度。不过，当数据被左移之后，超过了可以表示的数据范围时（比如byte整型值范围为-128~127），就有可能导致数据从负数变成正数，或从正数变成负数。算术右移相当于除以2。对正数不停进行算术右移，最终值将会变为0，但是对负数不停进行算术右移，其值永远都不会变为0，其最终值将会变为-1。

## 浮点数
浮点数是计算机中用来表示实数的数据类型。**整数在计算机中有固定的表示方法（或者叫存储格式）：补码**，同样，**浮点数也有固定的表示方法，并且形成了一份规范文件，叫做IEEE754标准**。绝大多数计算机都参照这个标准来存储浮点数。实际上，**浮点数在计算机中的存储格式类似科学记数法**。

在数学计算中，特别是在表示一些物理量时，比如星球之间的距离，为了方便表示这些超级大数，我们一般采用科学记数法。我们将数据表示成**x * 10^y**的形式，x叫做尾数或有效数字，y叫做指数、幂或者阶数。例如，我们将12350000表示为1.235 * 10^7。为了方便书写，我们一般会将科学记数法的表示形式，简化为**xEy**的形式。例如，我们把1.235 * 10^7简写为1.235E7。

对于一个实数，我们可以将其分为两部分：整数部分和小数部分。我们将整数部分和小数部分分别转换为二进制数，然后中间用点号（.）连接，就得到了实数对应的二进制表示。

整数转化成二进制表示，采用是除2求余的方法，如何将小数部分转换成二进制表示。对于小数的十进制表示，点号后的每一位都对应一个权值，依次为1/10，1/10^2，1/10^3...以此类推。同理，对于小数的二进制表示，点号后的每一位也对应一个权值，依次为1/2，1/2^2，1/2^3...以此类推。

类比十进制的乘10运算，假设某个小数w表示成二进制数之后为：0.xyz（x、y、z为0或1），将其乘2就相当于点号后移一位，变为：x.yz。对x.yz取整，我们就得到了x的值，即从0.xyz中分离出了第一位小数x。以此类推，每次乘以2，然后取整，这样就能依次得到小数点后的每一位。**这种将小数转换为二进制表示的方法叫做乘2取整法**。

不过，上述实数的二进制表示包含负号和点号，只适合人类阅读，但无法直接存储到计算机中。那么，怎么将类似-1100.011这样的二进制格式，转换为适合计算机存储的二进制格式呢？换句话说，如果我们用4字节去存储实数，那么，如何将类似-1100.011这样的二进制串的所有信息，都保存在这4个字节中呢？

实数的存储格式：定点数
* 借鉴整数符号的处理方法，我们将一串二进制位的最高位作为符号位来表示正负数。符号位为0表示正数，符号位为1表示负数。这样如何存储符号的问题解决了
* 如何存储点号，也就是，当把二进制位存储到4个字节中时，如何区分哪几位是整数部分，哪几位是小数部分。比较简单的方法是将实数表示为定点数，即固定整数和小数所占二进制位的个数，比如最高位为符号位，接下来的20位表示整数，最后11位表示小数。
* 定点数最大的问题是有时会浪费一些存储空间。比如，当我们表示一个只包含整数部分，不包含小数部分的实数时，也就是说，小数部分的二进制位都为0，即便整数部分都要溢出了，但也不能挪用小数部分的二进制位。同理，当我们要表示一个高精度的实数，并且它只包含小数不包含整数时，也就是整数部分的二进制位都是0，即便小数部分因为存储空间不够都要被截断了，但我们也不能挪用整数部分的二进制位。

实数的存储格式：浮点数
* 整数和小数的二进制位的个数是不固定的。这样不仅可以充分利用存储空间，还能够表示更大的数据范围和更高的小数精度。
* 计算机中的浮点数一般分为4字节单精度浮点数和8字节双精度浮点数，对应到Java语言中就是float类型和double类型。
* 根据IEEE754标准的规定，浮点数的二进制表示格式为(-1)^sM2^E，实际上就是二进制的科学记数法。(-1)^s表示符号，s为0时表示正数，s为1时表示负数。M是有效数字或尾数，E是指数、幂或阶数。
* 为了统一表示方式，IEEE754标准规定，M的整数位必须是1，即M必须大于等于1且小于2。这样-12.375就只能表示为(-1)^11.1000112^3，对应的S为1，M为1.00011，E为3。
* IEEE754标准规定，对于4字节单精度浮点数，最高位存储s，中间8位存储指数E，叫做指数域，最后23位存储有效数字，叫做有效数字域。对于8字节的双精度浮点数，最高位存储s，中间11位存储指数E，最后52位存储有效数字。
* 因为IEEE754标准规定，M的整数位总是为1，所以，**在存储M时，我们可以不用存储整数位1，只存储小数位即可**。也就是说，我们可以用23个二进制位存储24位有效数字。
* 指数E如何存储。E是一个整数，它既可以是负数（比如0.000011用科学记数法表示为1.1*2^(-5)，E为-5），也可以是正数。整数的存储方法有多种，上一节讲到原码和补码。用原码存储，8个二进制位可以表示的范围是[-127, 127]，用补码存储，8位二进制位可以表示的范围是[-128, 127]。不过，IEEE754并没有沿用原码或补码来存储E（当然，使用原码或补码也是可以的）。IEEE754限定指数E的范围是[-126, 127]，IEEE754将指数域解释为无符号类型，也就是，指数域中没有符号位，8个二进制位全是数值位，那么，指数域可表示数据范围是[0, 255]（0000 0000 ~ 1111 1111）。不过，这样，负指数就无法存储在指数域了。为了解决这个问题，**IEEE标准将指数E统一加127之后，再存储到指数域，同理，当从指数域中取出指数时，也要对应的减去127**。指数E的返回[-126, 127]加127之后，就变成了[1, 254]，正好落在指数域可表示的范围[0, 255]内。
* 那么指数域中的0和255岂不是没用到。如果IEEE754将指数E范围扩大一点，限定为[-127, 128]，那么这个范围加127之后，就变成了[0, 255]，这不就正好跟指数域可表示范围相吻合，就不浪费0和255两个值了吗？实际上，IEEEE754之所以把指数E的数据范围限定为[-126, 127]，是因为指数域中的0和255这两个值还有其他特殊用途。
  * 指数域为0（0000 0000）用来辅助表示浮点数0：IEEE规定有效数字M的整数位总是为1，并且，在存储有效数字M到有效数字域中时，其整数位1会被省略。反过来，从有效数字域中读取的二进制位的前面加1，才得到真正的有效数字。尽管这种存储方式节省了一位二进制位，但是也带来了新的问题，那就是无法表示0.0这个浮点数。因为0.0的有效数字是0，而有效数字域能表示的最小数为000....00000（23个0），将其翻译为有效数字时，需要在其前面加1，于是就变成了1000...00000（1个1，23个0），不再是原来的0。为了解决这个问题，IEEE754标准规定，当指数域为0时，从有效数字域中读取的二进制位不需要在前面加1。这样，指数域为0并且有效数字域为0，就表示浮点数0。
  * 指数域为255（1111 1111）用来辅助表示无穷大或NaN：当指数域为255，有效数字域为0时，表示正无穷大（s为0）或负无穷大（s为1）。当指数域为255，有效数字域的二进制位不全为0时，表示这是一个无意义数NaN（Not a Number）。在Java中的Float类中定义3个静态常量来表示正负无穷大和NaN，

浮点数的表示范围和精度：4字节的单精度浮点数来举例
* 浮点数的表示范围：在IEEE754标准规定的浮点数的存储结构中，有效数字域占23个二进制位，因此，有效数字M的最大值是1.111...11（总共有24个1）。指数的范围是[-126, 127]，因此，指数的最大值是127。综合起来，浮点数可以表示的最大值是1.111...11 * 2^127，最小值是-1.111...11*2^127。转化成十进制数约等于3.4E38和-3.4E38。浮点数可以表示的范围是非常大的。而同样占用4个字节存储空间的int类型的表示范围只有-2^31 ~ 2^31-1，也就是-1073741824 ~ 1073741823。那么，同样是4字节长度，为什么浮点数就可以表示这么大的数据范围呢？之所以浮点数能表示这么大的数据范围，是因为它有选择地表示这个范围内的一小部分的数，而非全部的数。一来，实数是无限多的，全部表示本身就是不可能的事，二来，根据排列组合知识，32个二进制位可以最多表示2^32个不同的数。根据鸽巢原理，用2^32个数来表示无限多的实数，必然会有重叠。由此我们可以得到：不同的实数，在用浮点数表示时，在计算机中存储的可能是相同的浮点数。
* 浮点数的精度问题：当某个实数表示成二进制科学计数法，并且其有效数字位数超过24位时，就会做精度舍弃，类似四舍五入的方法舍弃多出来的有效数字（注意不是直接截断舍弃，具体舍入的算法比较复杂，我们就不展开讲解了）。由此就会产生精度问题。某个实数存入计算机中，再次被取出时，有可能就不是之前存入的实数值了。这里需要注意一下，**不仅仅只有小数会有精度问题，整数也有精度问题。总之，只要有效数字个数超过24个，就会存在精度问题**。

浮点数的替代品BigDecimal
* 浮点数的表示存在误差，因此浮点数的计算也存在误差，不过，这个误差非常小，大部分对精度不是特别敏感的系统，用浮点数来表示实数就足够了。
* 对精度比较敏感的金融系统，代码中充斥着各种浮点数的表示和计算，一丢丢误差累积下来就会产生比较大的误差，因此，金融系统一般采用BigDecimal来表示实数。BigDecimal将整数部分和小数部分分开存储，小数部分也当做整数来存储，这样就能精确表示像0.1这样数据了。
* 注意递进BigDecimal中的是字符串“0.1”，而非float类型数据0.1f，否则BigDecimal将无法表示精确的0.1，这是原因0.1f这个字面量存储在计算机中时已经就是不准确的了，再传递给BigDecimal也就不准确了。
* BigDecimal还提供了相应的方法，进行精确的加减乘除操作，示例代码如下所示。注意，对于无法整除的除法操作，我们需要指明舍入（Rounding）方法，否则，程序将抛出ArithmeticException异常。
* 浮点数的关系操作（判等、大于、小于等）是比较复杂的，需要引入误差，示例如下所示。而BigDecimal完全不存在这个问题，有现成的方法可以使用。

浮点数的精度取舍方法
* 在金融系统里面，代码中浮点计算的结果，应该尽量多保留几位小数，在存入数据库或者展示给用户时，再按照业务需要做舍入。比如在计算过程中，浮点数保留8位小数，存入数据库中时保留5位小数，展示给用户时保留2位小数。
* 常用的舍入算法是四舍五入法，但是，它的累积误差比较大。如果我们通过四舍五入保留1位小数，那么，0.01舍，会产生-0.01的误差，而0.09入，会抵消0.01产生的-0.01的误差。同样，0.02舍和0.08入，0.03舍和0.07入，0.04舍和0.06入，累积下来，都可以实现正负误差相抵。而0.05入，产生+0.05的误差，无人抵消。所以，在数据分布比较均匀的情况下，对于求和操作，10次舍入就会产生一个+0.05的误差。对于金融系统来说，浮点计算非常频繁，100万次舍入操作，就会产生5万的误差。累积误差就比较大了。
* **金融系统经常用到的舍入方法是四舍六入五成双**，也叫做银行家舍入算法，此舍入算法是对四舍五入方法的改进。舍去位的数值小于5时，直接舍去；舍去位的数值大于5时，直接进位；当舍去位的数值等于5时，根据5前一位数的奇偶性来判断是否需要进位。如果前一位数是偶数，则5进位，如果前一位数是奇数，则5舍去。
* 当然银行家算法也不是适应用所有的情况，有时候我们还会根据业务需求选择其他舍入方法，比如分息向上取整，罚息向下取整，以保证客户不亏。
* 在开发中，我们要避免依赖数据库的舍入算法。Mysql中Decimal和Oracle中的Number经常用来存储高精度数据，比如Decimal(7,3)和Number(7, 3)，其中，7表示全部的数据位数，3表示小数点之后的数据位数。如果存储的数据超出了字段可表示的精度，Mysql会四舍五入，Oracle会直接截断。为了避免产生不可预测的结果，在存入数据库之前，最好按业务对精度的要求，提前做精度舍入，以免触发数据库的精度舍入。从数据库取出数据时，实数也要用BigDecimal来映射，避免映射为浮点数而导致的精确性问题。

两个很有趣的问题
1. 4字节单精度浮点数能否准确表示int能表示的所有整数呢？如果不能，那么哪个范围内的整数可以准确表示？
  * float 不能准确表示 int 能表示的所有整数，因为 float 有效数字部分实际上只有24位，但 int 可以用31位存储，转为 float 将会出现精度缺失。float 只能够表示 [-2^24, 2^24 - 1] 范围内的整数，这个范围需要借助指数域完成
  * 扩展：javascript 中使用 64 位浮点数同时存储整数和小数，因此能表示的最大整数不是 2^63-1，而是 2^53-1
2. 浮点数可以表示的最小的正数是多少？
  * 浮点数能够表示的最小正数应该是：1.0 * 2^-126，因为有效位数默认整数部分为1，23位全部存储0时最小，然后指数部分表示的最小值为 -126。

## 字符编码
字符、字符集和字符编码
* 字符（Character）可以理解为书面表达中可能用到的符号，包括各种文字、数字、标点、图形符号、控制符号（如回车换行）等。
* 字符集（Character Set）是一组字符的集合。不同语言会有不同的字符集，字符集不仅包含字符，还包含每个字符的编号。这里的编号只是方便索引，跟字符编码不是一回事。
* 字符编码（Character Encoding）是指计算机存储字符编号的格式。大部分情况下，在设计字符集时，都会同步设计字符编码。一般来说，一个字符集会对应一种字符编码，比如，GB2312字符集对应GB2312字符编码。不过，也有例外，同一个字符集也可以对应多种不同的字符编码，比如，Unicode字符集对应UTF-8、UTF-16、UTF-32三种不同的字符编码。

比较常用的字符集有ASCII、GB2312、GBK、GB18030、Unicode。其中，前三个字符集对应的字符编码跟字符集同名。Unicode字符集对应的字符编码有三种，分别是UTF-8、UTF-16、UTF-32。

ASCII字符集和字符编码
* ASCII字符集只包含128个字符，对应的编号如下图所示。因为只有128个字符，所以，ASCII字符集的字符编码很简单，使用1个字节中的低7位来存储编号，最高位默认为0。
* ASCII字符集中的字符分为两类：不可显示字符和可显示字符。编号0 ~ 31和127对应的字符为不可显示字符，编号32 ~ 126对应的字符为可显示字符。不可显示字符也叫做控制字符。当在一个字符串中包含一些控制字符时，控制字符并不会显示在计算机屏幕上，而是用来控制输出格式。比如常用的控制字符有回车符（ASCII码值为13）、换行符（ASCII码值为10）。
* 在字符串中存储可显示字符比较简单，但如何存储非可显示字符呢？实际上，我们可以使用\xxx这种格式来表示非可显示字符，其中，xxx为非可显示字符的ASCII码的八进制表示。当然，对于可显示字符，我们也可以用这种方式来表示。
* 实际上，对于部分常用的非可显示字符，我们还可以使用对应的转义字符来表示。比如\r表示回车，\n表示换行，\t表示tab，\0表示null。
* 我们也可以直接使用ASCII码值来表示字符。如下代码所示。字符a的ASCII码值为97，它存储在计算机中的二进制串，跟数值97存储在计算机中的二进制串，是一模一样的，都是0110 0001。对于0110 0001这个二进制串，到底是表示为字符a，还是数值97，实际上**全看编译器如何解读**。
* char类型数据之间还可以进行比较操作，对应的就是，将字符编码转变为无符号数之后进行大小比较。除此之外，char类型数据还可以进行加减操作，对应的就是，将字符编码转变为无符号数之后的加减操作

GB*系列字符集和字符编码
* ASCII只能表示128个字符，对于英文来说可能足够了，但是，对于中文、日文、韩文等，所包含的字符远远不止这些，所以，当计算机传播到世界各地之后，为了适应各地的语言，又相继发布了其他字符集和字符编码。支持中文的字符集和字符编码，大都以GB开头来命名，比如常见的有GB2312、GBK、GB18030。
* GB2312发布于1980年，是第一个中文字符集和字符编码。它采用定长存储方式，每个字符编号在计算机中都用2个字节来存储。尽管2个字节可以表示6万多（2^16）个不同的字符，但因为其特殊的编码方式，GB2312仅收录了6000多个汉字及其他符号，总共7000多个字符。尽管GB2312收录了使用频率超过99%的常用汉字，但对于一些罕用字、人名等，GB2312无法表示，毕竟中国汉字有10万多个，显然，GB2312是不够全面的。于是就出现了GBK。尽管GBK仍然使用2个字节，但因为其使用新的编码方式（对于GB*字符集的编码方式，我们不展开讲解），能表示的字符增多，比GB2312增加了2万多个汉字和符号。
* GB18030兼容GB2312和GBK，并且可表示的字符更多，共收录了7万多个汉字。GB18030采用变长编码方式，不同的字符使用不同长度的字节（1字节、2字节或4字节）来存储。

Unicode字符集和UTF*系列字符编码
* 各个语言都有自己的字符集和字符编码，同一串二进制位在不同的字符集和字符编码中，代表不同的字符。这就导致我们无法在一个文档中使用两种不同的语言（两种不同的字符集和字符编码）。为了解决这个问题，Unicode字符集就出现了。Unicode字符集包含大约100万个字符，涵盖了世界上所有语言的所有字符，每一个字符都对应一个不同的编号。我们一般习惯将字符编号表示为十六进制，并且辅以前缀“U+”，以表示此编号为Unicode字符编号。
* 尽管Unicode字符集中的字符个数超百万，但常用的并不多，为了让常用字符的编号尽可能小（这样计算机在存储字符串时会节省空间，待会会讲），Unicode字符集将编号分为两部分。
  * 编号从U+0 ~ U+FFFF，并且排除U+D800 ~ U+DFFF，分配给使用频率最高的字符，这几乎涵盖了各个语言中的常用字符。至于为什么要排除U+D800 ~ U+DFFF这个范围的编号，我们在讲完UTF-16字符编码后你就明白了。
  * 编号从U+10000 ~ U+10FFFF，大约有100多万个编号，分配给剩下的所有字符。
* Unicode只是一个字符集，包含字符及其编号，但并不包含字符编号在计算机中的存储方式，也就是字符编码。

按照编码的复杂程度，我们来依次讲解Unicode字符集对应的3种字符编码：UTF-32、UTF-16、UTF-8。
* UTF-32：UTF-32是定长编码，使用4个字节来存储Unicode编号。定长的好处就是编码简单，只需要将字符编号直接存入计算机即可。读取时的解码也非常简单，每读取四个字节解码为一个字符。
* UTF-16： UTF-16采用变长编码，U+0 ~ U+FFFF范围（不包含U+D800 ~ U+DFFF）内的编号使用2字节编码，U+10000 ~ U+10FFFF之间的编号采用4字节编码。采用变长编码方式，比起定长的UTF-32编码方式，更加节省存储空间。但是，编解码也复杂了很多。当从一个文本中读取2个字节之后，我们怎么知道这2个字节对应的数值，是U+0 ~ U+FFFF范围的2字节编码，还是U+10000 ~ U+10FFFF范围内的4字节编码的高十六位或低十六位呢？为了解决这个问题，UTF-16将U+0 ~ U+FFFF之间的Unicode编号，直接存储在2个字节中，而对于U+10000 ~ U+10FFFF之间的Unicode编号，采用如下特殊编码方式。
  * STEP1：将U+10000 ~ U+10FFFF范围内的Unicode编号减去10000，得到新的范围：U+00000 ~ U+FFFFF，新的范围内的每个编号，只使用20个二进制位就能表示。
  * STEP2：将20个二进制位中的高10位取出，放到UTF-16的4字节编码中的高16位中，前面多出的6位用110110补全。这样高16位的数据范围就变成了U+D800 ~ U+DBFF。
  * STEP3：将20个二进制位中的低10位取出，放到UTF-16的4字节编码中的低16位中，前面多出的6位用110111补全。这样低16位的数据范围就变成了U+DC00 ~ U+DFFF。
  * 从上述UTF-16的解码过程，我们可以得知，在Unicode字符集中，在U+0 ~ U+FFFF这个范围内，U+D800 ~ U+DFFF这个范围的编号并没有使用，就是为了对2字节编码跟4字节编码的高16位和低16位数据做区分。
* UTF-8：相比UTF-16，UTF-8对不同字符占用存储空间的大小，控制得更加精细，当然，编码也更加复杂。UTF-8同样使用变长编码，其中包括4种类型的编码：1字节编码、2字节编码、3字节编码、4字节编码。不同范围内的编号使用不同的编码。
  * U+0000 ~ U+007F范围内的编号使用1字节编码
  * U+0080 ~ U+07FF范围内的编号使用2字节编码
​  * U+0800 ~ U+FFFF范围内的编号使用3字节编码
  * U+10000 ~ U+10FFFF范围内的编号使用4字节编码
  * 在UTF-8的编码规则中，1字节编码的首字节的前缀为0，2字节编码的首字节的前缀为110，3字节编码的首字节的前缀为1110，4字节编码的首字节的前缀为11110。尾随字节的前缀均为10。
  * 跟UTF-16编码类似，UTF-8这样编码的目的是，明确读取出来的字节，属于哪种类型的编码（1字节编码、2字节编码...）。因为UTF-8的最短编码长度是1字节，在读取二进制文件进行解码时，我们每次读取一个字节，判定是哪种类型的首字节编码。假如是3字节编码的首字节编码，那么我们再顺序往下读取2个尾随字节。

既然UTF-8比UTF-16采用更加复杂的编码，那么，在平时的开发中，使用UTF-8是不是一定比使用UTF-16更加节省存储空间呢？答案是否定的
* 仔细观察编号范围与编码长度，我们可以发现，如果在开发中，我们存储英文字符居多，那么，使用UTF-8更加节省空间，因为为了兼容ASCII码，Unicode中编号0 ~ 127之间的字符跟ASCII码一一对应，也就是说，英文字符的Unicode编号在0 ~ 127之间，使用UTF-8编码只需要1个字节长度，而是使用UTF-16编码则需要2个节长度
* 如果存储非英文字符居多，比如中文，那么使用UTF-16反倒会更加节省空间。因为常用的非英文字符，在UTF-16中编码长度为2字节，而在UTF-8中编码长度为2字节或3字节，并且3字节居多。

为何C/C++中char占1个字节，而Java中char占2个字节?
* 因为C语言出现的比较早，彼时多数计算机还只支持英文系统，而C++又继承了C语言的特性，所以，C/C++在设计char类型时，使其只占用一个字节长度，只能存储ASCII字符，如果要存储非ASCII字符，如中文字符，C/C++选择使用char数组（char[]）。
* Java出现较晚，彼时Unicode已经流行，为了让char类型表示更多的字符，Java设计了占两个字节长度的char类型，存储部分Unicode字符，编号在U+0 ~ U+FFFF之间的Unicode字符会通过UTF-16编码之后存储到char类型变量中。
* 因为Java中的char类型只占2个字节长度，所以，并不能存储所有的Unicode字符。不过，平时经常用到的字符，一般都是Unicode编号处于U+0 ~ U+FFFF之间的字符，为了避免存储空间的浪费，Java让char类型只占2字节长度，只存储常用字符。
* 剩下的U+10000 ~ U+10FFFF范围内的Unicode字符在Java中如何存储呢？类似C/C++存储ASCII码之外字符的做法，Java使用char数组来存储U+10000 ~ U+10FFFFF范围内的字符

跟ASCII码类似，我们也有3种方法将Unicode字符赋值给char类型变量：
* 对于可显示字符，我们可以直接使用字符来赋值给变量。
* 对于所有字符（可显示或不可显示），我们都可以将字符对应的UTF-16编码表示为\uxxxx的形式赋值给变量。其中，xxxx为16进制。
* 对于所有字符，我们都可以将字符对应的Unicode编号赋值给变量。

内编码和外编码
* Java中的char、String的字符编码被称为内编码。内编码指的是字符在内存中的编码格式。对应外编码指的是外部输入输出（比如文件、命令行参数、源码中的字符串常量等）的编码方式。
* 在Java中，内编码为UTF-16字符编码。外编码通过JVM参数-Dfile.endcoding来指定。如果我们在运行程序时没有指定外编码，那么，外编码跟系统编码一致。比如，Linux默认系统编码为UTF-8。在未指定外编码的情况下，运行在Linux系统下的Java程序默认按照UTF-8字符编码来读取文件中内容。

## 字符串：压缩、常量池、不可变
针对字符串，Java提供了String类，封装了字符数组（char[]），并提供了大量操作字符串的方法。

在JDK8中，String底层依赖char数组实现，核心的属性只有value数组和hash。
* value数组用来存储字符串，在JDK8中为char类型。JDK9对其进行了优化，将其改为byte类型
* hash属性用来缓存的hashcode

String 类相关方法
* +运算符：C++语言支持运算符重载，不过，Java语言并不支持运算符重载，一来，运算符重载的设计思想来自函数式编程，并不是纯粹的面向对象设计思想。二来，Java语言设计的主旨之一就是简单，摒弃了C++中的很多复杂语法，比如指针和这里的运算符重载。尽管程序员自己编写的类无法重载运算符，但Java自己提供的String类却实现了加法操作。如下代码所示，两个String对象可以相加。**String类型作为最常用的类型之一，延续了基本类型及其包装类的设计思想，同样支持加法操作，这样使用起来就比较方便和统一**。String对象还可以跟其他任意类型的对象相加，最终的结果为String对象与其他对象的toString()函数的返回值相加
* length()方法的返回值是char类型value数组的长度。不管是英文还是中文，均占用一个char的存储空间。
* valueOf()：Java重载了一组valueOf()方法，可以将int、char、long、float、double、boolean等基本类型数据转化成String类型
* compareTo()：字符比较大小，是将字符对应的UTF-16二进制编码，重新解读为16位的无符号数，再进行比较。字符串比较大小，是从下标0开始，两个字符串中的相同下标位置的字符一一比较，当遇到第一组不相等的字符时，根据这组字符的大小关系，决定两个字符串的大小关系。当然，如果短字符串跟长字符串的前缀完全相同，那么规定短字符串小于长字符串。

String的压缩技术
* String类作为在开发中最常用的类型之一，在性能和使用方便程度上，都理应做到极致。
* 在JDK8以及之前的版本中，String类底层依赖char类型的数组来存储字符串。而上一节讲到，char类型存储的是字符的UTF-16编码，一个字符占2个字节长度，因此，使用char类型来存储英文等ASCII字符，会比较浪费空间。因此，在JDK9中，Java对String类进行了优化，将存储字符串的value数组的类型，**由char类型改为了byte类型**。
* coder属性的值是通过分析字符串来得到。如果在所存储的字符串中，每个字符对应的UTF-16编码值（2字节编码）都小于等于127，那么，这就说明字符串中只包含英文字符。我们就对字符串进行压缩存储，使用1个字节存储1个字符。实际上，String类中的很多操作，比如计算字符串长度的length()，以及根据下标返回字符的charAt(int index)等，都依赖coder属性的值

String的常量池技术
* Integer、Long等基本类型的包装类，使用常量池技术，缓存常用的数值对象，起到节省内存的作用。String作为常用的数据类型，也效仿了基本类型的包装类的做法，设计了常量池，缓存用过的字符串。
* String类型跟Integer等包装类类似，使用new方式创建对象，并不会触发常量池技术，只有在使用字符串常量赋值时，才会触发常量池技术。字符串常量封装成String对象存储在字符串常量池中。如果字符串常量对应的String对象在常量池中存在，那么，JVM直接使用这个已经存在的String对象，否则，JVM在常量池中创建封装了字符串常量的String对象
* 除了使用字符串常量赋值之外，我们还可以使用intern()方法，将分配在堆上的String对象，原模原样在常量池中复制一份。在平时的开发中，什么时候使用intern()方法呢？当我们无法通过字符串常量来给String变量赋值（比如使用现成的API从文件或数据库中读取字符串），但又存在大量重复字符串（比如数据库中有一个“公司”字段，这个字段有大量重复值）时，我们就可以将读取到的String对象，调用intern()方法，复制到常量池中。之后在代码中使用常量池中的String对象，原String对象就被JVM垃圾回收掉。

String的不可变性
* 不可变的意思是：其对象在创建完成之后，所有的属性都不可以再被修改，包括引用类型变量所引用的对象的属性。
* 原因一：因为String类使用了常量池技术，所以，有可能存在很多变量同时引用同一个String对象的情况。如果String对象允许修改，某段代码对String对象进行了修改，其他变量因为引用同一个String对象，获取到的数据值也紧跟着被修改，这样显然不符合大部分的业务开发需求。
* 原因二：字符串和整型数经常用来作为HashMap的键（key）。在平常的开发中，我们经常将对象的某个String类型或整型类型的属性作为key，对象本身作为value，存储在HashMap中。如果之后属性值又改变了，那么，此对象在HashMap中的存储位置，需要作相应的调整，否则就会导致此对象在HashMap中无法被查询到。这显然增加了编码的复杂度。String类中的hash属性。String类定义了自己的hashcode()函数。当将对象存储在HashMap（哈希表）中时，HashMap会调用对象的hashcode()函数来计算哈希值。对于String不可变对象来说，因为hash值在计算得到之后就不会再改变，所以，使用一个属性hash来缓存这个值，避免重复计算。
* 原因三：String的设计思想非常贴近基本类型及其包装类，比如支持+运算符等。因为基本类型及其包装类都是不可变的，所以，String也延续了它们的设计思路，也设计为不可变的。

StringBuilder 类
* 因为String是不可变类，当我们在拼接多个字符串时，执行效率会比较低
* 为了解决这个问题，Java设计了一个新的类StringBuilder，支持修改和动态扩容。这样就避免了for循环创建大量的String对象。

## 对象的内存结构
在平时的开发中，在项目上线之前，我们需要合理的预估项目运行所需的内存空间，以便合理地设置JVM内存空间的大小。JVM内存空间分为很多部分：栈内存、堆内存、方法区等。
* 栈内存中存储的是生命周期很短的数据，这些数据所占用的内存在函数结束之后就会被释放。
* 方法区存储的是代码，几乎是固定不变的，并且占用的空间比较少。
* 堆内存中存储的主要是对象。想要合理估算项目运行所需的JVM内存空间，我们需要知道如何计算一个对象所占内存的大小。

Java对象在内存中的存储结构包含三部分
* 对象头（Header）
  * 包含标记字（Mark Word）、类指针和数组长度（只有数组才有）
  * 标记字在32位JVM中占4字节长度，在64位JVM中占8字节长度。标记字存储对象在运行过程中的一些信息，比如GC分代年龄（age）、锁标志位（lock）、是否偏向锁（biased_lock）、线程ID（thread）、时间戳（epoch）、哈希值（hashcode）等。标记字应该是Java对象存储结构中最复杂的部分。标记字记录的大部分信息，都用于多线程和JVM垃圾回收。
  * 类信息存储在方法区。为了方便获取某个对象所对应的类信息，对象头中会存储一个类指针，指向方法区中的类信息，也就是对应类信息在方法区中的内存地址。不过，有一个小问题：这里为啥叫“类指针”，而不是“类引用”呢？毕竟，Java中并没有指针，存储内存地址的是引用。实际上，这里的指针是指C++指针，因为JVM是用C++语言实现的，对象的存储结构、类的存储结构都是由C++代码来定义的。
  * JVM将数组作为一种特殊的对象来看待，其内存存储结构跟普通对象几乎一样。唯一的区别是，数组的对象头中多了数组长度这样一个字段。在32位JVM和64位JVM中，此字段均占4字节长度。从而我们也可以得知，在Java中，数组的最大长度为2^32-1。
* 实例数据（Instance Data）
  * 对象中的非静态成员变量
  * 实例数据存储的是对象里的非静态成员变量，可以是基本类型，也可以是引用类型。因为静态成员变量属于类，而非对象，所以，静态成员变量并非存储在对象中。
  * 在内存中，对象的每个属性的内存地址，必须是自身字节长度的倍数。比如，long型、int型、char型属性的内存地址必须分别是8、4、2的倍数，如果不是，需要补齐。这样的存储要求叫做“字节对齐”，补齐的方法叫做“字节填充”。除了属性需要对齐填充之外，对象整体也需要对齐填充。对象整体按照8字节对齐，不足8字节的，在对象末尾补足8字节。这样每个对象都从8的倍数的内存地址处开始存储。
  * 对象中的属性并非按照定义的先后顺序来存储的。在存在字节对齐和对齐填充的情况下，对象中的属性以不同的顺序来存储，会导致对象所占用的总内存大小有所不同。
* 对齐填充（Padding）
  * 保证对象存储地址按照8字节对齐（64位JVM）或4字节对齐（32位JVM）而做的填充
  * 我们拿64位CPU和64位JVM来举例讲解。**CPU按照字（Word）为单位从内存中读取数据**。对于64位CPU，字的大小为8字节，也就是说，内存以8字节为单位，切分为很多块，CPU每次读取其中的一块。
  * 对于长度为8字节的long、double、引用类型数据，为了能一次性将其从内存读入CPU缓存，JVM必须将其存储在划分好的一个8字节内存块中。如果不做字节对齐，一个数据有可能横跨两个内存块。这样CPU就需要进行两次读取并且拼接，才能将数据加载到CPU缓存中。这样做不仅效率低，还不能保证数据访问（读写）的原子性。
  * 对于长度小于8字节的数据类型，比如float、int、short、char、byte、boolean，为了让它们能一次性从内存读取到CPU缓存，JVM只需要按照类型长度对齐即可，并不需要8字节对齐。对于对象，因为其Mark Word占8个字节，为了能做到8字节对齐，对于大小不是8的整数的对象，JVM在对象的末尾对齐填充，补齐8字节。

那么为什么对于小于 8 字节的类型，如 float，一定需要按照类型长度对齐呢
* CPU 缓存行的地址解析和控制逻辑是按照固定的字节边界对齐来优化的，因此数据按照其本身类型的自然边界对齐，CPU 进行缓存行的读取和解析时，可以直接根据预定义的硬件逻辑快速定位到所在的位置，无需进行额外的地址调整和复杂的位运算
* 实际运行时，CPU 通常会一次性处理多个连续的数据，当数据按照合适的对齐方式存储时，例如一组 float 都按照 4 的位数地址进行存储，在进行批量数据读取时，CPU 可以充分利用缓存行的特性，一次性将多个连续的数据加载到缓存中

对于对象的内存结构，我们可以使用JOL（Java Object Layout）工具来查看。
* OFFSET表示字段相对于对象首地址的偏移地址，单位是字节。
* SIZE表示字段的大小，单位是字节。
* TYPE表示字段的类型。
* VALUE为字段值。

压缩类指针和引用
* 我们发现，类指针和引用为4字节大小，而非8字节大小。这是因为JVM默认开启了类指针压缩（-XX:+UseCompressedClassPointers参数）和引用压缩（-XX:+UseCompressedOops参数）
* 因为在编程开发时，我们会频繁地使用引用类型和类指针（每个对象都有一个类指针）。在64位JVM中，引用类型和类指针占用8字节，如果将其压缩至4字节，这将大大节省存储空间。
* 因为类指针的压缩方式跟引用的压缩方式相似，所以，我们拿引用压缩举例讲解。压缩之后的引用类型属性只占4字节，引用类型属性中存储的是对象的内存地址，4字节可以寻址的内存大小为2^32个字节（一个字节一个地址），也就是4GB。如果设置的堆大小超过4GB，那么，有些对象的地址就无法在引用类型属性中存储了。
* 对象是按照8字节进行字节对齐的，也就是说，对象的首地址是8的倍数，表示成二进制之后，后三位二进制位均为0。引用类型属性存储的是对象的首地址，既然后三位都为0，那也就没必要存储了。这样，32个二进制位可以存储长度为35个二进制位的地址（后三位不存）。因此，4字节的引用类型可以寻址的内存空间大小变成了2^35个字节，也就是32GB。当我们要读取引用类型属性所引用的对象时，先从引用类型属性中读取压缩之后的对象首地址，然后左移3位便可得到真正的对象首地址。
* 如果设置的堆大小超过32GB，即便JVM设置了开启引用压缩，引用压缩也不会生效，那么，怎么才能突破32GB这个堆大小限制呢？
* 之所以可以用4字节寻址32GB的内存空间，是因为对象按照8字节对齐。如果我们让对象按照16字节对齐，那么，对象的内存地址末尾的4位二进制位都为0，这样我们就可以用32个二进制位存储长度为36个二进制位的地址。4字节引用类型能寻址的内存空间大小变成了2^36个字节，也就是64GB。如果想继续扩大寻址范围，我们只需要使用JVM参数-XX:ObjectAlignmentInBytes调大对象的对齐长度即可。 参数-XX:ObjectAlignmentInBytes的取值范围为[8, 256]，并且必须为2的幂次方（2^n形式）。

> Java之所以在有Integer等包装类的情况下，仍然引入int等基本类型，其中一个重要原因就是节省内存。现在，我们就可以更加准确的分析一下，int相对于Integer到底节省了多少内存。一个int数据占据4字节内存，一个Integer对象，对象头占据8字节（Mark Word）+4字节（类指针）=12个字节，然后再加上仅有的int类型属性，总共占据16字节。对比来看，一个Integer对象所占内存大小，是int类型数据所占内存大小的4倍。如果项目中大量使用数值，那么使用基本类型变量来表示，就比包装类对象，节省大量内存空间了。

## 关键字
重点讲解final和static这两个关键词。

final 关键字：可以修饰类、方法和变量。
* final类：final修饰的类叫做final类。final类不可被继承。
* final方法：父类中的final方法在子类中不可被重写。子类可以将父类中的非final方法，重写为final方法。
* final变量：final修饰的变量叫做final变量或常量。final变量只能被赋值一次，之后就不能再被修改。final修饰的变量有三类：类的成员变量、函数的局部变量和函数的参数。对变量进行final修饰主要目的有两个，其一是避免不可预期的修改，其二是方便编译优化和运行时指令优化，比如指令重排序。对于引用类型变量，final关键词只限制引用变量本身不可变，但引用变量所引用的对象的属性或者数组的元素是可变的。

如何设计一个不可变类
* 我们需要将类设置为final类，这样类就无法被继承
* 将类中所有的属性都设置为final。当然，如果能保证类中没有方法会改变某个属性的值，也可以不用将这个属性设置为final
* 如果类中的方法返回的引用类型属性，那么，方法应该返回属性的副本而非本身。否则，外部代码可以通过引用，修改对象中的属性

static 关键字：可以修饰变量、方法、代码块、嵌套类
* static变量：final可以修饰类的成员变量、函数的局部变量、函数参数，而static却只能修饰类的成员变量。static修饰的变量叫做静态变量。静态变量隶属于类。静态变量跟类的代码一起，存储在方法区。我们经常把static和final放在一起来修饰变量，用static final修饰的变量叫做静态常量。对于一些跟具体对象无关，又不会改变的常量数据，我们一般将其存储在静态常量中。静态常量的命名方式比较特殊，所有字母都大写。
* static方法：用static修饰的方法叫做静态方法。我们可以在不创建对象的情况下，通过类来调用静态方法。很多工具类中的方法都设计为静态方法，比如Math类、Collections类中的方法。
* static代码块：如果静态成员变量的初始化操作无法通过一个简单的赋值语句来完成，那么，我们可以将静态成员变量的初始化逻辑，放入static修饰的代码块（即静态代码块）中，如下所示。静态代码块在类加载时被执行。如果类中有多个静态代码块，那么静态代码块的执行顺序跟书写顺序相同。
* static嵌套类：跟final相同，static也可以修饰类。只不过，static只能修饰嵌套类。嵌套类是指定义在一个类中的类，因此，嵌套类也叫做内部类。常用的内部类有3种：普通内部类、静态内部类、匿名内部类。承载内部类的类叫做外部类。内部类在编译成字节码之后，会独立于外部类，生成一个新的class文件，命名方式为：外部类名$内部类名.class。匿名内部类比较特殊，因为匿名内部类没有名字，所以，其命名方式为：外部类名$[序号].class。其中，[序号]为1、2、3...表示此匿名内部类是外部类的第几个匿名内部类。

静态内部类实现的单例为何线程安全并支持延迟加载?
* 静态变量的初始化是在类加载时，而类是在被用到时才会被加载，比如创建对象、创建子类的对象、调用静态方法、调用静态变量、使用反射时。在这几种情况下，JVM会先将类加载到方法区。其次，外部类加载并不会导致内部类的加载。再次，类的加载过程是线程安全。
* 当我们调用Singleton.getInstance()来获取单例对象时，JVM会先将Singleton类加载。紧接着，getInstance()函数访问SingletonHolder类的静态变量，触发JVM加载SingletonHolder类。而加载SingletonHolder类会触发静态变量的初始化操作，也就是执行SingletonHolder类中的唯一一行代码。
* 因为instance的创建是在SingletonHolder类加载过程中完成的，而类加载过程是线程安全的，所以，instance的创建过程是线程安全的。因为只有在第一次调用getInstance()函数时，instance才会被创建，所以，instance的创建过程满足延迟加载特性。再此之后，即便再调用getInstance()函数，因为SingletonHolder类都已经加载到JVM中，并且instance静态变量也已经初始化完成，所以，instance不会再被重新赋值，这就保证getInstance()函数返回的是同一个Singleton实例。
* 为什么SingletonHolder设计为静态内部类，是否可以设计为普通内部类？为什么将SingletonHolder设计为private而非public？普通内部类不能定义静态变量和静态方法，因此，如果SingletonHolder设计为普通内部类，那么instance将不能设置为static。这样就导致instance无法在类加载时创建，也就无法解决其创建过程的线程安全问题。除此之外，因为SingletonHolder类不会被除Singleton之外的代码使用，所以，我们将其设置为private而非public。

## 容器概述
Java提供了各种现成的容器，比如ArrayList、LinkedList、HashMap，这些容器是对常用数据结构的封装，比如，ArrayList是对数组的封装、LinkedList是对链表的封装、HashMap是对哈希表的封装。

List（列表）
* ArrayList：底层依赖的数据结构为可动态扩容的数组
* LinkedList：底层依赖的数据结构为双向链表
* Vector：线程安全的ArrayList，JDK1.0中还没有JCF，只有少数的几个容器，比如Vector、Stack、HashTable，而且，它们都是线程安全的。JDK1.2设计了JCF，定义了一套完善的容器框架，从功能上完全可以替代JDK1.0中引入的Vector、Stack、HashTable。但为了兼容，Java并没有将Vector、Stack、HashTable从JDK中移除。尽管这三个容器没有被废弃，但已经不推荐使用了。

> 为了更符合程序员的开发习惯，JCF将线程安全的容器和非线程安全的容器分开来设计。在非多线程环境下，我们使用没有加锁的非线程安全的容器，性能更高。对于线程安全环境，我们可以使用Collections类提供的一些方法将非线程安全的容器转化为线程安全的容器，或者使用JUC（java.util.concurrent）提供的更高性能的线程安全容器。

Queue（队列）
* 双端队列（Deque）：双端队列的首尾两端均可添加和获取元素，ArrayDeque就是基于数组实现的双端队列，LinkedList实现了Deque接口
* 优先级队列（PriorityQueue）：优先级队列底层依赖堆这种数据结构来实现。堆又分为小顶堆和大顶堆。默认情况下，优先级队列基于小顶堆来实现的，即最先出队列的元素为当前队列中的最小值。当然，我们也可以通过使用Comparator接口的匿名类对象，改变优先级队列的实现方式，改为基于大顶堆来实现，即最先出队列的元素为当前队列中的最大值。因为堆的构建过程，需要比较节点中数据的大小，所以，添加到优先级队列中的元素，需要能够比较大小。比较大小的方法有两种：基于Comparable接口和基于Comparator接口。

Set（集合）
* 不存在下标的概念，不允许存储相同的数据
* Set容器包括HashSet、LinkedHashSet、TreeSet。从代码实现上来说，这三个类底层分别是依赖HashMap、LinkedHashMap、TreeMap这三个类实现的。
* TreeSet举例讲解。往TreeSet中存储对象obj，实际上就是将obj值作为键，空的Object对象做为值，一并存储到TreeMap中

Map（映射）
* Map容器包括HashMap、LinkedHashMap和TreeMap
* TreeMap是基于红黑树来实现的。TreeMap基于键值对的键来构建红黑树，键值对的值作为卫星数据，附属存储在红黑树的节点中。具体红黑树的实现原理
* TreeMap底层依赖红黑树，红黑树的构建也需要比较元素的大小，因此，在使用TreeMap时，要么键值对中的键实现Comparable接口，要么在创建TreeMap时传入实现了Comparator接口的匿名类对象。

## 容器工具类
了解工具类Collections中常用且复杂的几个函数
* sort()函数：对于基本类型数组排序，在JDK8及其以后版本中，Arrays类使用DualPivotQuickSort来实现，在JDK7及其之前版本中，Arrays类使用快速排序来实现。对于对象数组，在JDK8及其以后版本中，Arrays类使用TimSort来实现，在JDK7及其之前版本中，Arrays类使用归并排序来实现。针对对象数组的排序，Arrays类使用稳定的TimSort来实现。对于基本类型数组来说，相等的数据谁在前谁在后，毫无差别，因此，针对基本类型数据的排序，Arrays类使用对稳定性没有要求的DualPivotQuickSort来实现。
* binarySearch()函数：对已排序的List容器进行二分查找操作，
* emptyXXX()函数：用来返回一个空的容器，emptyXXX()函数一般用来替代返回null值的情况，但需要注意Collections.emptyList()函数返回的是一个新类（EmptyList）的对象，并且这个对象是Collections的静态成员变量。
* synchronizedXXX()函数：JCF中的容器都是非线程安全的，那么，如果我们要使用线程安全的容器，该怎么办呢？当然，我们首选使用不管是性能还是功能都更优的JUC（java.util.concurrent）并发容器，如ConcurrentHashMap、CopyOnWriteArrayList等。当没有合适的JUC并发容器可以使用时，我们可以使用Collections类中的synchronizedXXX()函数来创建线程安全的容器。synchronizedXXX()函数的底层实现原理非常简单，仅仅通过对每个方法进行加锁来避免了并发访问问题。这也是导致其性能不高的原因。
* unmodifiableXXX()函数：用来返回不可变容器。这里的不可变指的是容器内的数据只能访问，不可增删。unmodifiableXXX()函数的实现也比较简单，其实现原理跟synchronizedXXX()函数的实现原理类似。如下代码所示，在Collections类中定义了新的UnmodifiableXXX类，并且，重写了add()、remove()等增删操作，让其抛出UnsupportedOperationException异常。

## HashMap
HashMap容器是基于哈希表实现的。HashMap容器将键和值包裹为Node类对象存储在哈希表中。Node类的定义如下所示。
```java
public class HashMap<K,V> extends AbstractMap<K,V>
    implements Map<K,V>, Cloneable, Serializable {

  static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    V value;
    Node<K,V> next;

    Node(int hash, K key, V value, Node<K,V> next) {
        this.hash = hash;
        this.key = key;
        this.value = value;
        this.next = next;
    }
    //...省略getter、setter等方法...
  }

  transient Node<K,V>[] table;
  //...省略其他属性和方法...
}
```

解决哈希冲突问题的方法有多种，其中比较常用的方法是链表法（也叫做拉链法），这也正是HashMap容器所使用的方法。如上代码所示，table数组用来存储各个链表。Node类表示链表的节点。在Node类中，key表示键，value表示值，next为链表的next指针，hash为由key通过哈希函数计算得到的哈希值。在查询元素时，HashMap用此值做预判等。

哈希函数：哈希函数是哈希表中非常关键的组成部分。HashMap中的哈希函数如下代码所示。
```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

HashMap的哈希函数设计得非常简单，将键的hashCode()函数的返回值h，与移位之后的h进行异或操作，最终得到的值就作为哈希值。因为key经过此哈希函数计算之后，得到的哈希值范围非常大，有可能会超过table数组的长度n，所以，我们需要将哈希值跟n取模，才能最终得到数据存储在table数组中的下标（我们把这个下标值简称为“key对应数组下标”）。因为取模操作比较耗时，所以，在具体实现时，Java使用位运算来实现取模运算，如下代码所示。key将存储在table[index]对应的链表中。
```java
int index = hash(key) & (n-1);
```

哈希函数的一些细节
* key的hashCode()函数：hashCode()函数定义在Object类中，根据对象在内存中的地址来计算哈希值，当然，我们也可以在Object的子类中重写hashCode()函数
* h ^ (h >>> 16)：在hash()函数中，为什么不直接使用key的hashCode()函数返回值作为哈希值呢？一般来说，table数组的大小n不会很大，一般会小于2^16（65536）。而hashCode()函数的返回值h为int类型，长度为4个字节。在计算key对应的数组下标时，h跟n求模后，h的高16位信息将会丢失，相当于只使用了h的后16位信息。理论上讲，参与计算的信息越多，得到的数组下标越随机，数据在哈希表的各个链表中的分布就会越均匀。因此，我们将hashCode()函数的返回值与其高16位异或，这样所有的信息都没有浪费。这里为什么使用异或而非与、或来对h和h>>>16进行操作呢？这是因为异或产生的结果更加随机。二进制位进行与操作，0出现的概率更高，二进制位进行或操作，1出现的概率更高，而只有二进制位进行异或操作，1和0出现的概率才相等。
* 取模h & (n-1)：之所以可以用位运算h&(n-1)来替代取模运算h%n，有一个极其关键的前提是：HashMap中的table数组的大小n为2的幂次方。比如，n=2^4，将其减一之后的二进制串为：1111，我们拿1111跟h求与，就相当于取模操作。
* key可为null值：值为null的key的哈希值为0，对应数组下标为0，也就是说，值为null的key也可以存储在HashMap中。不过，一个HashMap容器只能存储一个值为null的key，这符合HashMap容器不允许存储重复key的要求。

键的不可变性
* Node类中的hash属性。hash属性存储的是key的哈希值（也就是通过hash(key)计算得到的值）。这个值的作用是预判等，提高查询速度。将这个值作为属性存储在节点中目的是避免重复计算。
* 当我们调用get(xkey)函数查询键xkey对应的值（value）时，HashMap容器先通过hash(xkey)函数计算得到xkey的哈希值，假设为xhash。xhash跟table数组的大小n取模，假设得到的数组下标为xIndex，也就说明x应该出现在table[xIndex]对应的链表中。
* 遍历table[xIndex]所对应的链表，查找属性key等于xkey的节点。当遍历到某个节点node之后,首先会拿node.hash，与x的哈希值xhash比较，如果不相等，则说明node.key跟xkey肯定不相等，就可以继续比较下一节点了。
* 如果node.hash跟xhash相等，那么，这也并不能说明这个节点就是我们要找的节点。因为哈希函数存在一定的冲突概率，所以，即便哈希值相等，node.key也未必就跟xkey相等。因此，我们需要再调用equals()方法，比较node.key与xkey是否真的相等，如果相等，那么这个节点就是我们要查找的节点。如果不相等，则继续遍历下一个节点，然后再进行上述比较。
* node.key和xkey为对象，需要调用equals()函数比较是否相等。node.hash和xhash为整数，直接使用等号即可判等。后者比前者的执行效率更高。通过预先比对哈希值，过滤掉node.key和xkey不可能相等的节点，以此来提高查询速度。
* 一般来讲，我们使用Integer、Long、String等类型的数据作为键。这些类型有一个共同的特点，那就是不可变。如果键在存入HashMap之后被更改，而其在table数组中存储的下标不随之变化的话，那么，这个键将无法再被查询到。
* 尽管在绝大部分情况下，我们都会选择使用Integer、Long、String等Java内置不可变类作为HashMap容器的键，但是，在极个别情况下，我们也有可能需要使用自定义类作为键，如下代码中的SequenceId类。对于作为键的自定义类，我们需要重写Object父类的两个函数：一个是用于计算哈希值的hashCode()函数，另一个是用于数据判等的equals()函数。除此之外，为了避免键被更改之后无法再被查询到，自定义类中参与实现hashCode()函数和equals()函数的属性需要设置为不可变的。

装载因子
* 为了方便使用，JCF提供的容器基本上都支持动态扩容。当容器容纳不下新的元素时，容器便会开启扩容，将数据搬移到更大的存储空间中。HashMap使用链表法解决哈希冲突，并且链表可以无限长，因此，HashMap不存在无法容纳新元素的情况。但是，当HashMap容器中的数据越来越多时，在table数组大小不变的情况下，链表的平均长度会越来越长，进而影响到HashMap容器中各个操作的执行效率，此时就需要扩容了。
* 具体什么时候扩容，主要由table数组的大小（n）和装载因子（loadFactor）决定。当HashMap容器中的元素个数超过n * loadFactor时，就会触发扩容。其中，n * loadFactor在HashMap类中定义为属性threshold。
* 装载因子的默认值0.75是权衡空间效率和时间效率，精心挑选出来的。在平时的开发中，我们不要轻易修改装载因子，除非我们对时间和空间有比较特殊的要求，比如，如果更关注时间效率，我们可以适当减小装载因子，这样哈希冲突的概率会更小，链表长度更短，增删改查操作更快，但空间消耗会更大；相反，如果更关注空间效率，我们可以适当增大装载因子，甚至可以将其设置为大于1，这样table数组中的空闲空间就更少，不过，这也会导致哈希冲突概率更大，链表长度更长，增删改查以及扩容都会变慢。

动态扩容
* 当调用put()函数往HashMap容器中添加键值对之后，如果HashMap容器中键值对的个数是否超过threshold（table数组大小*装载因子），那么就会触发HashMap的动态扩容，即申请一个新的table[]数组，大小为原table数组的2倍，并将原table数组中的链表节点，一个一个的搬移到新的table[]数组中。
* 在进行扩容时，HashMap会逐一处理table数组中的每条链表。在JDK8中，HashMap中的table数组还有可能存储的是红黑树而非链表（这点待会会讲）。因为红黑树的处理方法，跟链表的处理方法类似，所以，我们拿链表举例讲解。
* 因为新的table数组大小newCap是原table数组大小oldCap的两倍，所以，一些节点在新table数组中的存储位置将会改变，我们需要重新计算其对应的数组下标。但因为每个节点的key的哈希值，已经存储在节点的hash属性中，所以，不需要调用哈希函数重新计算，只需要将节点node中存储的hash值跟newCap取模即可。取模操作仍然可以使用位运算来替代，也就是node.hash & (newCap-1)，由此得到节点node搬移到新的table数组中的位置下标。
* 实际上，在JDK8中，新的位置下标并非通过node.hash跟newCap取模计算得到的，其计算过程做了更进一步的优化：如果node.hash & oldCap == 0，则节点在新table数组中的下标不变；如果node.hash & oldCap != 0，则节点在新table数组中的下标变为i+oldCap（i为节点在原table数组中的下标）。

链表树化
* 尽管我们可以通过装载因子，使HashMap容器中不会装载太多的键值对，但这只能限制平均链表长度，无法限制单个链表的长度。
* 链表过长会导致HashMap性能下降，针对这个问题，JDK8做了一些优化。当链表中的节点个数大于等于8，并且table数组的大小大于等于64时，HashMap会将链表转化为红黑树。假设原链表中包含n个节点，那么，增删改查操作的时间复杂度为O(n)。当我们将链表转化为红黑树之后，增删改查操作的时间复杂度变为O(logn)，性能大大提高。我们把将链表转化为红黑树的过程叫做链表树化（treeify）。
* 链表树化比较耗时，并且，存储同样个数的节点，红黑树占用的空间要比链表更大（红黑树节点包含两个指针，而链表节点只包含一个指针），因此，我们希望链表树化极少发生。如果table数组长度小于64，即便链表中的节点个数大于等于8，那么，这种情况也不会触发链表树化，而是会触发动态扩容。HashMap试图通过扩容将长链表拆分为短链表。在小数据量的情况下，扩容要比树化更简单、更省时间。
* 跟树化相反的过程叫做反树化（untreeify）。当红黑树中节点个数较少时，HashMap会将红黑树重新转换回链表。毕竟维护红黑树平衡的成本比较高，对于少数节点来说，使用链表存储比使用红黑树存储更加高效。触发反树化的场景有两个：一个是删除操作，另一个是动态扩容。

## LinkedHashMap
继承自 HashMap，并且增加了排序功能。底层是哈希表和双向有序列表的结合，因此利用 LinkedHashMap 可以轻松实现 LRU 缓存。不过 LRU 一般会限制缓存大小，当超过这个大小限制后才会触发淘汰操作。

## 迭代器
常用的遍历容器的方法有 4 中：for 循环、for-each 循环、迭代器、forEach 函数。

对于简单容器，比如ArrayList、LinkedList，其底层存储结构比较简单，我们直接使用for循环遍历即可。但是，对于复杂容器，比如HashSet、TreeSet，其底层的存储结构比较复杂，对应的遍历逻辑也比较复杂。为了减少程序员自己实现遍历逻辑的开发成本，这些复杂容器内置了遍历逻辑，并且包裹为迭代器供程序员使用。除此之外，为了统一访问方式，Java为简单容器也提供了迭代器的遍历方式。

当通过迭代器来遍历容器时，增加或删除容器中的元素，会导致不可预期的遍历结果。实际上，“不可预期”比直接出错更加可怕。程序有的时候运行正确，有的时候运行错误。一些隐藏很深、很难发现的bug就是这么产生的。那么，如何才能避免这种不可预期的运行结果呢？

## 异常
除了异常，错误码也是常用的出错处理方式。相对于错误码，异常有诸多优势，比如可以**携带更多的错误信息**（exception中可以有message、stack trace等信息）等。除此之外，**异常可以将正常业务代码和异常处理代码分离**，使得代码的可读性更好。

Java定义了很多内建异常，如下图所示。Throwable是所有异常的父类。Throwable包含两个子类：Error和Exception。Exception又派生出了子类RuntimeException。因此，所有内建Java异常可以分为3类：继承自Error的异常、继承自Exception的异常、继承自RuntimeException的异常。

继承自的Error的异常，是一种比较特殊的异常，用来表示程序无法处理的严重错误，**这些错误有可能导致线程或JVM的终止**，比如OutOfMemoryError、StackOverflowError、NoClassDefFoundError等。

继承自Exception的异常叫做受检异常（Checked Exception）或**编译时异常（Compile Exception）**，比如IOException、FileNotFoundException、InterruptedException等。在编写代码的时候，我们**需要主动去捕获或者在函数定义中声明此类异常，否则编译就会报错**。

继承自RuntimeException的异常叫做非受检异常（Unchecked Exception）或者**运行时异常（Runtime Exception）**，比如NullPointerException、ArithmeticException、ArrayIndexOutOfBoundsException等。跟编译时异常相反，在编写代码的时候，我们不需要主动捕获或在函数定义中声明此类异常。编译器在编译代码时，并不会检查代码是否有对运行时异常做了处理。

自定义异常跟大多数内建异常一样，要么作为受检异常继承自Exception，要么作为非受检异常继承自RuntimeException。那么，具体该如何选择呢？

对于代码bug（比如数组越界）以及不可恢复异常（比如数据库连接失败），即便我们**对异常进行了捕获也无法挽救，因此，我们倾向于使用非受检异常。对于可恢复异常、业务异常、预期可能发生的异常，比如提现金额大于余额的异常，我们更倾向于使用受检异常**。

不过，在平时的项目的开发中，我们几乎都是**依赖框架来编程，即业务逻辑运行在框架中。对于程序员自定义的异常，不管是受检异常还是非受检异常，在大部分情况下都会被框架兜底捕获并处理，并不会直接导致程序的终止**。从这个角度上来看，在编写业务代码时，业务异常定义为受检异常和非受检异常的差别不大。不过，受检异常一直被人诟病。有些人主张所有的异常情况都应该使用非受检异常。支持这种观点的理由主要有以下三个。
* 受检异常需要显式地在函数定义中声明。如果函数的代码逻辑有可能抛出很多受检异常，那么，函数定义会非常冗长，如下示例函数所示，函数使用起来也不方便，而且还会影响代码的可读性。
* 受检异常的处理比较繁琐。当使用如上示例函数时，我们需要显示的捕获每个异常或者在函数定义中重复声明。而非受检异常正好相反，我们不需要在函数定义中显示声明，并且是否需要捕获处理，也可以自由决定。
* 受检异常的使用违反开闭原则。如果我们给某个函数新增一个受检异常，那么，调用这个函数的上游函数都需要做相应的代码修改，直到某个函数将这个新增的异常捕获处理不再抛出为止。相反，使用非受检异常可以减少代码的改动范围。我们可以灵活地选择在某个函数中集中处理非受检异常，比如，在Spring中的AOP切面中集中处理异常。

不过，非受检异常也有弊端，它的优点其实也正是它的缺点。从刚刚的表述中，我们可以看出，非受检异常使用起来更加灵活，怎么处理异常的主动权交给了程序员。过于灵活会带来不可控，一些本应该捕获处理的异常就有可能被程序员遗漏。除此之外，因为非受检异常不需要显式地在函数定义中声明，所以，在使用函数时，我们就需要查看函数的实现代码，才能知道函数具体会抛出哪些异常。

当程序抛出异常时，我们应该如何处理抛出的异常呢？一般来讲，我们有3种处理方法：**捕获后记录日志、原封不动再抛出、包装成新异常抛出**。

当代码抛出异常时，选择以上哪种处理方法，有一个重要的参考原则，那就是：**函数只抛出跟函数所涉及业务相关的异常**。在函数内部，如果某块代码的异常行为，并不会导致调用此函数的上层代码出现异常行为，也就是说，上层代码并不关心被调用函数内部的这个异常，我们就可以**在函数内部将这个异常“消化掉”，即将其捕获并打印日志记录**。相反，如果函数内部的异常行为会导致调用此函数的上层代码出现异常行为，那么，我们就必须让上层代码感知到此异常的存在。如果此异常跟函数的业务相关，上层代码在调用此函数时，知道如何处理此异常，那么，函数直接将此异常抛出就可。如果此异常跟函数的业务无关，上层代码无法理解这个异常的含义，不知道如何处理，那么，我们就需要将此异常包裹成新的跟函数业务相关的异常再抛出。

异常最终的宿命终究是被捕获并记录异常信息，以便程序员debug问题。为了给程序员展示充足的异常信息，我们一般需要将整个异常调用链完整打印出来。异常调用链跟函数调用链类似，记录异常发生的整个过程，即从当前捕获的异常一直追溯到引起这个异常的最起初的异常为止。

在平时的开发中，我们还需要特别注意，对于异常的处理，要么记录，要么抛出，两者不能同时执行。错误的做法如下所示。在异常调用链中，我们只需要在异常生命周期结束时，打印异常调用链即可，不需要每次抛出异常时都记录异常，否则，将会导致异常调用链的重复打印。

一般来讲，程序有3种执行结构：顺序、分支和循环，而异常的运行机制却非常特别，不符合这3种执行结构。从上述执行结果，我们发现，不管try监听的代码块有没有异常抛出，finally代码块总是被执行，并且，**在finally代码执行完成之后，try代码块和catch代码块中的return语句才会被执行**。异常的这种特殊的运行机制，其底层是如何实现的呢？

异常独特的运行机制的实现原理主要包含以下3个部分内容。
* 异常表：异常表（Exception table）对应上图字节码中最后一部分。其中，from、to、target均表示字节码的行号。当行号在[from, to)之间的代码抛出type类型的异常时，JVM会跳转至target行字节码继续执行。
* 异常兜底：主要是捕获try代码块和catch代码块中未被捕获的异常，然后，在执行完finally代码块之后，原封不动的将未被捕获的异常抛出。
* finally 内联：JVM在生成字节码时，会将finally代码块内联（也就是插入）到try代码块和catch代码块中的return语句之前，这样就可以实现不管程序是否抛出异常，finally代码块总是会在函数返回之间执行。

异常性能分析：如果程序未抛出异常，那么程序的性能完全不会因为try-catch语句的引入而下降。异常导致程序变慢的情况，只发生在异常被抛出时。当一个异常被抛出时，程序中一般会额外增加这样三个操作：使用new创建异常、使用throw抛出异常、打印异常调用链。
* 使用 new 创建异常
  * 在堆上创建异常对象，并初始化对象的成员变量
  * 调用 Throwable 的 fillStackTrace() 函数生成栈追踪信息
* 使用 throw 抛出异常
  * 当函数执行throw语句抛出异常时，JVM底层会执行“栈展开（stack unwinding）”，依次将函数调用栈中的函数栈帧弹出，直到查找到哪个函数可以捕获这个异常为止。
  * 相对于普通的函数返回（调用return语句）导致的栈帧弹出，调用throw语句导致的栈展开除了包含栈帧弹出之外，还增加了额外的操作，即在函数的异常表中查找是否有可匹配的处理规则。如果异常抛出之后，经过很多函数调用，最终才被捕获，那么，查询这些函数的异常表的耗时就会比较多。这就是异常导致程序变慢的另一个原因。
* 打印异常调用链

异常最佳实践
* 尽管异常的创建、抛出、打印都会消耗一定的时间，但是，少量异常并不会导致程序明显变慢。只有在极端情况下，比如，在项目中，我们定义了大量的较常发生的业务异常，比如查询用户不存在时抛出UserNotExistingException异常。在高并发的情况下，这类业务异常就有可能频繁发生，进而触发大量异常的创建、抛出和打印，最终导致程序明显变慢。
* 我曾经开发过一个限流框架。当访问被限流时，限流框架抛出OverloadException异常。当促销活动导致业务系统流量暴增时，限流框架在生效的同时，也不幸导致了业务系统的宕机。最后，我追踪发现，这个问题就是由异常引起。当大量访问被限流时，线程会大量创建、抛出、打印OverloadException异常，从而导致整个系统响应变慢甚至超时。那么，怎么来解决这个问题呢？
* 实际上，对于OverloadException业务异常，其作用更像是错误码，我们没必要记录stackTrace栈追踪信息，只需要将一些有用的信息记录在异常的detailMessage成员变量和cause成员变量中即可。我们可以调用Throwable中特殊的构造函数，禁止在创建异常的同时调用fillStackTrace()函数来生成stackTrace栈追踪信息。

## IO 类
在JDK1.4之前，Java引入了java.io类库，用来支持基本的I/O操作。在JDK1.4及其之后，Java引入了java.nio类库，用来支持**非阻塞I/O模型的开发**。在JDK7中，Java对java.nio类库进行了升级，引入了更多的类，用来支持**异步I/O模型的开发**。

按照数据流向来分类，java.io 类库中的类可以分为两类
* 输入流：InputStream、Reader
* 输出流：OutputStream、Writer

> 所谓输入流，指的是将文件、网络、标准输入（System.in）、管道中的数据，输入到内存中。所谓输出流，指的是将内存中的数据输出到文件、网络、标准输出（System.out、System.err）、管道中。

按照数据流的读写单位来分类，java.io 类库中的类可以分为两类
* 字节流：InputStream、OutputStream
* 字符流：Reader、Writer

> 所谓字节流，指的是按照字节为单位从输入流中读取数据，或者将数据写入输出流。所谓字符流，指的是按照字符为单位从输入流中读取数据，或者将数据写入输出流。实际上，相比字节流，字符流只是多了一个字符编码转换的环节。

从java.io类图中，我们可以发现，Java分别为字符流和字节流设计了两套类。这两套类的代码实现有些重复。**毕竟I/O读写操作都是相同的，唯一的区别只是数据解析的方式不同**。因此，为字节流和字符流设计两套类完全是没有必要的。java.nio利用“组合优于继承”的设计思想，**引入Channel和Buffer的概念，移除了大量重复代码**。

原始类分类
* 文件：跟文件读写相关的类有FileInputStream、FileOutputStream、FileReader、FileWriter。
* 网络：java.io类库并没有提供专门的类用于网络I/O的读写，而是直接复用了InputStream类和OutputStream类来读写网络I/O。除此之外，单独使用java.io类库也并不能完成网络编程，需要借助java.net类库的配合。
* 内存：跟内存读写相关的类有：ByteArrayInputStream、ByteArrayOutputStream、CharArrayReader、CharArrayWriter、StringReader、StringWriter。我们将内存看做一种特殊的I/O系统，也可以像文件一样，当作Stream来读写。这些类的主要作用是实现兼容。
* 管道：跟管道读写相关的类有PipedInputStream、PipedOutputStream、PipedReader、PipedWriter。这里的管道跟Unix操作系统中的管道不同。Unix操作系统中的管道是进程间的通信工具，而这里的管道是线程间的通信工具。一个线程通过PipedOutputStream写入数据，另一个线程可以通过PipedInputStream读取数据
* 标准输入输出：**在操作系统中，一般会有三个标准I/O系统：标准输入、标准输出、标准错误输出**。标准输入对应I/O设备中的键盘，标准输出和标准错误输出对应I/O设备中的屏幕。在Java中，我们使用System.in来处理标准输入，它是一个定义在System类中的静态InputStream对象。我们使用System.out和System.err来处理标准输出和标准错误输出，它们都是定义在System类中的PrintStream对象。PrintStream为装饰器类，需要嵌套OutputStream来使用，支持按照某种格式输出数据

装饰器类分类
* 支持读写缓存功能的装饰器类
  * 支持读写缓存功能的装饰器类有BufferedInputStream、BufferedOutputStream、BufferedReader、BufferedWriter。这4个类的作用非常相似。
  * 对比InputStream，BufferedInputStream会在内存中维护一个8192字节大小的缓存。当读取数据时，如果缓存中没有足够的数据，那么，read()函数会向操作系统内核请求数据，读取8192字节大小的数据存储到缓存中，然后read()函数再从缓存中返回需要的数据量。如果缓存中有足够多的数据，那么，read()函数直接从缓存中读取数据，不再请求操作系统。
  * 向操作系统内核请求数据，需要使用系统调用，引起用户态和内核态的切换，是非常耗时的。读取相同大小的数据，相比于普通的InputStream，BufferedInputStream请求操作系统内核的次数更少。不过，如果read()函数每次请求的数据量都大于等于8192字节，那么，缓存就不起作用了。
* 支持基本类型数据读写的装饰器类
  * DataInputStream支持将从输入流中读取的数据解析为基本类型（byte、char、short、int、float、double等）。DataOutputStream类支持将基本类型数据转化为字节数组写入输出流。
  * 调用DataOutputStream的readChar()、writeChar()函数，我们也可以按字符为单位读取、写入数据，但跟字符流类不同的地方是，DataOutputStream类一次只能处理一个字符，而字符流类可以处理char数组，并且字符流类提供的函数更多，功能更加丰富。
* 支持对象读写装饰器类：ObjectInputStream支持将从输入流中读取的数据反序列化为对象，ObjectOutputStream支持将对象序列化之后写入到输出流。
* 支持格式化打印数据的装饰器类
  * PrintStream和PrintWriter可以将数据按照一定的格式转化为字符串后，再写入到输出流。

## NIO 类
java.nio类库在JDK1.4中引入。nio的全称为**New I/O**。因为相对于java.io，java.nio新增了非阻塞的I/O访问方式，所以，有人把nio解读为**Non-blocking I/O**。除此之外，尽管java.nio从功能上可以完全替代java.io，但是，在平时的开发中，**对于普通的文件读写，我们更倾向于使用简单的java.io，而对于比较复杂的网络编程，我们更倾向于使用java.nio。基于此，还有人把nio解读为Network I/O**。

Stream是java.io类库中的一个核心概念。所有的I/O都抽象为Stream。读写Stream就等同于读写I/O。在java.nio中，**Stream被Channel所替代**。除此之外，java.nio还引入了一个新的概念：**Buffer**，用来存储待写入或读取的数据。除了上面提到的Buffer、Channel之外，java.nio中还有两个重要的概念：Selector和异步Channel。
* Buffer
  * 本质上是一块内存，相当于 java.io 编程时申请的 byte 数组
* Channel
  * 常用的Channel有FileChannel、DatagramChannel、SocketChannel、ServerSocketChannel。FileChannel用于文件读写。DatagramChannel、SocketChannel、ServerSocketChannel用于网络编程。DatagramChannel用来读写UDP数据，SocketChannel和ServerSocketChannel用来读写TCP数据。SocketChannel和ServerSocketChannel的区别在于，ServerSocketChannel用于服务器编程，可以使用accept()函数监听客户端（SocketChannel）的连接请求。
  * java.io中的Stream要么只能读，要么只能写，而java.nio中的Channel既可以读，也可以写。这也是java.nio比java.io类少的另一个重要原因。除此之外，Channel的设计也利用了“组合优于继承”的设计思想。 java.nio中包含大量的Channel接口，每个接口定义了一种功能。每个Channel类通过实现不同的接口组合，来支持不同的功能组合。
  * Channel有两种运行模式：阻塞模式和非阻塞模式。其中，FileChannel只支持阻塞模式。DatagramChannel、SocketChannel、ServerSocketChannel支持阻塞和非阻塞两种模式，默认为阻塞模式。我们可以调用configureBlocking(false)函数将其设置为非阻塞模式。非阻塞Channel一般会配合Selector，用于实现多路复用I/O模型。那么，到底什么是阻塞模式？什么是非阻塞模式呢？
  * Channel有两种运行模式：阻塞模式和非阻塞模式。其中，FileChannel只支持阻塞模式。DatagramChannel、SocketChannel、ServerSocketChannel支持阻塞和非阻塞两种模式，默认为阻塞模式。我们可以调用configureBlocking(false)函数将其设置为非阻塞模式。非阻塞Channel一般会配合Selector，用于实现多路复用I/O模型。那么，到底什么是阻塞模式？什么是非阻塞模式呢？
  * 在操作系统层面，主要的I/O有：文件、网络、标准输入输出、管道。文件是没有非阻塞模式的。毕竟文件不存在不可读和不可写的情况。网络、标准输入输出、管道都存在阻塞和非阻塞两种模式。我们拿最常用的网络来举例讲解。
  * 一般来讲，应用程序调用read()或write()函数读取或写入数据，数据会在应用程序缓冲区、内核缓冲区、I/O设备这三者之间拷贝传递。当调用read()函数读取数据时，如果内核读缓冲区中没有可读数据，比如网络连接的对方此时并未发送数据过来，那么，在阻塞模式下，read()函数会等待，直到对方发送数据过来，内核读缓冲区中有数据可读时，才会将内核读缓冲区中的数据拷贝到应用程序缓存中，然后read()函数才返回，在非阻塞模式下，read()函数会直接返回并报告读取情况。当调用write()函数时，如果内核写缓冲区中没有足够空间承载应用程序缓存中的数据，比如网络不好，原来的数据还没来得及发送出去，那么，在阻塞模式下，write()函数会等待，直到内核写缓冲区中有足够空间，并且应用程序缓冲区中的数据全部写入内核写缓冲区，write()函数才会返回。在非阻塞模式下，write()函数会能写多少写多少，即便还有一部分未能写入内核写缓冲区，也不会等待，直接返回并报告写入情况。
  * 实际上，除了read()和write()函数有阻塞和非阻塞这两种模式之外，ServerSocketChannel中用于服务器接收客户端连接的accpet()函数，也有阻塞和非阻塞两种模式。在阻塞模式下，调用accept()函数会等待，直到有客户端连接到来才返回。在非阻塞模式下，如果没有客户端连接到来，调用accept()函数会直接返回。
* Selector
  * 在网络编程中，如果我们使用非阻塞模式的read()、write()、accept()函数，那么，我们需要通过while循环，不停轮询调用read()、write()、accept()函数，查看是否有数据可读、是否可写、是否有客户端连接到来。
  * 多路复用I/O模型是网络编程中非常经典的一种I/O模型。为了实现多路复用I/O模型，Unix提供了epoll库，Windows提供了iocp库，BSD提供了kequeue库...Java作为一种跨平台语言，对不同操作系统的实现方式进行了封装，提供了统一的Selector。
  * 我们可以调用register()函数，将需要监听的Channel注册到Selector中。Selector底层会通过轮询的方式，查看哪些Channel可读、可写、可连接，并将其返回处理。
* 异步 Channel
  * 尽管使用Selector可以避免程序员自己手写轮询代码，但是，Selector底层仍然依赖轮询来实现。在JDK7中，java.nio类库做了升级，引入了支持异步模式的Channel，主要包括：AsynchronousFileChannel、AsynchronousSocketChannel、AsynchronousServerSocketChannel。而前面讲到的Channel都是同步模式的。那么，什么是同步模式？什么是异步模式呢？同步和异步这两个概念，跟阻塞和非阻塞又是否有联系呢？
  * 在异步模式下，Channel不再注册到Selector，而是注册到操作系统内核中，由内核来通知某个Channel可读、可写或可连接，java.nio收到通知之后，为了不阻塞主线程，会使用线程池去执行事先注册的回调函数。

> 我们通过一个生活中的例子来形象解释一下。假设你去一家餐厅就餐，因为就餐的人太多，需要取号等位。取号之后，如果你站在餐厅门口一直等待被叫号，啥都不干，那么，这就是阻塞模式。如果你先去商场里逛一逛，一会回来看一下有没有轮到你，没有就继续再去逛，那么，这就是非阻塞模式。如果你在取号时，登记了手机号码，那么，你就可以放心去逛商场了。等叫到你的号时，服务员会打电话通知你，这就是异步模式。相反，如果需要自己去查看有没有轮到你，不管是阻塞模式还是非阻塞模式，都是同步模式。

Java IO模型
* 阻塞 I/O 模型（BIO）
  * 阻塞I/O模型指的是利用阻塞模式来实现服务器。一般来说，这种模型需要配合多线程来实现。
  * 一般来讲，服务器需要连接大量客户端，因为read()函数是阻塞函数，所以，为了实时接收客户端发来的数据，服务器需要创建大量线程，每个线程负责等待读取（调用read()函数）一个客户端的数据。因为java.io支持阻塞模式，java.nio既支持阻塞模式又支持非阻塞模式，所以，java.io和java.nio都可以实现阻塞I/O模型。
  * 如果有n个客户端连接服务器，那么，服务器需要创建n+1个线程，其中n个线程用于调用read()函数，另外一个线程用来调用accept()阻塞函数。当连接的客户端非常多时，服务器需要创建大量线程，而每个线程会分配一个线程栈，需要占用一定的内存空间。当线程比较多时，内存资源的消耗就会比较大。并且大量线程来回切换，也会导致服务器整体处理性能的下降。除此之外，大部分线程可能都阻塞在read()函数上，等待数据的到来，什么都不做但又要白白占用内存和线程资源，非常浪费。
* 非阻塞 I/O 模型（NIO）
  * 非阻塞I/O模型指的是利用非阻塞模式来开发服务器，一般需要配合Selector多路复用器。因此，这种模型也叫做多路复用I/O模型。因为java.io只支持阻塞模式，所以，这种模型只能通过java.nio来实现。
  * 如果有n个客户端连接服务器，那么，就会创建n+1个Channel，其中一个serverChannel用于接受客户端的连接，另外n个clientChannel用于与客户端进行通信。这n+1个Channel均注册到Selector中。Selector会间隔一定时间轮训这n+1个Channel，查找可连接、可读、可写的Channel，然后再进行连接、读取、写入操作。
  * 多路复用I/O模型只需要一个线程即可，解决了阻塞I/O模型线程开销大的问题。不过，这种模型依然存在问题。如果某些clientChannel读写的数据量比较大，或者逻辑操作比较复杂，耗时比较久，因为所有的工作都在一个线程中完成，那么其他clientChannel便迟迟得不到处理，最终的效果就是，服务器响应客户端的延迟很大。
  * 为了解决这个问题，我们可以引入线程池，对于Selector检测到有数据可读的clientChannel，我们从线程池中取线程来处理。我们知道，阻塞I/O模型也用到了多线程，跟这里的区别在于，不管有没有数据可读，阻塞I/O模型中的每个clientSocket都会一直占用线程。而这里的多线程只会处理经过Selector筛选之后有可读数据的clientChannel，并且处理完之后就释放回线程池，线程的利用率更高。
* 异步 I/O 模型（AIO）
  * 实际上，上述多路复用加线程池所实现的I/O模型就是异步I/O模型。对于异步I/O模型，使用java.nio的异步Channel来实现更加优雅。示例如下代码所示。如果我们通过异步Channel调用accept()、read()、write()函数，那么，当有连接建立、数据读取完成或数据写入完成时，底层会通过线程池执行对应的回调函数。

> 实际上，在平时的开发中，我们一般不会直接使用java.nio类库，而是使用Netty等框架来进行网络编程，这些框架封装了网络编程的复杂性，使用起来更加简单，开发效率更高。

实际上，不同的操作系统会提供不同的I/O模型。Java是一种跨平台语言，为了屏蔽各个操作系统I/O模型的差异，设计了3种新的I/O模型：BIO（阻塞I/O）、NIO（非阻塞I/O）、AIO（异步I/O），并且提供了I/O类库来支持这3种I/O模型的代码实现。而Java的I/O类库底层需要依赖操作系统的I/O接口来实现，因此，从本质上来讲，Java I/O模型只是对操作系统I/O模型的重新封装。

在某些情况下，我们确实必须使用java.nio，比如网络编程。尽管使用java.io，并配合java.net，也可以进行网络编程，但java.io只支持阻塞模式，只能实现阻塞I/O模型，对于大部分网络编程来说，都是不够的。而java.nio提供了非阻塞模式、Selector多路复用器、异步模式，能够实现更加高性能的网络模型，比如非阻塞I/O模型、异步I/O模型。相比java.io而言，在网络编程方面，java.nio的优势更加明显。

对于使用java.io还是java.nio进行文件读写，按照你的喜好或者团队的编程习惯来选择就好。总结一下的话，对于网络编程，我们首选java.nio，对于文件读写，java.io和java.nio都可以。

## 高速 I/O
尽管在平时的业务开发中，我们很少会用到它们 IO 类库和 NIO 类库，但是，对于一些常用中间件、基础系统，比如Kakfa、RocketMQ、MySQL等，其内部实现涉及大量的文件和网络等I/O读写操作。I/O读写是否高效，直接决定了这些中间件和基础系统的性能，是优化的重中之重。

要了解I/O读写的底层实现原理，我们需要先了解两个非常重要的概念：内核态和用户态。

操作系统包含计算机运行所需要的核心程序，这部分程序用来访问硬件资源，比如调度CPU、读写磁盘、网卡、内存等。我们把这部分程序叫做操作系统内核（简称内核）。因为操作硬件资源非常容易出错，并且一旦出错，错误将非常严重，大部分情况下都会导致计算机宕机，所以，操作系统不允许应用程序直接访问硬件资源。如果应用程序需要访问硬件资源，比如读写磁盘，那么，只能通过操作系统提供的API来实现。我们把操作系统提供给应用程序使用的API称为系统调用。

系统调用比较底层，使用起来不够方便，于是，Linux操作系统在此之上又提供了库函数，比如Glibc库、Posix库，对系统调用进行封装，提供更加简单易用的函数，供应用程序开发使用，比如Glibc中的malloc()函数底层封装了sbrk()系统调用，fread()、fwrite()函数底层封装了read()、write()系统调用。在进行应用程序开发时，我们既可以使用库函数，也可以直接使用系统调用，比如，对于内存分配，我们一般使用malloc()库函数，对于文件读写，我们一般直接使用read()、write()系统调用。

除此之外，Linux操作系统还提供了Shell这一特别的程序，也就是我们平时所说的命令行。Shell让我们能够在不进行编程的情况下，通过在命令行中运行Shell命令或脚本，达到访问硬件的目的，比如，使用cp命令拷贝文件，使用rm命令删除文件等。

为了避免应用程序在运行时，访问到内核所用的内存空间，操作系统将虚拟内存空间分为内核空间和用户空间两部分。而我们经常提到的内核态和用户态，实际上指的是CPU运行所处的状态。当CPU执行内核程序时，CPU进入内核态。在内核态下，CPU拥有最高权限，可以执行所有的机器指令，当然，也可以访问硬件设备。当CPU执行应用程序时，CPU进入用户态，在用户态下，CPU权限被限制，只能执行部分机器指令，当然，也无法访问硬件设备。除此之外，在内核态下，CPU可以访问所有的虚拟内存空间，包括用户空间和内核空间，在用户态下，CPU只能访问用户空间，不能访问内核空间。

相比于应用程序内的普通函数调用，系统调用要慢的多，耗时的地方主要在于内核态和用户态的上下文切换。
* 寄存器保存与恢复耗时
* 缓存失效带来的性能损耗

CPU 减负之 DMA 技术

从I/O读写的底层实现原理，我们可以发现，在I/O读写过程中，CPU一直参与其中，负责I/O设备与内核缓冲区之间，以及内核缓冲区与应用程序缓冲区之间的数据拷贝。而CPU最擅长的是运算，比如加法运算、位运算，让CPU去做拷贝数据这种简单工作（将二进制位0或1从一个存储单元移动到另一个存储单元），实际上是大材小用。除此之外，相对于CPU来说，像硬盘、网卡等I/O设备的读写速度非常慢，在I/O读写的过程中，CPU会一直被占用，无法处理其他事情，这无疑是非常浪费CPU资源的。

于是，科学家们便发明了DMA（Direct Memory Access）技术。通过在主板上安装一个叫做DMAC（DMA Controller，DMA控制器）的协处理器（或叫芯片），协助CPU来完成I/O设备的数据读写工作。随着计算机的发展，安装在计算机上的I/O设备越来越多，仅在主板上安装一个通用的DMAC已经远远不够了，因此，现在很多I/O设备都自带DMAC，比如硬盘、网卡、显示器都有各自的DMAC。

具体来讲，DMA是怎样工作的呢？

当调用read()函数从I/O设备读取数据时，通过系统调用，CPU会进入内核态，发送I/O请求到DMAC，告知DMAC从I/O设备中读取哪些数据到哪块内存，之后CPU便去做其他事情，由DMAC来完成将数据从I/O设备拷贝到内核缓冲区的工作。当DMAC完成拷贝工作之后，DMAC通过中断，通知CPU内核缓冲区中的数据已经准备就绪，然后CPU再将内核读缓冲区中的数据拷贝到应用程序缓冲区。

当调用write()函数将数据写入I/O设备时，通过系统调用，CPU会进入内核态，将数据从应用程序缓冲区拷贝到内核缓冲区，然后发送I/O请求给DMAC，告知其将哪块内存中的数据拷贝到I/O设备中，之后CPU便去做其他事情了，由DMAC来完成将数据从内核写缓冲区中拷贝到I/O设备中的工作。

通过DMA技术，不管是读取I/O数据还是写入I/O数据，CPU只需要参与一次数据拷贝，I/O操作不再占用大量的CPU资源，CPU利用率提高。

我们介绍了I/O读写的底层实现原理，不管读还是写，即便存在DMA，**都需要进行2次数据的拷贝和1次系统调用，而1次系统调用又会导致2次用户态和内核态的上下文切换**。如何利用mmap和零拷贝技术，对上述读写过程进行优化，以此来提高I/O读写速度。

mmap（memory-mapped file，内存映射文件）是提高文件读写性能的有效技术。注意，mmap一般用于文件，像网络这种数据未知的I/O设备，不适合使用mmap。
* 和普通I/O读写的底层实现原理，已经完全不同了，已经不再强调区分内核缓冲区和应用程序缓冲区这两个概念了，取而代之，我们需要对操作系统中的**物理内存、虚拟内存、缺页中断**有一定了解
* 物理内存被操作系统中同时运行的多个进程所共享，你占几块，我占几块，他占几块...每个进程都要记录自己占用了哪几块。进程操作这些不连续的内存地址会比较复杂，于是，操作系统便在物理内存之上，抽象出来了虚拟内存的概念。每个进程都有一个独享的虚拟内存，地址从0开始并且是连续的，操作系统负责记录虚拟内存跟物理内存之间的映射关系。这样每个进程只需要操作虚拟内存地址即可，操作系统负责将虚拟内存地址转化成物理地址。
* 操作系统在执行程序时，并不是把整个程序都加载到物理内存再执行。毕竟物理内存是有限的，并且操作系统还要同时运行多个程序。操作系统只会在物理内存中加载程序的一小段代码。如果CPU在执行代码的过程中，发现待执行的代码没有在物理内存中，那么，就会向操作系统发出一个缺页中断。操作系统接收到缺页中断之后，会将待执行的代码从磁盘加载到物理内存。如果物理内存中没有空闲空间存储待执行的代码，操作系统会将不再被执行的代码置换出物理内存，并回写到磁盘。利用这种置换（swap）机制，即便物理内存很小，我们也可以运行很大的程序。

mmap()函数返回的ptr指针，存储的就是文件映射到虚拟内存之后的首地址。使用mmap技术，我们不需要使用read()、write()函数，直接操作虚拟内存空间（也就是操作ptr），即可实现对文件的读写。

mmap相当于直接将数据在磁盘和用户空间之间互相拷贝，相对于使用read()、write()系统调用读写文件，数据拷贝次数由2次减少为1次。除此之外，使用mmap读写文件，只需要在开始时，调用一次mmap()系统调用建立好映射关系，之后读写文件就像读写内存一样，并不需要使用read()、write()系统调用，这也减少系统调用引起的用户态和内核态上下文切换的耗时。

那么，相比于使用read()、write()读写文件，使用mmap读写文件是不是就一定性能更高呢？显然，这个答案是否定的。不然，为什么还会有那么多项目使用read()、write()来读写文件呢？

mmap能够减少一次数据拷贝，但这并不是免费的。mmap实现原理更加复杂。使用mmap将会带来一些额外的开销，比如建立虚拟内存与文件之间的映射会比较耗时，缺页中断导致上下文切换也会比较耗时。对于少量文件读写，使用read()、write()函数更加合适。尽管使用read()、write()函数需要进行两次数据拷贝，但是拷贝的数据量都很小，并不会太影响读写性能。对于大文件的读写，数据拷贝非常影响读写性能，因此，使用mmap的优势就更加明显。一般来讲，在项目中使用mmap之前，我们需要做一些测试来验证，其是否能真正提高读写性能。

mmap和零拷贝都可提高I/O的读写速度，只不过，它们作用的场景不同。mmap主要用于文件读写，而零拷贝（Zero-copy）主要用于两个I/O设备之间互相传输数据。特别是在将文件中的数据发送到网络或者将从网络接收的数据存储到文件这一场景中经常会用到零拷贝技术。

零拷贝并不是完全没有拷贝，而是减少了不必要的拷贝次数。使用零拷贝技术，我们不需要将数据从内核读缓冲区拷贝到应用程序缓冲区，而是直接从内核读缓冲区拷贝到内核写缓冲区。这样就节省了1次数据拷贝。除此之外，为了实现将文件发送到网络，普通的I/O流程需要调用2次系统调用（先执行read()，再执行write()），而使用零拷贝技术，应用程序只需要调用一次系统调用（执行sendfile()）。系统调用减少了一次，用户态和内核态的上下文切换减少了2次，性能也就得到了提高。

Linux操作系统提供了sendfile()系统调来实现零拷贝技术。除了文件到网络的数据传输，对于两个文件之间的数据传输，我们也可以使用零拷贝技术。

## 泛型
泛型在Java中是一个非常重要的语法。Java中的容器都支持泛型。Java泛型不支持int等基本类型，因此，在Java容器中不能存储基本类型数据，比如List< int >这样是不合法的。而C++泛型支持int等基本类型，因此，在C++容器中可以存储基本类型数据，比如vector< int >这样是合法的。那么，为什么C++泛型支持int等基本类型而Java泛型不支持呢？

泛型本质上就是对类型的参数化。在类的定义中，我们可以把类型当做参数。当使用类时，我们向类型参数传入具体类型。

对于泛型，除了以上基本用法之外，我们还可以使用extends上界限定符，限定类型参数的具体取值范围。例如，< T extends Person > 表示限定传入类型参数的具体类型必须是Person或者Person的子类。需要注意的是，泛型中只有extends，没有implements。**这里的extends既可以表示类型继承，也可以表示接口实现**。例如，< T extends Closeable > 表示限定传入类型参数的具体类型必须实现了Closeable接口。

除了类型参数之外，泛型中还有另外一个常用语法：**？通配符**。

通配符跟类型参数的应用场景并不相同。类型参数一般用来定义泛型类、泛型接口和泛型方法，而通配符跟Integer、Person、String这些具体类型无异，用来具体化泛型类或泛型接口，可以看做一种特殊的具体类型。当我们在具体化某个泛型类或泛型接口，但又无法指明明确的具体类型时，我们就可以使用通配符这种特殊的具体类型。

通配符常用于方法参数中，当方法中的某个参数为泛型类或接口时，如果我们无法指定具体的类型，那么就可以使用通配符来表示可以匹配任意类型。示例代码如下所示。reverse()方法中的list对应的类是一个泛型类。在使用时，我们需要为list传入具体类型，但是，在reverse()函数定义中，我们并不知道具体的类型是什么，因此，我们就用通配符来替代具体类型。
```java
public class Collections {
  public static void reverse(List<?> list) { ... }
}
```

当然，在reverse()函数中，我们也可以使用类型参数替代通配符，如下所示，只不过，此时的reverse()函数便是一个泛型方法。在泛型方法中，方法的前面需要添加< T >;类型参数声明，而使用通配符定义的reverse()函数中，并没有类型参数声明。这是两者的主要区别。
```java
public class Collections {
  public static <T> void reverse(List<T> list) { ... }
}
```

在以下两种情况下，我们只能使用通配符，而不能使用类型参数。
* <? super Person>
* <? extends T> 或 <? super T>

前面我们只讲到了extends上界限定符，实际上，对应地还有super下界限定符。extends上界限定符既可以用于类型参数（如< T extends Student >），也可以用于通配符（如< ? extends Student >）。而super下界限定符只能用于通配符，比如< ? super Student >，**表示传入通配符的具体类型为Student或者Student的父类**。

通配符可以extends或super类型参数，但类型参数不可以extends或super类型参数。示例代码如下所示。
```java
// 合法
public <T> void copy(List<? super T> dest, List<? extends T> src) { ... }

// 非法
public <T, U, S> void copy(List<U super T> dest, List<S extends T> src) { ... }
```

泛型的类型擦除
* 泛型只是一个语法糖，在编译时，编译器会使用泛型做类型检查，但是代码编译为字节码之后，泛型中的类型参数和通配符通通替换成上界，比如 < T > 替换为 Object，< T extends String > 替换为 String
* 因为Java泛型的类型擦除，我们不能使用new T()来创建类型参数对象。在代码编译成字节之后，类型信息已经擦除，因此，在运行时，JVM无法确定具体类型，也就无法知道T是否存在无参构造函数，也就无法使用new来创建T对象了。
* 除此之外，Java泛型这种独特的实现方式，也导致了只有引用类型才可以作为类型参数，而基本类型并不继承自Object，无法做类型擦除，因此无法作为类型参数。也就是说，Java泛型并不支持基本类型。
* Java泛型无法实现基本类型，也带来了一些开发上的困难，比如讲到的用于排序的DualPivotQuickSort类，为了支持不同的基本类型，分别定义了不同的排序函数，而每个函数都要重复实现一遍类似的排序逻辑，代码实现非常不美观。
* C++也支持泛型，只不过它有另一个叫法，叫做模板（Templates），之所以C++泛型可以支持基本类型，是因为其底层实现方式跟Java泛型的实现方式完全不同。C++中的泛型有点类似宏定义。当某个cpp文件用到泛型类时，编译器会将泛型类中的类型参数，替换为具体类型，也就是将泛型类转换为具体类，然后内联到这个cpp文件中。如果有多个cpp文件使用同一个泛型类，那么就要生成多个具体类。可想而知，这种实现方式并不高效。对于一个泛型类，JVM中只需要保存一个类型擦除之后的类即可，但是C++需要生成多个不同的具体类。

## 反射
尽管在平时的业务开发中，我们很少会用到**反射、注解、动态代理**这些比较高级的Java语法，但是，在框架开发中，它们却非常常用，可以说是支撑框架开发的核心技术。比如，我们常用的Spring框架，**其中的IOC就是基于反射实现的，AOP就是基于动态代理实现的，配置就是基于注解实现的**。尽管在业务开发中我们不常用到它们，但是要想阅读框架的源码，掌握这些技术是必不可少的。

反射的作用
* 创建对象：并不是所有对象的创建，都是在编写代码时实现知道的，如果在代码运行过程中，需要根据配置、输入、执行结果等，动态创建一些额外的对象，这时候就无法使用 new 了，因此在程序运行期间，动态地告知 JVM 去创建某个类的对象，这种方法就是反射
* 执行方法：跟创建对象类似，尽管执行方法总是发生在运行时，但是申请执行方法的时机却可以不同
* 获取类信息：除了创建对象、执行方法之外，反射还能够获取对象的类信息，包括类中的构造函数、方法、成员变量等信息。稍后要讲到的注解，实际上，就是依赖反射的这个作用。

反射的用法需要 4 个类
* Class：Class类提供了大量的方法，可以获取类的信息。有如下三种方式创建 Class 对象
  * Class<?> clazz = Class.forName("com.demo.Student")
  * Class<?> clazz = Student.class
  * Class<?> clazz = student.getClass()
* Constructor：用于存储构造函数信息
  * Class类中也包含newInstance()方法。区别在于，Class类中的newInstance()方法只能通过无参构造函数来创建对象，而Constructor类中的newInstance()方法支持通过有参构造函数来创建对象。
* Method：用于存储方法相关的信息
  * invoke() 方法可以执行其参数中指定的方法
* Field：存储成员变量的信息

反射攻击：Constructor、Method、Field这三个类包含一个公共的方法，如下所示，它能够改变构造函数、方法、成员变量的访问权限。
```java
public void setAccessible(boolean flag);
```

利用这个方法，我们可以将私有的构造函数、方法、成员变量设置为可访问的，这样就可以越过访问权限的限制，在代码中访问私有的构造函数、方法和成员变量。

反射的应用：Spring 可以看做是一个 IOC 容器，IOC 容器就是一个大的工厂类，负责在程序启动时，根据配置事先创建好对象。当应用程序需要使用某个对象时，直接从容器中获取即可。

在普通的工厂模式中，工厂类要创建哪个对象是事先确定好的，并且是写死在工厂类代码中的。作为一个通用的框架来说，框架代码跟应用代码应该是高度解耦的。IOC容器事先并不知道应用会创建哪些对象，不可能把某个应用要创建的对象写死在框架代码中。一般的做法是，应用程序通过配置文件定义好需要创建的对象。IOC容器读取配置文件，并将每个要创建的对象信息解析为一定的内存结构：BeanDefinition，然后根据BeanDefinition中的信息，通过反射创建对象。

相比普通的对象创建和执行，使用反射创建对象和执行方法，增加的额外耗时产生在哪里呢？
* 安全性检查：对于普通的对象创建和执行，大量的安全性检查（比如传入某个方法的数据类型必须与参数类型匹配、在某个对象上调用某个方法必须确保这个对象有这个方法）都是在编译时完成的，不占用运行时间，但是，对于反射，因为其是在运行时才确定创建什么对象、执行什么方法的，所以，安全性检查无法在编译时执行，只能在运行时真正创建对象、执行方法时再执行，那么，这就会增加额外的运行时间。
* 查找类或方法：当我们使用反射创建对象或执行方法时，我们需要通过类名、方法名去查找对应的类或方法，而类名、方法名都是字符串，字符串匹配比较慢速。而正常情况下，在代码经过编译之后得到的字节码中，每个类和方法都会分配一个对应的编号，保存在常量池中，代码中所有出现类或方法的地方，都会被替换为编号，示例代码及其字节码如下所示，其中，字节码中的常量池（Constant pool）中保存了各个类、方法的编号。对象创建通过“new #编码”来实现，方法执行通过“invokespecial #编号”来实现。相比于通过类名、方法名这些字符串来查找类和方法，通过编号来查找对应的类或方法，显然要快得多。

## 注解
定义一个Java注解，仍然需要用到注解。@Target、@Retention等这些用于定义注解的注解，叫做元注解。接下来，我们依次来看下，这些元注解都是干什么用的。
* @Target
  * 用来描述注解的使用范围，一般来讲，注解常用于类、方法、成员变量。但对于构造函数、局部变量、参数、包也是可以的
  * 一个注解可以多个使用范围，如果不使用 @Target 标记使用范围，那么注解可以用于任何范围
* @Retention
  * 用来描述注解的可见范围（或叫生命周期），SOURCE 表示仅在源码中可见、编译成字节码后，注解信息将被丢弃，比如 @Override 就是 SOURCE，CLASS 表示注解在源码、字节码中均可见，但在运行时不可见，无法在程序运行时，利用反射获取到代码中的这类注解信息，RUNTIME 则表示注解在源码、字节码、运行时均可见，其生命周期最长，可以在程序运行时，利用反射来获取代码中的这类注解信息。
* @Documented：用来表示注解信息会输出到 Javadoc 文档中
* @interface：用来定义注解，在注解中，我们还可以定义一些变量。变量的定义比较特殊，跟普通类中的变量定义方式不同。注解使用方法来定义变量。对于只有一个变量的注解，可以将其定义为value，这样在使用时，我们可以不指定变量的名称。

读取注解：大部分情况下，只定义和标记注解还不够，还需要有读取注解并做相应处理的代码逻辑，才能发挥注解的真正作用。**注解的定义、标记、读取三者缺一不可**。这就相当于，在推荐算法中，我们只定义标签和给数据打标签是没用的，我们还需要设计根据标签分类数据的算法，这样才能发挥标签的作用。对于Java内建注解，编译器和JVM都可以对其进行读取和处理，比如@Override注解，编译器在编译代码时，会读取所有标记了@Override的方法，并且检查父类中是否有同名方法，如果没有则编译报错。对于自定义注解，我们需要自己开发相应的读取和处理逻辑。如何来读取代码中的注解信息呢？这就要用到上一节课讲到的反射语法。因为反射作用于代码运行时，所以，**从侧面上，我们也可以得出，自定义注解的@Retention可见范围一般应该设置为RUNTIME**。

注解应用
* 替代注释：如 @VisibleForTesting 仅祈祷注释的作用，相对于注释，更加规范、统一，可读性更好
* 替代 Marker Interface：Java中有一种特殊的接口，叫做标记接口（Marker Interface）。标记接口中不包含任何方法，跟注解类似，起到标记作用。比如，常见的标记接口有RandomAccess、Cloneable、Serializable等。如ArrayList容器实现了这三个标记接口，用于表示ArrayList容器支持随机访问、克隆、序列化。在某些代码逻辑中，我们可以根据标记接口，判断对象是否可以执行某些操作。如下所示，java.util.Collections类中的binarySearch()函数，会根据不同类型的List容器执行不同的二分查找逻辑。对于支持随机访问的List容器，也就是实现了RandomAccess标记接口的List容器，binarySearch()函数调用indexedBinarySearch()函数来实现二分查找。标记接口只是起到标记作用，注解也可以起到标记的作用，因此，我们可以使用注解来替代标记接口，比如，我们可以将RandomAccess标记接口替换为如下注解。在ArrayList类中，我们使用@RandomAccess注解来表示ArrayList容器支持随机访问。
* 替代 XML 配置文件

## 动态代理
在平时的开发中，我们也经常利用动态代理，为代码添加额外的功能，比如日志、事务、鉴权限流、性能监控等。在面试中，我们也经常被问及动态代理相关的问题，比如为什么基于JDK实现的动态代理必须要求原始类有接口？

静态和动态这两个概念在本书中被反复提到，一般来讲，静态指的是某个事件发生在编译时，动态指的是某个事件发生在运行。将这个规则应用到代理模式，那么，静态代理指的就是在编译时生成代理类的字节码（即Class文件），动态代理指的就是在运行时生成代理类的字节码。动态代理生成的代理类的字节码仅仅存在于内存中，并不会生成对应的class文件，从而避免了静态代理需要编写大量代理类的问题。之所以可以实现动态代理，是因为JVM设计得非常灵活。不管字节码是预先编译好的（class文件），还是在内存中临时生成的（典型应用：动态代理），又或者从网络加载而来的（典型应用：Applet），只要它符合类的字节码格式，就可以在运行时被JVM解析并加载。

动态代理实现的方式有两类，一类是使用JDK提供的类来实现，一类是使用第三方的字节码类库来实现，比如CGLIB、BECL、ASM、Javassit等。字节码类库功能非常强大，可以动态生成和修改字节码，生成代理类的字节码当然不在话下。不过，字节码比较底层，直接编辑字节码，对于程序员来说是一个很大挑战。因此，我们一般采用JDK提供的类或者CGLIB这种使用起来比较友好的字节码类库来实现动态代理。接下来，我们就介绍一下这两种常用的动态代理实现方式。

基于 JDK 实现动态代理：动态代理具有哪些方法只跟接口有关，跟原始类没有任何关系，这也是基于 JDK 实现的动态代理，要求原始类必须有接口定义才行的原因，除此之外，加载到 JVM 中的类都要有类名，这样才能在实例化对象时按照类名查找到对应的类的定义。

基于JDK实现动态代理，耗时的地方主要有两处：一是运行时动态生成代理类的字节码，二是利用反射执行方法。对于使用反射执行方法的性能，反射会额外增加一部分耗时，但如果方法中的逻辑比较复杂，执行时间比较长，那么，相比而言，反射额外增加的耗时可以忽略。相反，如果方法中的逻辑比较简单，执行时间很短，那么，反射额外增加的耗时就不能忽略了。对于动态生成代理类的字节码，这部分耗时确实会比较多。而静态代理类的字节码是在编译时生成的，不占用运行时间。

在讲解静态代理时，我们提到，静态代理也有两种实现方式，一种是基于接口，一种基于继承。同理，动态代理也有基于接口和基于继承这两种实现方式。基于JDK的动态代理实现方式，使用接口来生成动态代理类，要求原始类必须定义接口，因此，是一种基于接口的动态代理实现方式。基于CGLIB的动态代理实现方式，并不依赖接口，通过继承原始类来生成动态代理类，因此，是一种基于继承的动态代理实现方式。

## 函数式编程
每个编程范式都有自己独特的地方，这就是它们会被抽象出来作为一种范式的原因。**面向对象编程最大的特点是：以类、对象作为组织代码的单元以及它的四大特性。面向过程编程最大的特点是：以函数作为组织代码的单元，数据与方法相分离。**那么函数式编程最独特的地方又在哪里呢？

实际上，函数式编程最独特的地方在于它的编程思想。函数式编程认为，程序可以用一系列数学函数或表达式的组合来表示。**函数式编程是程序面向数学的更底层的抽象，它将计算过程描述为表达式**。不过，这样说你肯定会有疑问，**真的可以把任何程序都表示成一组数学表达式吗？**

理论上讲是可以的。但是，并不是所有的程序都适合这么做。**函数式编程有它自己适合的应用场景，比如科学计算、数据处理、统计分析等**。在这些领域，程序往往比较容易用数学表达式来表示，比起非函数式编程，实现同样的功能，函数式编程需要更少的代码。但是，对于强业务相关的大型业务系统开发来说，费劲吧啦地将它抽象成数学表达式，硬要用函数式编程来实现，显然是自讨苦吃。相反，在这种应用场景下，面向对象编程更加合适，写出来的代码的可读性和可维护性更好。因此，我个人觉得，**函数式编程并不能完全替代更加通用的面向对象编程范式**。但是，作为一种补充，它也有很大存在、发展和学习的意义。

Java为函数式编程引入了4个新的语法：函数接口（Functional Interface）、Lambda表达式、方法引用、Stream流。
* 函数接口的作用是让我们可以把函数包裹成接口（interface），来实现把函数当做参数一样来使用；
* 引入Lambda表达式的作用是简化函数接口的匿名实现类的代码编写；
* 方法引用的作用是进一步简化Lambda表达式；
* Stream流用来支持通过“.”级联多个函数操作的代码编写方式。