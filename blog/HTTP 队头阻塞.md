队头阻塞

## 什么是队头阻塞
现实生活中一个很好的比喻就是只有一个收银台的杂货店。一个顾客买了很多东西，最后会耽误排在他后面的人，因为顾客是以先进先出（First In, First Out）的方式服务的。

## HTTP/1.1 的队头阻塞
简单而言请求文件的过程为
1. 请求某个文件，如 script.js
2. HTTP 会在文件内容上添加一些 headers，然后 payload+headers 传输给 TCP
3. 如果不能将整个文件放入一个 TCP 包中，则还会分成多个 TCP 包

你需要了解 HTTP/1.1 的两个重要特征
* 纯文本协议
* 接收者使用 Content-Length header 来知道每个响应的结束位置和另一个响应的开始位置

正是这两个特征，假设我们要下载 script.js 和 style.css 两个文件，这种情况下，在下载整个JS文件之前，CSS 必须等待，尽管它要小得多，其实可以更早地解析/使用。

这是 HTTP/1.1 协议设计方式的一个基础限制。如果您只有一个 HTTP/1.1 连接，那么在您切换到发送新资源之前，必须完整地传输资源响应。如果前面的资源创建缓慢（例如，从数据库查询动态生成的 index.html）或者，如上所述，如果前面的资源很大。这些问题可能会引起队头阻塞问题。

这就是为什么浏览器开始为 HTTP/1.1 上的每个页面加载打开多个并行 TCP 连接（通常为6个）。这样，请求可以分布在这些单独的连接上，并且不再有队头阻塞。这就是为什么浏览器开始为 HTTP/1.1 上的每个页面加载打开多个并行 TCP 连接（通常为6个）。这样，请求可以分布在这些单独的连接上，并且不再有队头阻塞。

## HTTP/2（基于 TCP）的队头阻塞
HTTP/1.1 有一个队头阻塞问题，一个大的或慢的响应会延迟后面的其他响应。这主要是因为协议本质上是纯文本的，在资源块（resource chunks）之间不使用分隔符。作为一种解决办法，浏览器打开许多并行 TCP 连接，这既不高效，也不可扩展。

HTTP/2 的目标非常明确：我们能够回到单个 TCP 连接，解决队头阻塞问题。换一种说法：我们希望能够正确地复用资源块（resource chunks）。这在 HTTP/1.1 中是不可能的，因为没有办法分辨一个块属于哪个资源，或者它在哪里结束，另一个块从哪里开始。

HTTP/2 非常优雅地解决了这一问题，它在资源块之前添加了帧（frames）。

HTTP/2 在每个块前面放置一个所谓的数据帧（DATA frame）。这些数据帧主要包含两个关键的元数据。首先：下面的块属于哪个资源。每个资源的“字节流（bytestream）”都被分配了一个唯一的数字，即流 id（stream id）。第二：块的大小是多少。

HTTP/2 在每个块前面放置一个所谓的数据帧（DATA frame）。这些数据帧主要包含两个关键的元数据。首先：下面的块属于哪个资源。每个资源的“字节流（bytestream）”都被分配了一个唯一的数字，即流id（stream id）。第二：块的大小是多少。

因此，通过“framing”单个消息，HTTP/2 比 HTTP/1.1 更加灵活。它允许在单个 TCP 连接上通过交错排列块来多路传输多个资源。它还解决了第一个资源缓慢时的队头阻塞问题：而不必等待查询数据库生成的 index.html，服务器可以在等待 index.html 时开始发送其他资源。

HTTP/2 的方式一个重要结果是，我们突然需要一种方法让浏览器与服务器通信，单个连接的带宽如何跨资源分布（distributed）。换一种说法：资源块应该如何“调度（scheduled）”或交错（interleaved）。如果我们再次用 1 和 2 来表示，我们会发现对于 HTTP/1.1，唯一的选项是 11112222（我们称之为顺序的（sequential））。然而， HTTP/2 有更多的自由：
* 公平多路复用（例如两个渐进的 JPEGs）：12121212
* 加权多路复用（2是1的两倍）：22122122121
* 反向顺序调度（例如2是密钥服务器推送的资源）：22221111
* 部分调度（流1被中止且未完整发送）：112222

已经解决了 HTTP/1.1 的队头阻塞，是的，但是 TCP 的队头阻塞呢？事实证明，HTTP/2 只解决了 HTTP 级别的队头阻塞，我们可以称之为“应用层”队头阻塞。然而，在典型的网络模型中，还需要考虑下面的其他层。

如果我们想将多个 HTTP/2 资源多路传输到一个 TCP 连接上，这确实会产生重要的后果。

虽然我们和浏览器都知道我们正在获取 JavaScript 和 CSS 文件，但 HTTP/2 不需要知道这一点。它只知道它在使用来自不同资源流 id （stream id）的块。然而，TCP 甚至不知道它在传输 HTTP！TCP 所知道的就是它被赋予了一系列字节，它必须从一台计算机传输另一台计算机。为此，它使用特定最大大小（maximum size）的数据包，通常大约为1450字节。每个数据包只跟踪它携带的数据的那一部分（字节范围），这样原始数据就可以按照正确的顺序重建。

如果 TCP 数据包2在网络中丢失，但数据包1和数据包3已经到达，会发生什么情况？请记住，TCP并不知道它正在承载 HTTP/2，只知道它需要按顺序传递数据。因此，它知道数据包1的内容可以安全使用，并将这些内容传递给浏览器。然而，它发现数据包1中的字节和数据包3中的字节（放数据包2 的地方）之间存在间隙，因此还不能将数据包3传递给浏览器。TCP 将数据包3保存在其接收缓冲区（receive buffer）中，直到它接收到数据包2的重传副本（这至少需要往返服务器一次），之后它可以按照正确的顺序将这两个数据包都传递给浏览器。换个说法：丢失的数据包2 队头阻塞（HOL blocking）数据包3！

总之，TCP 不知道 HTTP/2 的独立流（streams）这一事实意味着 TCP 层队头阻塞（由于丢失或延迟的数据包）也最终导致 HTTP 队头阻塞！

TCP 队头阻塞是真实存在的，但是它对 Web 性能的影响要比HTTP/1.1 队头阻塞小得多，HTTP/1.1 队头阻塞几乎可以保证每次都会遇到它，而且它也会受到 TCP 队头阻塞的影响！

TCP 队头阻塞是真实存在的，但是它对 Web 性能的影响要比HTTP/1.1 队头阻塞小得多，HTTP/1.1 队头阻塞几乎可以保证每次都会遇到它，而且它也会受到 TCP 队头阻塞的影响！

事实上，我们看到（也许出乎意料），HTTP/2 目前部署在浏览器和服务器中，在大多数情况下通常与 HTTP/1.1 一样快或略快。在我看来，这部分是因为网站在优化 HTTP/2 方面做得更好，部分原因是浏览器仍然经常打开多个并行 HTTP/2 连接（要么是因为站点仍然在不同的服务器上共享资源，要么是因为与安全相关的副作用），从而使两者兼得。

然而，也有一些情况（特别是在数据包丢失率较高的低速网络上），6个连接的 HTTP/1.1 仍然比一个连接的 HTTP/2 更为出色，这通常是由于 TCP 级别的队头阻塞问题造成的。正是这个事实极大地推动了新的 QUIC 传输协议的开发，以取代 TCP。

## HTTP/3（基于 QUIC）的队头阻塞
总结一下我们目前所学到的：
* HTTP/1.1 有队头阻塞，因为它需要完整地发送响应，并且不能多路复用它们
* HTTP/2 通过引入“帧”（frames）标识每个资源块属于哪个“流”（stream）来解决这个问题
* 然而，TCP 不知道这些单独的“流”（streams），只是把所有的东西看作一个大流（1 big stream）
* 如果一个 TCP 包丢失，所有后续的包都需要等待它的重传，即使它们包含来自不同流的无关联数据。TCP 具有传输层队头阻塞。

如何解决 TCP 的问题：解决方案很简单：我们“只是”需要让传输层知道不同的、独立的流！这样，如果一个流的数据丢失，传输层本身就知道它不需要阻塞其他流。

尽管这个解决方案概念简单，但在现实中却很难实现。由于各种原因，不可能改变 TCP 本身使其具有流意识（stream-aware）。选择的替代方法是以 QUIC 的形式实现一个全新的传输层协议。为了使 QUIC 现实中可以部署在因特网上，它运行在不可靠的 UDP 协议之上。然而，非常重要的是，这并不意味着 QUIC 本身也是不可靠的！在许多方面，QUIC 应该被看作是一个 TCP 2.0。它包括 TCP 的所有特性（可靠性、拥塞控制、流量控制、排序等）的最佳版本，以及更多其他特性。QUIC还完全集成了TLS（参见图6），并且不允许未加密的连接。因为 QUIC 与 TCP 如此不同，这也意味着我们不能仅仅在其上运行 HTTP/2，这就是为什么创建了 HTTP/3。

让 QUIC 知道不同的流（streams）是非常简单的。QUIC 受到 HTTP/2 帧方式（framing-approach）的启发，还添加了自己的帧（frames）；在本例中是流帧（STREAM frame）。流id（stream id）以前在 HTTP/2 的数据帧（DATA frame）中，现在被下移到传输层的 QUIC 流帧（STREAM frame）中。这也说明了如果我们想使用 QUIC，我们需要一个新版本的 HTTP 的原因之一：如果我们只在 QUIC 之上运行 HTTP/2，那么我们将有两个（可能冲突的）“流层”（stream layers）。相反，HTTP/3 从 HTTP 层删除了流的概念（它的数据帧（DATA frames）没有流id），而是重新使用底层的 QUIC 流。

注意：这并不意味着 QUIC 突然知道 JS 或 CSS 文件，甚至知道它正在传输 HTTP；和 TCP 一样，QUIC 应该是一个通用的、可重用的协议。它只知道有独立的流（streams），它可以单独处理，而不必知道它们到底是什么。

具体分析看来源文章的图例，说的很详细。

这种方式有几个重要的后果。最有影响的是 QUIC 数据可能不再以与发送时完全相同的顺序发送到浏览器。对于 TCP，如果您发送数据包1、2和3，它们的内容将以完全相同的顺序发送到浏览器（这就是导致队头阻塞的第一个原因）。然而，对于 QUIC，在上面的第二个示例中，在数据包1丢失的情况下，浏览器首先看到数据包2的内容，然后是数据包3的最后一部分，然后是数据包1的（重传），然后是数据包3的第一部分。换言之：QUIC 在单个资源流中保留了顺序，但不再跨单个流（individual streams）进行排序。

这是需要 HTTP/3 的第二个也是最重要的原因，因为事实证明，HTTP/2 中的几个系统非常严重地依赖于 TCP 跨流（across streams）的完全确定性排序。

QUIC 和 HTTP/3 真的完全消除了队头阻塞？
* 即使在QUIC中，我们仍然有一种队头阻塞的形式：如果在单个流中有一个字节间隙，那么流的后面部分仍然会被阻塞，直到这个间隙被填满。
* QUIC 的队头阻塞移除只有在多个资源流同时活动时才有效。这样，如果其中一个流上有包丢失，其他流仍然可以继续。这就是我们在上面图9的例子中看到的。然而，如果在某一时刻只有一个流在活动，任何丢包都会影响到这条孤独的流，我们仍然会被阻塞，即使在 QUIC

现在，这是什么意思？对于最佳性能，我们有两个相互冲突的性能优化建议：
* 从 QUIC 的队头阻塞移除中获利：多路复用发送资源（12121212）
* 为了确保浏览器能够尽快处理核心资源：按顺序发送资源（11112222）

## 来源
* [译文：QUIC 和 HTTP/3 队头阻塞的细节](https://mp.weixin.qq.com/s/-Z5ei-zXMfjPHUrQZs87ag)
* [原文：Head-of-Line Blocking in QUIC and HTTP/3: The Details](https://github.com/rmarx/holblocking-blogpost)