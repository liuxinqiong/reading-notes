## 认识分布式系统
随着技术的发展，应用的功能越来越多，性能要求也越来越高，单台机器的 CPU 、内存、磁盘 I/O 等慢慢就接近性能上限。

于是我们将系统拆分成更小的、独立的组件，部署在多台计算机上，通过网络连接协同工作，共同完成一个更大的目标，这样由一台机器组成的集群，对外来看像一台超级机器一样运转。

只要有2个或2个以上的子系统（或叫应用、程序）协同工作对外提供一致的服务，就叫做分布式系统。我们用Nginx做负载均衡，后面挂着多个Tomcat部署应用程序，应用程序再链接数据库，这就是一个简单的分布式系统。

除了分布式系统，后面我们还会讲到分布式锁、分布式事务、分布式一致性、分布式缓存。那么，这些概念里的“分布式”跟分布式系统里的“分布式”所具有的意思是一样的。即，多个子系统协同工作。
* 分布式缓存是指将比如Redis这样的缓存系统部署在多台机器上，对外提供更强大（更快、容量更大）的缓存服务。
* 分布式锁指的就是多个应用对同一数据进行互斥访问。
* 分布式事务就是多个子系统的逻辑一起执行，表现出事务特性（ACID），简单理解为，要么都执行，要么都不执行。
* 分布式一致性问题就是指多个节点上的数据是否一致的问题。

分布式系统相较于单体应用有哪些优势呢？
* 突破了单机的性能瓶颈：机器硬件的升级我们叫做垂直扩展，增加机器我们叫做水平扩展。垂直扩展一定是有上限的，但从理论上讲，只要系统设计得当，水平扩展是没有上限的。
* 提升了系统的可用性：对于单体应用来说，单点故障是致命弱点。服务器硬件故障、操作系统崩溃、代码异常出错，甚至一次部署，都可能导致整个应用瘫痪或中断、以及所有或部分数据的丢失。而分布式系统可以消除单点故障。只要设计得当，即使部分节点宕机，整体系统仍能继续提供服务。比如，Redis的主从模式，从节点复制主节点中的数据，数据通过备份，避免了丢失，而且，即便主节点挂掉，Redis也会自动在从节点中选举新的主节点，对外继续提供服务，避免了服务的瘫痪。再比如，Nginx作为负载均衡和反向代理，将流量路由到多台部署了应用的Tomcat机器上，单台Tomcat机器挂掉，Nginx会将流量分配给其他机器，这样，整个服务还可以照常运行，提高了可用性。
* 提高开发效率：分布式系统将大的系统拆分成多个小系统，每个小系统可以独立开发、测试、部署，显然开发效率要更高。而且，每次代码改动，影响的范围更小，系统整体的稳定性更高。

分布式带来了巨大的优势，但也引入了单体架构中不存在的、极其复杂的挑战。这些挑战源于一个根本事实：**网络是不可靠的，节点是可能失效的，信息传递是有延迟的**。我们具体来看下有哪些挑战。
* 节点故障：分布式系统中的任何节点都可能在任何时刻因硬件故障和软件Bug等原因而停止工作。依赖该节点的请求会失败，其持有的数据可能暂时或永久丢失（如果没有冗余）。当然，冗余设计可以解决数据丢失问题，也就是，将数据复制到多个节点，即使一个节点失效，其他节点也能接管，但这也直接引出了下一个问题。
* 数据一致性：当一个节点上的数据更新时，如何保证所有副本（其他节点上的数据）在同一时刻看到相同的值？在网络延迟和节点故障存在的情况下，这极其困难。
* 网络问题：节点间通信需要时间，且时间长短不确定，而且，消息有可能丢失、重复、乱序。这使得精确协调多个节点的行为（如达成共识）变得复杂。
* 时钟不同步：不同节点的物理时钟不可能完全精确同步。依赖本地时间戳进行事件排序或超时判断可能导致错误。分布式系统中通常需要逻辑时钟（如 Lamport Timestamps, Vector Clocks）或更复杂的方案（TrueTime, HLC）。
* 共识问题：如何在存在故障节点的分布式系统中，让所有正常节点就某个值或某个决策达成一致？Paxos、Raft、ZAB 等算法是解决共识问题的经典方案（如 etcd 用 Raft）。
* 事务管理：跨多个服务/节点的事务（分布式事务）比单机数据库事务复杂得多。保证 ACID 属性需要引入如 2PC、3PC、TCC、Saga 等补偿机制，这些机制本身也有复杂性和性能开销。

分布式系统设计：CAP 定理

CAP 定理由 Eric Brewer 提出，他指出，在一个异步网络（存在延迟、丢包、分区风险）且可能出现节点故障的分布式系统中，以下三个特性无法同时完全满足：
* C (Consistency - 一致性)：所有节点在同一时刻看到的数据是完全相同的（强一致性）。任何一次读操作都能读到最近一次写操作的结果。
* A (Availability - 可用性)：系统始终可用，每一个向系统发出的请求，都必须在有限的时间内得到一个非错的响应（不能是超时或失败）。
* P (Partition Tolerance - 分区容错性)：当系统中节点之间因网络故障导致通信中断（形成网络分区 Network Partition）时，系统整体仍然能够继续提供服务。

CAP理论迫使我们在开发分布式系统时，根据业务特点和要求，对CAP做抉择，选择牺牲哪一个。

CP 系统 (牺牲 A)：当发生网络分区时，为了保证所有节点数据强一致（C），系统可能拒绝部分或所有写操作，甚至拒绝某些读操作（返回错误或超时），从而牺牲可用性（A）。例如：ZooKeeper, etcd, HBase（强一致模式下）。它们优先确保数据在所有分区内都是一致的，即使代价是暂时不可用。再例如：银行核心转账系统通常选择CP。想象跨行转账时网络分区，系统宁愿告诉你“服务暂时不可用，请稍后再试”，也绝不会冒险让两边账户不一致（比如钱扣了但对方没收到）。

AP 系统 (牺牲 C)：当发生网络分区时，为了保证系统整体仍然可用（A），允许不同分区内的节点数据暂时不一致（牺牲 C）。系统会继续处理读写请求，并在分区恢复后解决冲突。例如：Cassandra, DynamoDB，它们优先确保用户可以读写，接受短暂的数据不一致。再例如：社交媒体的点赞、状态发布通常选择AP。假设微信朋友圈发布时网络分区，系统允许你在本地分区发布成功并立即看到（可用性），但可能稍后网络恢复时，其他分区才看到你的更新（暂时不一致）

CA 系统 (牺牲 P)：这通常只存在于理论上或极小的局域网内。它假设网络永远不会分区（P）。这在广域网环境下是不现实的。也就是说，在绝大多数分布式系统中，网络分区是基本上无法避免的，因此，CA分布式系统可以看做是不存在的。

当然，对于大多数分布式系统来说，我们并不能非黑即白地粗略地选择CP还是AP。一方面，可用性对于互联网产品非常重要，我们不能抛弃，另一方面，数据的一致性也非常重要，还有，强一致性也很难实现，因此，结合实际，工程师又提出了另一个分布式系统设计的理论：**BASE理论**。

Basically Available (基本可用)：系统在出现不可预知故障时，允许损失部分功能或响应时间稍有延长，但核心功能仍然可用。比如电商大促时，可能关闭非核心的商品评论功能，或查询响应稍慢，但下单支付主流程必须保证可用。

Soft state (软状态)：系统中的数据状态不要求时刻保持强一致，允许存在中间状态（不同副本间数据存在短暂差异），且该状态的存在不会影响系统整体可用性。比如用户积分，可能在主库更新后，异步复制到其他库，中间存在延迟。

Eventually consistent (最终一致)：系统保证在没有新的更新操作情况下，经过一段时间的同步（如秒级、分钟级），所有数据副本最终将达到一致的状态。 这是弱一致性中被广泛接受的一种形式。比如你修改了头像，可能几秒钟后，所有好友才都能看到新头像。

## 系统架构评价指标
评价一个系统，除了关注它的功能是否实现完备，还要看它的**非功能特性**，这就好比你买车不能只看外观，还得看油耗、安全性、舒适性、操控性一样，评价一个系统设计是否优秀，也得从多个维度去分析。

主要评价指标：高并发、高可用、高性能、可靠性和伸缩性等。

高性能主要看响应时间和吞吐量
* 响应时间：一般来说，我们会关注响应时间的百分位值而非平均值，因为平均值很容易掩盖问题。想象一下：10个请求，9个耗时 100ms，1个耗时 10s，平均是 1090ms，看起来还行？但那个等了10秒的用户可能已经骂娘了！因此，我们一般关注P90（90分位值，也就是90% 的请求响应时间小于等于这个值）、P95、P99、P99.9等。
* 吞吐量：在单位时间内能处理多少工作量，常见的指标有 QPS 、TPS，QPS 表示每秒能成功处理的请求数量，TPS 特指每秒能成功完成的业务事务数量，一个事务通常代表一个完成的业务操作（比如用户下单这个事务，可能包含扣库存、创建订单、支付等多个步骤）。 TPS 更能反应核心业务的处理能力。

高并发能力就是衡量系统在短时间内处理大量同时涌入请求的能力。它和高性能紧密相关，但侧重点不同：性能关心单个请求快不快，并发关心同时来一大波请求时系统整体能不能撑住、会不会崩溃。

高可用的终极目标就是让用户感觉不到系统出过问题，服务一直“在线”。衡量系统可用性的最直观的指标就是SLA（服务等级协议），比如承诺“全年99.9%可用”，那一年里允许的宕机时间大概就是8.76小时。对应SLA 99.99%的允许宕机时间为52.6分钟了，对应SLA 99.999%的允许宕机时间为5.26分钟。很多大厂的关键应用都要求4个9或者5个9的SLA。当然，SLA要求越高，付出的成本和系统的复杂度就会成指数级增长。

高可用是我们在做大型系统设计时必须要考虑的方面。互联网产品使用的机器大都是普通的服务器，可靠性等各方面都不会很高，而大型应用都对应成百上千甚至上万的服务器，服务器的数量比较多，即便很小的故障概率也会被放大，每天也必定会有几台服务器发生硬件故障。除此之外，网络通信中断、软件运行异常、代码bug等等，都有可能导致某个服务器临时运行异常。

高可用设计的目的就是不管是硬件还是软件故障发生的情况下，服务依然可以使用，数据依然不会丢失。要达到这样的目的，要做的工作有很多，根据对SLA不同的要求，可以选择性的使用其中某些方法。
* 冗余：消除单点，比如集群部署、主备复制、异地多活。应用部署在多台服务器上同时提供访问，数据存储在多台服务器上互相备份，任何一台服务器宕机都不会影响应用的整体可用，也不会导致数据丢失。
* 容错：降级、熔断、限流、异步，部分挂不影响全局。某个非核心服务挂了（比如积分服务暂时不可用），别影响用户下单这个核心流程，给它降级处理（比如先记下来，等恢复了再补算）。依赖的外部服务挂了（比如短信网关），要有备用方案或者优雅地提示用户。
* 可观测性：监控、日志、追踪都要有，能时刻了解系统的运行情况，即便出了问题，也可以快速发现和定位问题，是网络抽风了，还是某段代码把CPU吃满了，或者数据库连接池爆了？快速解决问题，也能提高可用性。
* 自动化：故障发现、切换、恢复这些操作，不能光靠人工干预，否则，效率太低还容易出错，得靠监控系统自动发现、自动处理。

可靠性更宽泛一些，它不仅关注系统“活着”（可用性），还关注系统“正确地活着”。它衡量的是系统在指定条件下、指定时间内，无故障地完成指定功能的能力。比如，你往数据库存一条数据，它是不是100%存进去了，并且是否存对了？处理一笔支付，是不是准确无误地完成了？这涉及到数据的正确性、一致性、服务的健壮性（容错性）。避免单点故障是高可靠的基础，同时还需要完善的错误处理机制、数据校验、事务保证、幂等设计等等。一个系统可能可用性很高（很少宕机），但如果它时不时给你算错账、丢数据，那可靠性也是不及格的。

伸缩性也叫做扩展性，英文翻译Scalability。伸缩性衡量的是系统通过增加（或减少）资源来应对负载变化的能力。比如，用户访问量翻10倍，目前的架构是否可以通过增加资源（机器）轻松应对。

伸缩分为两类：垂直伸缩和水平伸缩。垂直伸缩指的是给单台服务器加CPU、加内存、换更好的硬盘。简单粗暴，但有上限，成本也高。水平伸缩指的是通过增加服务器数量来分担压力。这才是互联网分布式系统的终极方案。水平伸缩性好不好，关键看架构设计是否无状态（Stateless）、能否方便地添加/移除节点、负载均衡是否高效、数据存储是否支持水平拆分（分库分表、NoSQL）。一个伸缩性好的系统，面对业务增长，能相对平滑地通过加机器来应对，而不是动不动就要推倒重来。

当然，伸缩并不只是增加资源，还有可能减少资源，实现根据流量动态调整资源的投入。几乎所有的业务都访问高峰和低谷，比如白天比晚上高，周末比工作日高，促销时期或者突发事件也可能导致流量的突增，为实现高可用与成本效益的平衡，系统需要在高峰时弹性扩容保障性能，低谷时智能缩容释放冗余资源，完成动态的按需伸缩。

## 系统架构设计原则
架构上的 KISS 原则：能用简单方案解决的，绝对不用复杂方案！

一个预期用户量就几千的内部审批系统，数据量很小，业务逻辑也不复杂。结果架构师为了体现项目的难度和技术含量，要求必须上微服务！要用 Kubernetes 编排！消息队列用 Kafka 保证高吞吐！数据库分库分表预留未来十年容量！结果呢？团队大部分精力都耗在搭建、运维、调试这套“豪华”基础设施上了，核心审批流程反而不关注，导致功能实现不完善、代码bug多多。项目部署一次像打仗，出个问题排查链路长得能绕地球一圈。这就是典型的“杀鸡用牛刀”，为了技术而技术，完全违背 KISS。

在架构设计中，每引入一个组件，对架构的复杂度、维护成本、可用性等都随之带来很大的影响。我们拿可用性举例，你想，如果系统引入5个组件，一个组件的可用性是99%，那么，组合在一起，这个系统的可用性就是99%的5次方，也就是0.95，所以，能用已有的组件简单解决的问题，就不要大动干戈的引入新的复杂组件。

架构设计也是如此。复杂的架构都是演进而来的。你看现在的淘宝、微信等大型应用，架构都非常的复杂，但这也都是几年、十几年一点一点演进的结果，你去网上搜一下，就会发现，它们的最初架构也都是非常简单的。是用户量、性能压力、高可用的要求、业务需求等等实实在在的需求推动了架构的演进，才导致架构越来越复杂，而非架构师脱离需求单纯靠拍脑袋设计出来的。

其实，架构过度设计的情况非常常见，甚至我也经常如此。设计一个新电商平台，才刚起步，用户、商品、订单量都很少，架构师就开始担心：“万一我们明天就成淘宝了呢？”于是，在设计订单系统时，预设了支撑每秒 10 万订单的架构；设计了极其复杂的分布式事务补偿机制（TCC/Saga）；把库存、优惠券、积分等服务拆得极细，预设了它们未来各自独立演进、需要强事务协调的场景。结果：开发周期巨长，初期版本臃肿不堪，性能可能还不如一个设计良好的单体（因为分布式调用开销）。更惨的是，业务发展可能根本没达到预期，或者方向变了，这些“提前量”全成了无用功，成了维护的负担。

而正确的做法应该是，清晰定义当前的核心需求和已知的、近期的（比如未来3-6个月）可预见需求。比如，初期订单系统，保证核心下单流程正确、库存不超卖是重点。一个在单体应用内，利用数据库事务（或乐观锁）+ 清晰的业务逻辑就能搞定，简单可靠。当真实需求出现、真实瓶颈暴露时（比如订单量真的大幅增长了，数据库成为瓶颈；或者业务要求积分和订单解耦独立发展），再根据当时的实际情况，运用合适的架构模式（比如引入消息队列异步化、拆分服务、优化数据库）去解决。让架构随着业务一起成长。

其实，对于大部分产品，初期，一个单体应用配合清晰的模块划分（比如使用gradle module组织模块）和一个够用的数据库，可能比强行拆分成一堆微服务要简单高效得多，开发快、部署快、问题也好排查。简单不是简陋，而是用最匹配当前需求和团队能力的方案。 别为了显得“高级”而堆砌技术栈，那叫技术虚荣心作祟。

设计架构是工程实践，不是理论研究，理论研究有最优解，但架构设计并没有放之四海而皆准的最优解，抛开业务谈论架构设计都是耍流氓。我们设计架构，目标从来不是捣鼓出一个理论上无懈可击的“完美艺术品”，而是工程上合适、可用、稳定的务实解决方案。千万别掉进追求“牛逼”、追求“完善”的陷阱里。

## 系统设计硬件常识
理解硬件特性是进行合理技术选型和资源匹配的关键。不同的系统对硬件资源的需求差别巨大：**比如 Nginx 这类反向代理服务是典型的 CPU 消耗大户，处理海量并发连接需要强劲的计算能力；而像 RocketMQ、Kafka 这样的消息队列以及 Redis 这类内存数据库，则对内存容量和带宽有着极高的需求；数据库应用则常常受限于磁盘 I/O 性能（尤其是随机读写 IOPS）**。如果给 Nginx 配了大内存但 CPU 较弱，或者给 RocketMQ 配了顶级 CPU 却内存不足，就如同给跑车加柴油——再好的架构设计也难以发挥其应有的优势。

### CPU

CPU 主频（GHz）：主频，单位是 GHz（吉赫兹），比如 3.5 GHz。简单说，它代表了 CPU 内部时钟一秒钟能“滴答”多少次。每一次“滴答”，CPU 就有可能完成一个最基本的操作步骤（指令周期的一部分）。主频可以理解为引擎的“转速”。主频越高（比如 3.5GHz > 2.0GHz），单个核心执行简单指令的速度通常越快。但要注意，高主频往往伴随高功耗和高发热，而且现代软件的性能瓶颈往往不在单核速度上。单纯追求超高主频就像只追求跑车最高时速，但日常驾驶是否跑得快，还要看“道路”情况等。

多级缓存（L1/L2/L3 Cache）：这是 CPU 解决“内存速度跟不上”问题的秘密武器。它是集成在 CPU 芯片内部（或者紧挨着 CPU）的一块非常非常快、但容量相对较小的存储区域。速度比内存快几十甚至上百倍！

多核CPU：既然提高单个核心的主频越来越难，那就干脆多造几个核心塞进一个 CPU 里！这就是多核 CPU。 每一个核心为一个独立的、完整的 CPU 执行单元，拥有自己的 L1/L2 缓存（通常），能独立执行指令流。这是提升并行处理能力的根本。 操作系统可以把不同的程序（进程），或者同一个程序的不同部分（线程），分配到不同的核心上去同时执行。编译大项目、渲染视频、运行大型服务器处理并发请求，多核的优势非常明显。**多线程编程是发挥多核威力的关键！ 如果你的程序是单线程的，那它一次只能在一个核心上跑，其他 7 个核心可能就在“围观”，性能潜力完全没发挥出来**。

超线程（HT/SMT）：超线程（Hyper-Threading, HT，或更通用的 SMT - Simultaneous Multi-Threading）让一个物理核心在操作系统和程序看来，像是两个逻辑核心。一个物理核心内部有很多执行单元（比如算数的、处理内存的）。在执行一个线程时，这些单元可能不会都被充分利用（比如这个线程在等内存数据，计算单元就闲着了）。超线程技术让这个核心能同时交错执行两个线程的指令。当一个线程在等待（比如等数据），核心可以立刻切换到执行另一个线程的指令，尽量让核心内部的各种资源都忙起来，减少空闲时间。

在资源没有被完全挤占的情况下（比如两个线程一个主要在计算，一个主要在读写内存），它能提高 CPU 核心的利用率，带来一定的性能提升（可能提升 10%-30%），相当于“花一份钱，干更多的活”。

如果两个线程都非常“重”，都要全力争抢同一个核心的计算资源（比如都是密集计算型），那么开启超线程可能反而比关闭它性能更差！ 因为核心内部资源不够分，调度开销反而成了负担。

对于后台服务、Web 服务器这种经常有 I/O 等待（网络、磁盘）的任务，超线程通常效果不错。但对于极致性能要求、延迟极其敏感（如高频交易）或纯粹的计算密集型负载（如科学计算、视频编码），有时关闭超线程性能更稳定、可预测。

CPU 架构（x86 vs ARM）：就像汽车的发动机有汽油机和柴油机之分。x86 (Intel/AMD) 是服务器和PC的绝对主流，生态成熟，性能强劲，尤其在高性能计算领域。ARM 架构（如苹果M系列）以高能效比著称，功耗低、发热小，在云计算（追求密度和能效）、移动端、边缘计算领域大放异彩。架构师选型时需要权衡性能、功耗、成本和软件生态兼容性。

### 内存
容量：容量即内存大小，是机器资源的主要指标！ 当物理内存不够用时，操作系统会把一部分暂时不用的数据“赶”到慢得多的硬盘（交换区/Swap）上，等需要时再读回来。这个过程叫 Swapping。我不说你也能看得出，Swapping严重影响程序的性能。因此，架构师必须根据应用类型预估内存需求，一定要给服务器配置充足的内存，“宁大勿小”是基本原则！一般来讲，操作系统本身需要几个GB的内存，这个要刨除在外，除此之外，还要预留30%左右的内存应对突发流量。综合起来就是服务器的内存大小需求。

速度和带宽：内存速度影响 CPU 获取数据的速度。主要由内存类型 (DDR4/DDR5) 和 通道数 决定。DDR4是目前主流、成熟稳定的内存类型，DDR5 比 DDR4 更快、更省电、容量更大，是趋势。尤其是追求高性能（数据库、内存计算、大数据节点）或大容量（>256GB），DDR5 是更优选择。除此之外，通道数也很重要。通道数直接决定了内存和CPU之间的带宽！ 通道越多，CPU 和内存之间能同时传输的数据量就越大。现在内存多为双通道或者四通道，八通道多应用于高性能服务器。

操作系统位数（32位vs64位）：32位系统只能用 32 根“地址线”来指定内存位置。这最多能管理 2^32 = 4GB 的内存地址空间。实际可用通常小于 4GB (部分地址被硬件保留，如BIOS)。相应地，64位系统理论上能管理 2^64 字节的内存，这是一个天文数字（16 EB），远远超出目前任何单台服务器的物理内存容量。要使用超过4GB的内存，必须使用64位操作系统。现在几乎所有服务器和主流桌面都是 64 位。

### 硬盘
硬盘的选择非常重要，因为它往往是系统性能的瓶颈。

硬盘一般分为HDD（机械硬盘）和SSD（固态硬盘）。机械硬盘靠磁头和旋转的盘片读写数据，磁头需要移动到正确的磁道（寻道时间），然后等待盘片旋转到正确的位置（旋转延迟），最后才能读写数据。这两个机械动作导致延迟高达毫秒(ms)级（通常是 5ms - 20ms），因此，随机读写延迟高，读写速度慢。固态硬盘用闪存芯片存储，没有机械部件，数据通过电子信号读写。 随机读写性能碾压 HDD（快几十上百倍），延迟超低（us级），读写速度快。

我们一般用 IOPS、吞吐量、延迟来评价硬盘的性能
* IOPS（Input/Output Operations Per Second）：IOPS指的是每秒读写操作次数，衡量硬盘处理小文件、随机读写能力。数据库、消息队列等非常关注这个指标。简单感受一下：高端 NVMe SSD 100万+ IOPS，SATA SSD 10 万 IOPS，HDD 100-200 IOPS
* 吞吐量：吞吐量指的是单位时间内传输的数据量（如 MB/s, GB/s），它衡量的是处理大文件、连续读写的能力。视频编辑、大型文件传输、备份恢复、科学计算（读写大矩阵）等场景关注这个指标。简单感受一下：高端 NVMe SSD 3000～7000MB/s，SATA SSD 500~600MB/s，HDD 80~200 MB/s
* 延迟：延迟指的是从发出 I/O 请求到收到响应的时间，它衡量磁盘对单个请求的响应速度。对延迟敏感型应用（如高频交易数据库、实时分析）极端重要！简单感受一下：高端 NVMe SSD 50～150us，SATA SSD 500~1000us，HDD 5~20 ms

### 网络
对于分布式系统来说，系统之间的通信都要经过网络，因此，网络的性能也是需要特别关注的。

网络我们一般关注带宽和延迟两个指标。
* 带宽（Bandwidth）：带宽指的是每秒传输的数据量，单位bit per second，缩写为bps，千兆网就是1000Mbps，也就是1Gbps。注意，带宽的单位是bit，而非byte，也就是说，带宽为100Mbps的网络的理论最高下载速度为100/8=12.5MB/s，不过，受设备性能、线路衰减、高峰期拥塞影响，实际仅达理论值60%-80%。
* 延迟（RTT）：延迟的单位是毫秒，指的是数据包从节点A 点跑到 B 点再回来的时间，它衡量的是响应速度。我们平常打游戏的时候，感觉到很卡，可能就是因为网络延迟高的原因。一般来讲，靠的越近，延迟越小，局域网内的机器之间通信，延迟要远小于公域网内的机器之间通信。