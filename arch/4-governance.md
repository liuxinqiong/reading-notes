## 微服务架构
当单体架构遇到下面这些问题的时候，我们就可以考虑做微服务拆分了
* 代码库变得极其臃肿庞大，维护成本高，编译一次要十几分钟甚至更长；
* 一个项目的研发团队过大，沟通成本高，代码提交冲突多，研发效率低；
* 一个小小的线上 Bug 可能就需要整个应用重新打包、测试、部署，风险高、耗时长；
* 即使只有某个模块（比如商品）流量暴增，我们也得对整个系统做扩展，浪费资源；
* 技术栈的更新举步维艰，没人敢大规模重构代码，生怕牵一发动全身。

微服务的拆分，核心依据是业务领域本身，目标是识别出那些功能高度内聚，彼此耦合低的业务边界，并遵循单一职责原则进行划分。除了业务领域这个核心依据，微服务拆分还需要综合考虑几个关键因素
* 服务粒度的把控：判断服务是否“足够单一”，不仅要看业务边界，还要结合其实际复杂度和代码规模。例如，当用户登录逻辑膨胀（支持微信、手机、邮箱等多种方式，并集成单点登录、统一认证、黑名单管理等丰富功能），将其独立出来成为专门的服务就显得必要且合理。
* 服务数量的平衡：虽然拆分能降低单个服务的复杂度，但过度拆分（导致微服务数量过多）会显著提升整个系统的复杂度。这就像代码设计，类太大难以维护，但拆分成无数个零碎的小类同样会让代码变得难以理解和维护。
* 团队结构的匹配：服务的拆分往往对应着团队的划分。在一个大型团队中，如果所有人挤在同一个巨型项目里协作，效率必然低下。更优的做法是根据团队规模，将其拆分为多个3-5人的小团队，每个小团队专注负责少数几个微服务的全生命周期（开发、测试、运维），这样能最大化协作效率。
* 渐进式拆分策略：实践中，我们很少会一次性将整个单体彻底拆散。更明智的做法是在遇到性能瓶颈或管理困难时，优先将那些核心度高、性能压力大或变化频繁的业务模块拆出来独立成微服务，然后，随着项目演进，再逐步拆分其他部分。这种渐进的方式能有效避免过度拆分带来的服务治理负担。

微服务之间是可以有依赖（调用）的，但不允许存在循环依赖。为了进一步理清调用关系，我们可以引入分层结构。例如，**基础服务**（如用户服务）位于底层，被众多上层服务（如订单服务、支付服务）所依赖；**业务核心服务**（如订单服务、支付服务）位于中层；而**聚合服务**（如交易服务）则位于顶层。微服务之间的调用必须遵循一定规则：**上层服务可以调用下层服务，但禁止反向调用；同层服务之间，优先通过消息中间件进行异步通信，尽量减少直接的同步调用**。这种基于业务拆分为基础、辅以合理分层和调用约束的设计，能有效避免服务间形成错综复杂的网状依赖，使整个系统的调用链路保持简洁清晰，极大地方便后期的维护和问题排查。

微服务架构的收益
* 技术异构性
* 弹性与容错
* 独立扩展性
* 快速部署交付
* 清晰的业务边界
* 隔离的数据存储

微服务带来的挑战
* 服务之间的调用走网络，也带来了很多分布式系统的共性问题，比如分布式事务。
* 服务的调用错综复杂，调用链路变长，定位线上问题跨系统跨团队，效率低。
* 拆分成很多小的微服务之后，服务运维成本变高，比如部署、监控、日志收集等。
* 系统架构复杂熵变高，调用关系复杂，若缺乏服务治理，一个服务的故障可能引起雪崩效应。

要驾驭好微服务架构，就离不开一系列关键基础设施和组件的支撑，来完成服务的治理工作，这也是我们后续文章将要逐一深入探讨的核心技术：
* RPC 框架：这是服务间高效通信的基石（如 gRPC, Dubbo, Thrift），它封装了网络通信、序列化等底层细节，让远程调用像本地调用一样简单、高效。
* 服务注册与发现：服务实例动态上线、下线，调用方如何知道去哪里找它们？服务注册中心就是微服务世界的“通信录”，动态实时更新和推送在线服务实例。
* 配置中心：每一个微服务实例都依赖一定的配置，如果将配置存储在实例本地，那么，修改配置将会是一个大工程。配置中心可以统一、动态地管理成百上千个服务的配置项，并可以避免修改配置时重启服务！
* API 网关：它是系统对外的统一入口（或叫“门面”），负责路由转发、API聚合、协议转换、认证鉴权、限流熔断、日志监控等。
* 服务鉴权：在分布式环境下，如何确保服务间调用的安全可信？如何控制用户对API的访问权限？服务的鉴权就是必不可少的功能！
* 限流、降级与熔断：面对流量洪峰、服务故障，如何保护系统不被打垮？限流控制流量入口；熔断快速失败，防止雪崩；降级提供有损但可用的服务。这是构建弹性系统的关键手段。
* 链路追踪：一个用户请求可能穿越十几个服务，如何快速定位性能瓶颈或故障点？分布式链路追踪系统（如Zipkin, SkyWalking）通过唯一TraceID串联整条调用链，让Debug不再是大海捞针。

## RPC 框架
RPC框架是微服务架构下，服务间进行通信的基础设施。简单来说，RPC框架的目标是：让开发者调用一个远程服务的方法，感觉就像调用本地方法一样简单直观。 我们写代码调用远程服务时，直接使用类似 orderService.createOrder(userId, items)这样编写即可，至于orderService的这个方法是在同一个进程里，还是在千里之外的另一台服务器上，RPC框架帮我们屏蔽掉了这些底层细节。

想象一下本地方法调用：调用方和被调用方在同一个进程、同一块内存空间里。参数传递直接压栈，方法执行结果瞬间返回，整个过程高效、直接、可靠。但当服务被拆分成微服务，部署到不同的主机甚至不同的数据中心时，情况就变得复杂了： 服务间通信必须通过网络，而网络具有不可靠性，延迟、抖动、丢包、中断都是常态。**一次调用可能成功、可能超时、可能根本到达不了对方**。不同进程（甚至不同机器）拥有独立的内存空间。调用方内存中的参数对象（比如 Order 对象），无法直接传递给被调用方。

远程调用背后都发生了什么
* 动态代理：作为开发者，你只需要定义一个接口（比如 UserService）。RPC 框架在客户端会使用动态代理技术，在运行时动态生成这个接口的一个实现类（即代理类）。当调用 userService.getUser(id) 时，实际上调用的是这个代理类的方法，它负责数据的序列化和反序列化，以及跟服务提供方之间的通信。
* 序列化协议：序列化协议包括数据格式定义，以及如何序列化和反序列化。对于RPC框架来说，序列化协议的选择至关重要，它直接影响编解码速度、数据大小、传输效率，以及跨语言能力（Java调用Go服务），间接影响到RPC框架的性能。常见的序列化协议有：语言原生，如 Java 的 Serializable，性能差、跨语言差，不推荐；文本类型，如 JSON，XML 等，可读性好，但性能较低，体积大，二进制类型，常用的有 Protobuf，Thrift，Hessian 等，在微服务场景下，Protobuf 和 Thrift 因其高效和跨语言特性被广泛采用
* 通信协议：序列化的字节流需要通过网络从客服端传输到服务提供方，选择哪种底层传输协议来传输数据，会直接影响 RPC 调用的性能和可靠性，常用的有 TCP 、UDP 和 HTTP 协议
* 网络模型：客户端与服务提供方通信时，会建立网络连接（Socket）。但如果每次远程调用都创建新连接，三次握手的开销会让高频调用变得难以承受。因此，RPC框架的客户端普遍采用连接池技术来管理和复用TCP连接。连接池预先建立并维护一定数量的活跃连接，当需要发起调用时，直接从池中获取可用连接；调用结束后，连接归还池中等待复用。这大大减少了连接建立/断开的开销，提升了吞吐量。
* 业务线程：在NIO模型下，I/O线程需要保持高速运转，不能被耗时的业务逻辑阻塞。因此，必须将解码后的业务请求任务（通常是一个 Runnable 或 Callable）提交到一个独立的、大小可控的业务线程池中排队执行。这保证了I/O线程的高响应性，同时业务逻辑也能得到并发执行。线程池的大小（核心线程数、最大线程数）、任务队列类型（有界/无界）和大小、拒绝策略（当队列满且线程达上限时如何处理新任务）都需要根据业务特性谨慎配置。
* 服务发现：服务提供方可能有多个实例，且实例会动态上下线。客户端如何找到可用的目标实例？早期采用硬编码的方式，在客户端直接配置服务方的IP和端口。这种方式缺乏灵活性，难以应对动态变化，基本已被淘汰。现在，服务注册和发现已经成为RPC框架的标配。服务提供方在部署时需要启动一个服务注册中心（如Nacos, Consul, Zookeeper, Eureka）。服务提供方启动时向注册中心注册自己的地址信息（服务名、IP、端口等信息）。客户端发起请求前，会向注册中心查询目标服务名对应的可用实例列表，并从中选择一个进行调用。这解决了服务实例动态变化的问题。
* 执行调用：服务提供方接收到网络请求后，反序列化字节流，还原出方法标识符和参数。通过反射调用本地真正的服务实现（比如 OrderServiceImpl.createOrder(...)），获取服务实现的执行结果（或异常），将结果（或异常信息）序列化成字节流，通过网络将响应字节流传回客户端。客户端接收到响应字节流后，反序列化还原出结果对象或异常，最终将结果返回给本地调用者代码，或者抛出异常。至此，一次完整的RPC调用完成。框架在背后默默处理了网络通信、数据转换、服务定位等复杂工作，开发者只需关注业务接口的定义和实现。

关于 TCP、UDP 和 HTTP 协议的特点
* **TCP协议是最主流的选择**，因为它提供了可靠的、面向连接的传输保障。它能确保数据包按顺序到达、不丢失（通过重传机制）、不重复。这对于要求数据准确无误的RPC调用至关重要。像Dubbo深度依赖TCP。不过，TCP的连接建立（三次握手）和断开（四次挥手）有一定开销，在高频调用场景下，RPC框架通常会使用连接池来复用TCP连接，避免频繁开关连接的性能损耗。另外，RPC框架需要在TCP协议之上定义自己的应用层协议来区分不同的请求/响应消息（比如Dubbo协议）。
* UDP协议则是无连接、不可靠的传输协议。它的优势在于极低的延迟和开销（没有连接建立过程，头部开销小），特别适合对实时性要求极高且能容忍少量丢包的应用（如音视频流、在线游戏）。但在RPC场景下，直接使用原生UDP风险很大，数据可能丢失、乱序、重复。因此，如果要在RPC中使用UDP，框架必须在应用层自行实现丢失重传、数据包排序、流量控制等复杂的可靠性机制，这大大增加了实现的难度和复杂度。纯粹的UDP在通用RPC框架中较少直接使用。
* HTTP协议是七层网络协议，它定义了自己的数据传输格式。**HTTP协议一般是基于文本传输，而非二进制**。使用HTTP作为RPC的传输层有其优势，即具有高度兼容性，但其缺点也比较明显，请求和响应中包含很多无用的头信息（ HTTP/1.1 的头部重复且庞大，每次请求都重复发送类似头部（如 User-Agent, Cookie, Accept），浪费带宽，增加延迟），传输同样大小的数据，消息大小要比基于TCP协议的消息要大，传输效率低、耗时更长。不过，后面我们讲到gRPC的时候，你会发现，**gRPC是基于HTTP协议实现的**，为什么gRPC会选择使用Http协议呢？我们留在后面讲解。

常见的 I/O 模型
* BIO (Blocking I/O，阻塞式)： 这是最朴素的模型。每个客户端连接到来，服务端就分配一个专用线程处理业务逻辑。这种方式简单，但当并发连接数暴涨时，线程数量会急剧膨胀。每个线程都需要内存（栈空间）和CPU上下文切换开销，最终导致资源耗尽、性能急剧下降。因此，它只适合连接数非常少的简单场景。
* NIO (Non-blocking I/O，非阻塞式) / 多路复用： 这是现代高性能RPC框架的主流选择，核心思想是一个或少量线程就能高效管理大量连接。它利用操作系统提供的select/poll/epoll（Linux）或kqueue（BSD）等机制，非阻塞地轮询多个连接。当某个连接有数据可读（新请求到来）或可写（响应可发送）时，I/O线程才去处理它。处理通常分两步：1) I/O线程快速读取请求数据（或写入响应数据）；2) 将解码后的业务请求分发到专门的业务线程池去执行实际逻辑。这样，I/O线程只负责高速的I/O操作，避免了被慢业务阻塞；业务线程池则专注于执行代码逻辑。大名鼎鼎的 Netty 网络库就是Java生态实现NIO模型的标杆，被 Dubbo 框架使用。
* AIO (Asynchronous I/O，异步I/O)： 它更进一步，应用程序发起I/O操作（读或写）后立刻返回，操作系统内核会在操作真正完成时再通知应用程序。理论上这是最高效的模型，完全解放了应用线程。但在实际应用中（尤其是在Linux平台），其成熟度、性能优势相对于成熟的NIO方案并不显著，且编程模型更复杂，因此在主流RPC框架中应用相对较少。

常用框架
* Dubbo：Java生态的经典高性能RPC框架。优势在于功能丰富全面（服务治理能力强）、性能优异、社区活跃、中文文档和案例丰富。最新版本对云原生支持越来越好。是许多大型互联网公司Java技术栈的首选。
* Thrift：Thrift 由 Facebook 开发并贡献给 Apache，其核心设计哲学是“接口定义先行” 和强大的跨语言能力。Thrift 要求你先用其专属的接口定义语言 (IDL) 来严格描述你的服务接口（方法、参数、返回值）和数据结构。这个 .thrift 文件就是服务提供方和调用方之间的契约。“一次定义，到处生成” ，写好 IDL 文件后，使用 Thrift 的编译器 (thrift -gen) 可以自动生成多种目标语言（如 Java, C++, Python, Go, PHP, JavaScript 等）的客户端和服务端骨架代码，由此实现跨语言 RPC 调用。
* gRPC 是 Google 主导的高性能、开源、通用的 RPC 框架，它建立在两个强大的技术基础之上：HTTP/2 和 Protocol Buffers (Protobuf)。Protobuf类似Thrift，是跨语言的序列化协议，同样是基于IDL来实现。我们重点看下HTTP/2。前面我们讲到，HTTP协议相对于TCP协议，存在性能问题，因此一般RPC框架不会选择使用HTTP协议作为通信协议，但是，HTTP/2是对HTTP协议的重大革新，它使用二进制替代文本格式、以及多路复用和头部压缩等特性，解决了 HTTP/1.1 的诸多性能问题。基于HTTP/2协议传输数据不逊于基于TCP传输，而且还具有高度的兼容性（只要支持HTTP协议的客户端都可以调用其接口），因此，gRPC选择使用HTTP/2作为其网络通信协议。

## 服务发现
不知道你是否还记得，我们前面讲到过负载均衡，在多个后端服务实例的前面，部署一个负载均衡器（比如，Nginx、HaProxy、LVS、F5等），这样服务调用者（也叫做客户端）只需要记录负载局衡器的地址，无须记录后端服务实例的地址，这样服务实例的上下线等动态变更是由负载局衡器维护的，对于服务调用者来说是透明的。你看，这里的负载均衡器也可以起到服务注册中心的作用。

不过，对于微服务架构来说，服务注册和发现功能一般是集成在RPC框架中，而非使用已经存在的现成的负载均衡器来实现，主要原因还是考虑到性能。我们前面讲过，对于某些负载均衡器（比如F5、LVS），经过配置之后，可以支持后端服务的响应直接返回给客户端，而不经过负载均衡器，但是，为了精确地分流流量，起到负载均衡的作用，所有的请求都要经过负载均衡器。经过负载均衡器的转发，多了一步网络通信，显然会影响性能，对于追求高性能的RPC调用来说，显然可以有更优的选择。因此，一般RPC框架会自己实现服务的注册和发现功能，让服务注册中心只起到管理服务地址的作用，客户端和服务方之间的RPC通信直接进行，不经过服务注册中心。这样没有流量的转发，性能肯定更高一些！

为了实现服务的注册和发现，我们要有一个地方集中存储服务的地址信息（各个实例的地址列表）。前面我们讲到很多存储系统，比如Redis、MySQL、其他NoSQL数据库，当然，使用它们也没有太大问题（比如，Redis因为其完善的高可用架构Redis Cluster和消息通知pub/sub功能，其实也经常用作注册中心），但是，RPC框架更倾向于选择专用组件：Zookeeper、Nacos、Etcd等，天然解决了数据的一致性、高可用性和可靠性，并且提供了友好的编程接口，编程实现简单。每种RPC框架支持的注册中心类型不同，像Dubbo RPC框架支持多种注册中心（Zookeeper、Nacos、Redis），使用Dubbo时，我们可以通过配置灵活的选择注册中心类型（一般选运维和开发最熟悉的）。

基本的服务注册和发现流程
* 当一个服务实例启动并准备好提供服务后，它会主动向注册中心（比如Nacos等）请求登记地址信息（也就是调用Nacos的接口存储数据）。当客户端要调用服务接口时，会先向注册中心查询服务的实例地址列表，获取实例地址列表之后，通过一定的负载均衡算法，选择一个服务实例进行接口调用。
* 当然，这是最基本的处理流程，其实还有很多性能问题和不完备的地方。比如，每次接口调用都要先查询注册中心获取实例地址列表，这是比较影响接口调用性能的。解决这个问题的方法也比较简单，可以在客户端加一个缓存，缓存实例地址列表。这样就避免了每次接口调用都查询注册中心。
* 当然，这样做又带来了新的问题：缓存和注册中心的数据一致性问题。也就是，当注册中心中的实例地址更新了（有实例下线了或者新的实例启动了等），怎么快速通知各个客户端的缓存进行更新？一般情况下，客户端会跟注册中心维持一种pub/sub的长连接，当注册中心有数据更新时，客户端会监听到数据更新并更新本地的缓存。

心跳检测
* 心跳检测是非常常用的可用性检测方案，比如前面讲到的负载均衡器（Nginx、LVS等）的健康检查、Netty长连接的有效性检查都是基于心跳检测。这里我们也可以使用心跳检测来实现服务实例下线或崩溃的发现。
* 服务实例在启动注册到注册中心之后，定期（心跳间隔，比如每隔5秒）就向注册中心发送一个心跳包，证明自己存活着（可用）。如果注册中心在连续一段时间内（超时时间，比如20秒）没有收到任何来自该实例的心跳包，则判定该实例下线或者崩溃，就将其从注册中心删除，基于pub/sub消息通知机制，所有监听此服务地址列表的客户端都会受到地址更新消息，并更新自己的缓存数据。
* 注意，因为网络抖动问题，心跳有可能发送失败，因此，为了避免注册中心错误地判定实例下线，超时时间一定要比心跳间隔大，比如上面举例中提到的超时时间为20秒，心跳间隔是5秒，起码要3~4次连续的心跳都没有收到，注册中心才会判定实例下线，这样就可以大大降低因为网络抖动而导致的误判。
* 前面提到注册中心有很多类型，比如Nacos、Zookeeper、Redis等，其中Nacos、Zookeeper本身就具有会话的心跳检测功能，Redis可以使用Key的过期时间（如30秒）模拟心跳，服务实例需定期续期（使用 EXPIRE 命令），否则 Key 过期后会被清理。这也是我们一般使用Nacos、Zookeeper这些专用的分布式协调系统、以及Redis实现注册中心，而非使用MySQL、其他NoSQL数据库（比如MongoDB）实现注册中心（基于它们实现心跳机制较为复杂）的其中一个原因。

容错机制
* 客户端缓存持久化：在注册中心不可用时，客户端使用最后一次有效的服务地址缓存继续提供服务。虽然此时无法感知新实例上线或实例下线，但大部分存量服务仍可用。
* 服务静态列表降级：在配置文件中预设关键服务的地址列表，当注册中心不可用时切换到静态配置，保证核心链路可用。
* 注册中心集群容灾：生产环境必须部署注册中心集群（如Nacos集群跨机房部署），即使部分节点宕机，整体仍可提供服务。

前面我们反复提到一个问题，客户端缓存服务地址和注册中心的一致性问题，也就是当注册中心检测到服务实例下线并推送更新时，客户端可能因为网络延迟或处理不及时，本地缓存中仍保留着已下线的实例地址。如果调用方此时使用这个“僵尸地址”发起请求，必然导致调用失败，这个问题该怎么解决呢？
* 本地缓存自动过期：客户端缓存的地址列表设置一个较短的本地有效期（比如30秒），即使未收到注册中心的更新通知，过期后也会主动去注册中心拉取最新列表。这样避免因推送丢失导致长期使用脏缓存。
* 调用失败快速剔除：在发起RPC调用时，如果某个实例连续调用失败（如连接超时、服务不可用），客户端会临时标记该实例为不可用，并在一段时间内（如10秒）不再向其发送请求或者降低该实例的权重减少调用（下一节在讲负载均衡中讲解）。

当然，即使地址列表是最新的，调用过程中仍可能因网络闪断、服务瞬时过载等问题导致失败。这时不能简单放弃这个服务实例，而需通过调用策略容错：
* Failover（失败自动切换）：当调用失败时，自动切换到其他实例重试。这是最常用的策略，特别适合读操作。注意：写操作需谨慎使用，避免重复提交（需服务端幂等）。
* Failfast（快速失败）：只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。
* Failsafe（失败安全）：调用失败后仅打印日志，不抛异常，适用于可降级的非核心功能（如日志上报）。
* Failback（失败自动恢复）：后台记录失败请求，定时重发，通常用于消息通知操作。
* Forking（并行调用）：同时调用多个实例，只要有一个成功就返回结果，适合对实时性要求极高的场景。
* Broadcast（广播）：调用所有服务实例，逐个调用，任意一台报错则报错 ，通常用于通知所有服务实例更新缓存等本地资源信息。

负载均衡可以分为两类：代理类负载均衡和客户端负载均衡。前面讲到的Nginx、LVS等是代理类负载均衡，而PRC框架使用的是客户端负载均衡，由RPC框架在客户端SDK中实现，客户端根据负载均衡算法，从服务实例地址列表中选择一个实例进行调用。这种设计避免了代理转发带来的性能损耗。

一般情况下，RPC框架支持多种负载均衡策略，其实我们在前面章节中讲解复杂均衡时也都提到过。
* 随机调用策略：随机选择服务器节点，该策略可以对不同服务器实例设置不同的权重，权重越大分配流量越高。
* 轮询调用策略：均匀地将请求分配到各个机器上。如果各个机器的性能不一样，容易导致性能差的机器负载过高，所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。
* 最少活跃数策略：根据服务器的运行状态去选择服务，如果某个机器性能越差，那么接收的请求越少，越不活跃，此时就会给不活跃的性能差的机器分配更少的请求。
* 一致性哈希算法：相同参数的请求一定会被分发到固定的服务器节点。当某个服务器节点挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。

## 配置中心
在单体架构时代，配置通常散落在应用的 properties、yml 或 xml 配置文件中，并且，改个数据库地址，调整个线程池大小都需要重启应用。这样做虽然比较麻烦，但因为部署的实例不多故还能忍受。然而，当进入微服务架构时代，系统拆分成数十个微服务后，这种原始的配置管理方式瞬间变成了运维的噩梦。想象一下，你需要紧急调整所有服务的超时阈值以应对突发的网络抖动，却要逐个登录服务器、修改文件、重启服务，这无异于一场大工程，而且人工操作还容易出错。配置中心在微服务架构下应运而生，成为微服务架构中不可或缺的基础系统。

常用的配置中心有很多，比较主流的有Nacos、Apollo、Spring Cloud Config。其中，Nacos是阿里开源的配置中心，同时它还具有服务注册和发现的功能（上一节课我们讲过），使用Dubbo RPC框架首选Nacos。Apollo是携程开源的配置中心。Spring Cloud Config是Spring Cloud生态的配置中心，与Spring Boot应用集成简单。

配置中心的核心价值，远不止于配置“集中存储”这么简单，在微服务架构中，它还起到了很多其他作用：
* 动态更新：修改一个配置项（如某个降级开关、日志级别、缓存过期时间），能够接近实时地推送到所有相关的服务实例，无需重启。这在故障排查、容量调整、功能灰度发布时至关重要。
* 配置隔离：当数十个项目共享同一个配置中心时，不同的项目、不同的环境（开发、测试、预发布、生产等）需要完全隔离的配置集，避免人为误操作导致的配置污染。
* 版本回滚：配置中心会像代码仓库管理源代码那样管理配置变更，记录每次修改的完整快照。通过可视化的版本对比功能，运维人员可以清晰看到不同版本间的差异，一键触发回滚操作。
* 加密存储：对于敏感信息，比如数据库密码、API密钥等，需要安全的存储（加密）、传输和访问控制机制，不能像普通配置一样明文展示。
* 权限控制：防止一个项目访问另一个项目的配置，同时，同一个项目内，不同身份的参与人员具有不同的访问权限，杜绝"实习生误删/改重要配置"这类灾难事故。
* 审计追踪： 谁在什么时候修改了什么配置？修改前后的值是什么？配置中心会提供完整的审计追踪，避免误操作或者恶意操作，方便追责与快速恢复。

配置实时更新
* 配置中心可以简单划分为客户端和服务端，服务端存储配置，客户端拉取配置并使用。那么，如何实现客户端迅速感知并获取配置变更呢？当然，**最简单的实现方法是基于消息中间件的pub/sub模式**，但是，对于业务开发者来说，使用配置中心还要引入消息中间件，使用成本是比较高的，更好的方法应该是all in one，开箱即用！其实，如果追求实现简单，我们可以采用客户端轮训的方式，对于关心的配置，间隔固定时间（比如1秒）拉取一次服务端的配置，当然，为了减少数据传输量，在拉取配置时，客户端传递上一次拉取的时间，基于此时间进行过滤，拉取在此时间之后有更新的配置。
* 不过，**基于轮训的方式，虽然实现比较简单（服务端只需要开放接口即可），但是，大量客户端频繁的轮询（建立连接、释放连接）会对服务器有较大压力**，而且，大部分情况下，配置可能都没有更新，大部分轮训都是在做无用功。你可能会说，那把轮训的时间间隔设置大一点，10秒请求一次不就行了吗？但是，这样又会带来新的问题，那就是：服务器变更了配置，但客户端有过很久（最长10秒）才能感知到配置变更。
***轮询相当于pull通信方式，既然pull不行，那么我们就改成 push**。push的通信方式需要客户端和服务端保持长连接，当有配置更新时，服务端通过长连接实时地将配置更新推送给客户端。但是，这种方式需要维护长连接（例如需要心跳检测以进行连接有效性检查），编程实现就复杂了。除此之外，大部分情况下，配置都是没有更新的，维持长连接似乎也是有点大材小用！
* 因此，**像Nacos、Apollo一般都采用了长轮询的通信方式**。客户端发起一个请求到服务端，询问“我关心的配置是否有更新？”如果此时没有更新，服务端会 hold 住这个连接不立即返回，直到发生客户端关心的配置变更（立即返回变更数据）或达到一个较长的超时时间（如 30s、60s），此时返回空响应。客户端收到响应（无论是否有数据）后，立即发起下一次长轮询请求。这种方式相比短轮询（如每 5s 请求一次）大大减少了无效请求次数，同时保证了变更通知的实时性（通常在秒级）。相比于长连接（需要基于TCP或WebSocket实现），长轮询（可以基于HTTP实现）编程实现要简单得多，即便因为网络抖动等原因，导致单次连接的断开，客户端也会主动发起下一次连接，连接的管理比长连接要简单许多，但实现的效果跟长连接相差无几（在配置不会频繁更新这个场景下）。

## API 网关
在微服务架构中，一个系统可能被拆分成很多个微服务，微服务属于内部服务，如果直接暴露接口给前端应用（web、app、小程序、IoT设备等）调用，会存在诸多问题，比如调用关系可能会比较混乱（前端应用需要接入很多后端微服务），也会遇到协议不兼容的问题（比如，前端可能不适合直接调用Dubbo接口），此时，我们就需要**在前端和微服务之间构建一个中间层，提供集中的、统一的接口给前端调用，这个中间层就是API网关**。

想象一下在一个经过微服务拆分的项目中，我们可能有十几个甚至几十个独立运行的微服务，每个微服务都暴露着自己的接口，如果前端应用直接与这些服务通信，就会存在以下一些问题：
* 前端与后端强耦合：对于前端应用的开发工程师来说，他需要知道后端业务的划分方式，每个微服务的职责分工和边界
* 通信协议难以兼容：我们知道，微服务一般采用RPC框架暴露接口，为了性能，RPC框架往往采用长连接、二进制数据格式来进行通信，并且，不同的微服务还可能采用不同的RPC框架来开发，而对于很多前端应用来说，如果去兼容适配每个微服务的通信协议，增加了前端应用的开发成本，这也是不应该的！
* 通用功能重复开发：有一些通用的非业务功能，也就是前面文章中提到的服务治理，比如用户鉴权、接口限流、降级、熔断、访问日志记录、监控等，如果每个微服务都重复开发，不仅效率低下，开发成本高，而且容易出错，更难以保证一致性！我们需要有一个地方集中放置这些功能！

API网关就是为了解决以上问题而产生的，API网关介于前端和微服务之间，为前端提供一个统一的接入点。所有的前端应用都只与API网关直接交互，API网关接收到前端的接口请求之后，经过服务治理，协议的转换，再根据路由规则，将接口请求路由给对应的微服务处理，处理之后的结果，再经由API网关返回给前端应用。其实，API网关就是一种反向代理！

API 网关都具备哪些核心供能
* 路由转发
* 负载均衡
* 配置中心：API 网关为了实现高性能、高可用，需要集群部署，跟微服务的配置相似，API 网关的配置（主要是路由配置、服务治理的各种策略）也需要集中化存储，也就是配置中心。这样配置的更新不需要重启 API 网关，并且便于统一管理
* 协议转化：微服务内部可能采用高性能但对外不友好的协议（如 gRPC、Dubbo、Thrift）。网关可以承担协议转换的重任，对外暴露标准的、易于理解的HTTP API，对内则使用最适合的协议与后端服务通信。这大大简化了客户端的集成难度，也保护了内部技术选型的灵活性。
* 服务治理：微服务主要负责业务逻辑的实现，我们可以将非业务的功能，也就是**鉴权、限流、降级、熔断、日志、监控、部分调用链追踪等服务治理工作，从微服务中剥离出来，统一放到API网关中的实现**。这样也就实现了业务功能和非业务功能的解耦合，业务的频繁更新只需要更新单个微服务，并不需要更新API网关，系统更加稳定。

### BFF 服务
一般来讲，微服务为了保证接口的通用性，往往会就暴露细粒度的接口，即一个接口包含简单单一的业务。这样，对于复杂的业务，需要调用多个细粒度的微服务接口才能完成。内网通信比较快，不同层级的微服务之间互相调用，问题不大。但是，对于前端应用来说，与后端服务之间的通信往往需要经过公网甚至是移动网络，通信成本往往很高，甚至高于业务逻辑执行的成本（时间）。如果实现一个稍复杂的业务逻辑，需要调用多个后端接口，多次往返的网络通信，势必增加前端应用的响应时间，影响用户体验。

为了解决这个问题，于是就有了Backend For Frontend（BFF），也就是为前端服务的后端。BFF是在微服务的上层，它根据特定前端的UI需求，调用下游多个微服务（如用户服务、订单服务、商品服务），获取所需数据，并将其聚合、裁剪、转换成一个符合该前端UI展示要求的数据，然后一次性返回给前端。这大大减少了前端的请求次数和数据处理复杂度。

实际上，现在很多项目的前端形态都趋于多样化（Web、iOS、Android、小程序等），每个前端应用的功能和页面可能都不尽相同，对接口的需求也不同，基于此，我们甚至可以为每个不同的前端应用构建独立的BFF，这样不管是接口性能还是前端开发的体验都会更好！

有些同学可能会说，是不是可以将API聚合的工作放到API网关中实现呢？比如，前面讲到，Spring Cloud Gateway需要集成到一个Spring Boot应用中运行，那么，我们在这个Spring Boot中就可以实现接口聚合的逻辑。这样做的好处是运维简单（不用维护两套东西：API网关和BFF），以及减少网络传输耗时，原本请求要经过BFF再到微服务，现在直接从API网关到达微服务，减少了一次网络通信。

对于一些小型项目或者初创项目，我们可以接受这种混合开发模式。但是，这样做也会带来很多其他问题，第一点，有些API网关不支持特定类型的API聚合业务逻辑的编写（比如Kong不支持在自定义Lua脚本中调用Dubbo接口，只支持HTTP接口和gRPC接口）；第二点，修改BFF业务逻辑（如新增字段等）需要重启全部的网关实例，可能要中断全部的服务；第三点，BFF和API网关柔和在一起，互相竞争系统资源，且无法针对性的做伸缩扩容。从架构清晰、解耦、演进的角度来说，将BFF和API网关独立开发和部署，长期来看是更加合理的选择。

## 认证鉴权
认证就是解决“你是谁”问题，一般是登陆之后获取访问凭证，后续通过凭证来访问服务。鉴权是解决“你能干啥”问题，一般是认证之后获取用户身份，然后根据用户身份获取用户授权的访问范围，以此来限制用户对某些服务的访问。接下来，我们会侧重讲解认证。

实际上，在微服务架构中，包含两类认证鉴权。**一类是用户跟服务之间的认证鉴权，另一类是服务跟服务之间的认证鉴权**，比如在大公司中，往往有中台的概念，中台提供众多通用业务的微服务，供各个不同业务线调用，尽管业务线对中台的调用是内部调用，相对于外部调用还算安全，但是大公司人员杂乱，为了保证中台系统的安全、可用性，以及方便做容量规划，各个业务系统在调用中台微服务也应该有认证鉴权逻辑。

接下来，我们先来看一些常用的经典的认证鉴权的实现方式，包含：Session、Token、JWT、AccessKey，然后再来看，它们如何集成到微服务架构中。

### Session
当用户登录成功后，服务器会创建一个对应的 Session 会话对象，用于存储本次会话的状态信息（包含session ID和用户ID等用户身份标识）。随后，服务器需要将此 Session 对象持久化到某个存储介质中。在早期单机部署的场景下，像 Tomcat 这类 Web 容器默认会将 Session 存储在单个实例的内存中。这种方式的弊端非常明显：一旦服务器重启，所有会话状态都会丢失；同时，若采用多实例部署，由于会话数据无法在各个实例间共享，必须依赖会话黏滞（Session Sticky）策略，才能确保同一用户的后续请求始终被路由到登录时的那个实例上。

随着分布式架构和集群部署成为主流，为了解决会话共享的问题，**常见的做法是将 Session 集中存储到外部分布式缓存中间件（例如 Memcached 或 Redis）中**。这种架构演进使得多个服务实例可以无状态地共享同一份会话数据，从而打破了会话黏滞的限制，提升了系统的可扩展性和可靠性。

### Token
基于Session这种认证方式在网站应用中非常完美，但是，这套机制的实现深度耦合了浏览器，浏览器被设计为自动管理、自动携带 Cookie。认证的处理过程严重依赖 HTTP 协议头（Set-Cookie, Cookie）。随着互联网技术的发展，前端形态逐渐多样化，除了网站之外，还有APP、小程序、桌面应用、IoT设备等等。基于Session的认证机制是一套为浏览器量身定制的、高度封装的解决方案。它简单，但被浏览器生态绑定。如果把它强行用于非浏览器环境，会非常别扭（需要手动模拟浏览器的Cookie）。不过，它的处理思想完全可以借鉴，只需要对其实现方式稍作调整，于是，就有了适合多端应用的基于Token的认证方式。

Token是一种显式的（不像Cookie，浏览器会在发送请求时自动附带）、协议无关（不绑定HTTP）的认证方式。前端（不一定是浏览器）需要多做一些事情，主动管理Token（存到本地存储、内存等），并在每次请求时主动附加。这虽然比自动带 Cookie 多了一步，但换来了巨大的灵活性和清晰的控制权。

用户调用登陆API成功登陆之后，后端会生成一个唯一Token（比如使用MD5对随机字符串和时间戳加密得到）发送给前端，并将Token与用户标识一并存储到Redis中。前端收到Token之后，需要主动存储这个Token，不同的前端存储方式不同，APP通常需要借助原生提供的安全存储机制，例如 iOS 的 Keychain 或 Android 的 Keystore，来安全地持久化Token；现代浏览器则可以存储在 localStorage、sessionStorage 中，或依旧使用Cookie；而微信小程序这类平台化应用，则需使用其框架提供的异步存储API，如 wx.setStorageSync。

前端后续调用后端的API，都会将这个值一并传递给后端，对于HTTP请求来说，Token一般放到HTTP header中来传递，格式为：Authorization: Bearer 。后端收到这个Token之后，会搜索Redis，验证这个token是否是自己派发的合法token，然后才能执行API对应的后端逻辑。

大概的逻辑已经走通了，但是，还是不够全面。

**Token没有过期时间吗？**当然不行，不安全啊，万一泄露了，可能发生重放攻击，也就是黑客拿泄露的Token去冒充用户访问后端API接口。虽然可能性不大，但是也要考虑的。所以，Token需要设置一个过期时间，一并存储在数据库中，校验Token的同时，校验过期时间，如果过期，前端就引导用户去重新登录。

但是，仍然存在问题，那就是Token的过期时间太长，又会增加安全隐患，过期时间太短，又会导致用户频繁重新登陆。这个问题该怎么解决呢？

于是，我们又增加了一个refreshToken，原来的Token改名为accessToken。**refreshToken的过期时间比较长，比如可以是7天，accessToken的过期时间比较短，比如可以是2个小时**。

当用户登陆成功之后，生成accessToken、refreshToken，并将对应的过期时间，以及uid存储到数据库中，然后将accessToken和refreshToken一并传递给前端，前端找个地方存储这两个Token。平时，前端只是用accessToken，不使用refreshToken，这样就能尽量保证refreshToken的安全性。当accessToken过期之后，前端再将refreshToken传递到后端请求新的accessToken。如果refreshToken也过期了，那么只能引导用户重新登陆了。

问题又来了，为什么将refreshToken给到前端，不给不行吗？每次accessToken过期后，后端就自动生成一个新的accessToken，不行吗？不行！如果这样的话，泄露的accessToken可以用这种方法持续的换取新的accessToken，那么，refreshToken就起不到作用了。

除此之外，为了保证refreshToken足够安全，我们可以实现refreshToken轮转机制，每次使用refreshToken获取新的accessToken后，就让此refreshToken立刻失效，并同时生成一个新的refreshToken返回给前端。这进一步增加了攻击者利用盗取refreshToken的难度。

### JWT
上述讲解的基于Token的认证方式有一个问题：Token需要存储到Redis中，后续用于比对校验以及根据Token获取用户标识。这种方式依赖Redis，每次接口调用都要访问一次Redis，对性能有所影响，而且还需要部署高可用的Redis集群，增加了运维的复杂度，并且，理论上，每引入一个组件（Redis），系统的可用性也会相应的有所下降。

那么，怎么才能去掉Redis呢？要达到这个目的，**Token需要具备两个特性：自包含和自验证。自包含的意思是，从Token中就可以解析出用户标识信息，不需要额外存储和查询。自验证的意思是，通过Token本身就能校验是否是后端合法签发的，不需要去Redis中比对**。基于这两个优化思想，于是就有了新的认证方式：JWT（JSON Web Token）认证。接下来，我们具体看下JWT的实现原理。

一个 JWT 看起来是一长串看似乱码的字符串，用两个点.分隔成三部分：Header.Payload.Signature，举例如下所示：
```shell
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ
```

分别看下这三个部分的含义
* Header：由两部分组成，type 表示令牌类型，就是 jwt，alg 表示签名算法，比如 HS256 或 RS256，这个对象会通过 BASE64 编码，形成 JWT 的第一部分
* Payload：也是一个 JSON 对象，用来存放实际需要传递的声明，他是实现自包含的关键，同样，这个 JSON 对象也会通过 Base64Url 编码形成 JWT 的第二部分。
* Signature：签名是 JWT 最精妙的部分，它是实现自验证的关键。

签名的生成方式如下：
```
Signature = Algorithm(
               base64UrlEncode(header) + "." + base64UrlEncode(payload),
               secret)
```

计算出的结果（一个二进制哈希值）再经过 Base64Url 编码，就得到了 JWT 的第三部分。

接收方收到 JWT 后，会用同样的算法和密钥（如果是对称加密）重新计算前两部分的签名，并与传来的第三部分进行比对。如果任何一部分（Header 或 Payload）被篡改，计算出的签名将完全不同，验证就会失败。

理解了结构，我们来看 JWT 是如何在系统中流转和起作用的。

用户向后端服务器提供凭证（如用户名密码）。服务器验证凭证无误后，用一个安全存储的密钥生成 JWT，然后将完整的 JWT 返回给客户端。客户端（如浏览器或手机App）收到 JWT，需要妥善存储。此后，客户端在请求需要认证的 API 时，必须在 Authorization 请求头中以 Bearer 模式携带该令牌：Authorization: Bearer <your.jwt.token>。

后端服务器接收到请求，并从请求中提取出 JWT。验证流程是完全本地化的、无状态的，这是 JWT 性能优势的关键：首先，用点.分割字符串，得到三部分。服务器使用预配置的密钥，对收到的前两部分（Header.Payload）重新计算签名，并与收到的第三部分（Signature）进行比对。如果不一致，立即拒绝请求。签名验证通过后，解码 Payload，逐一检查声明（Claims），比如当前时间是否小于过期时间（令牌是否已过期？），当前时间是否大于生效时间？签发者是否是我信任的？等。如果任何一项检查失败，则返回拒绝请求。

相对于Session认证方式，服务端不需要存储会话状态，这使得应用可以轻松地水平扩展，添加再多实例也无须担心会话同步问题。相对于Token认证方式，它避免了每次请求都查询 Redis 来校验Token，本地计算验证签名的速度极快。但JWT认证方式也存在一些问题。

**无法主动失效是 JWT 最大的问题，一个签发出去的 JWT 在自然过期之前始终有效。无法像 Session 和 Token一样在服务端直接“踢掉”一个用户（通过删除对应的session和token的方法）**。

### AccessKey
前面我们讲到了三种认证方式：Session、Token、JWT，这三种认证方式更多的是用户和服务之间的认证。前面我们也提到，在微服务架构中，还存在一种服务与服务之间的认证，这种认证怎么实现呢？**AccessKey便是解决这类认证问题的方案**，也是阿里云在普遍使用的一种服务（比如OSS等）访问的认证方式。我们具体来看下它的实现原理。

**AccessKey包含两部分：AccessKey ID 和 AccessKey Secret**。服务方给每个服务调用方均颁发一个Access Key ID 和一个 Access Key Secret。其中，AccessKey ID 用来唯一标识调用方，AccessKey Secret是用来加密生成签名的密钥，本身永远不会在网络上传输，绝对不能泄露。像阿里云建议定期更换AccessKey Secret以提高安全性。

调用方对服务方的每一次请求，都要附带签名，签名的生成算法并不复杂，利用加密算法（比如HMAC-SHA1）使用AccessKey Secret对待签名字符串进行加密，然后就得到了签名。待加密字符串通常由以下几个部分按特定顺序拼接而成，例如StringToSign = HTTPMethod + "&" + URL + "&" + Timestamp。
* HTTP 方法（如 GET 、PUT）
* 请求的路径和参数
* 时间戳（防止重放攻击，服务器会验证时间戳的有效性，例如允许 15 分钟内的偏差）
* 其他需要验证的信息

服务方收到请求之后，根据请求中的AccessKey ID找到对应的AccessKey Secret，然后用相同的加密算法和规则重新生成签名，并与调用方传递过来的签名进行对比，如果一直，表明身份合法且请求未被篡改，否则拒绝调用服务。

### 接入网关
方案一：API 网关完全处理认证和鉴权，在这种方案下，网关不仅负责路由、限流，还直接负责用户密码验证、Token 的生成与校验。处理流程如下：
1. 用户登录请求 POST /login 直接到达 API 网关
2. 网关提取请求中的用户名和米面
3. 网关直接调用用户服务来验证凭据
4. 验证通过后，网关自己生成 JWT 或 TOKEN
5. 网关将 Token 返回给客户端，并可能将其与用户信息的映射关系存入 Redis（非 JWT 的情况）
6. 对于后续请求，网关会自己验证 TOKEN 签名、过期信息，并查询 Redis 获取用户信息（非 JWT 的情况）

方案二：API 网关集成+独立认证服务器，API 网关负责认证的“接入”， 用户的身份确认（用户名和密码验证）、Token的签发、验证、管理等工作交给专业的认证服务器去做。具体流程如下：

登录认证（认证服务器职责）
* 用户向网关发送 POST /login 请求。
* 网关不做认证处理，只是根据路由规则，将请求透传给后端的认证服务器。
* 认证服务器验证用户名/密码。
* 验证通过后，认证服务器生成 Token/JWT，并将 Token 和用户信息存入 Redis（非JWT的情况），最后将 Token 返回给客户端。至此，网关没有处理任何认证逻辑。

请求鉴权（网关与认证服务器协作）
* 客户端访问业务 API GET /api/orders，在 Header 中携带 Authorization: Bearer 。
* 请求到达 API 网关。
* 网关拦截请求，提取 Token，并调用认证服务器提供的验证接口来验证 Token 的有效性。
* 认证服务器确认 Token 有效且未过期，然后将对应的用户信息（如用户ID、权限范围等）返回给网关。
* 网关将用户信息（如用户ID）注入到请求中，然后将请求转发给后端的微服务。
* 微服务收到请求，它信任网关注入的用户信息，直接使用其中的用户ID进行业务逻辑处理，无需再次验证 Token。

## 接口限流

## 降级熔断

## 链路追踪