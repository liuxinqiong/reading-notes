## 高效读写
存储系统实现高效读写的底层通用方案，包括WAL存储、刷盘策略、读写缓存、零拷贝、避免锁竞争、批处理、数据压缩等。

### WAL 存储
对于大部分存储系统来说，数据都需要持久化，也就是最终落盘（写入磁盘）。磁盘是一个慢速IO设备，随机读写，比顺序读写性能要差一大截。

为了提到数据写入磁盘的效率，其中一个有效的办法就是把随机写变成顺序写，怎么做到呢？WAL存储方式就可以实现这样的效果。WAL全称是Write-Ahead-Logging，其核心思想就是在真正修改数据之前，先把操作本身（或者操作产生的数据变更）顺序地、只追加地写入一个日志文件。 数据本身的更新（比如修改 B+ 树结构）可能很慢且复杂。WAL 先把变更“存个档”，后续可以异步、批量地应用这些变更到主数据结构上，提高了响应速度。

### 刷盘策略
数据写到内存缓冲区是很快的，但内存是易失的。最终，数据需要落到非易失的磁盘上才算真正安全。刷盘策略决定了我们何时、如何将内存中的数据同步到磁盘。这本质上是在性能（低延迟、高吞吐） 和持久性（数据不丢失）之间做权衡。数据在最终写入磁盘之前，会经过系统开辟的内存缓冲区（比如，InnoDB的Buffer Pool），操作系统内核管理的Page Cache（由操作系统决定何时刷到物理磁盘），磁盘控制器缓存（现在磁盘都有自己的缓存）。针对写入操作写入哪个缓存之后就返回，由此就产生了各种不同的刷盘策略。
* 同步刷盘：必须等待数据成功写入物理磁盘（或至少是磁盘的缓存）后，写操作才返回成功。这是最安全的策略，保证数据绝不丢失（在硬件不能损坏的前提下），但这也是最慢的，因为要等待缓慢的磁盘 I/O 完成。fsync 或 fdatasync 系统调用可以实现这一强制刷盘效果。
* 异步刷盘：数据写入到操作系统的 Page Cache 就认为写操作成功返回了。操作系统会在后台某个时间点（例如脏页达到一定比例、超时、或系统调用 sync）将 Page Cache 的数据批量刷到磁盘。性能极高，因为避免了等待磁盘 I/O，但如果在刷盘前系统崩溃（如断电），Page Cache 中的数据就会丢失。write 系统调用通常就是异步刷盘（除非设置特殊标志）。
* 折中策略：间隔固定时间（比如每秒）或者累积 N 条操作之后刷一次盘（比如调用fsync系统调用）。在安全性和性能之间取得平衡。

### 读写缓存
磁盘再快也快不过内存。缓存是提升读写效率最立竿见影的手段，其核心就是把访问最频繁的“热数据”放在更快的内存里。在存储系统中，主要可以利用两个层级的缓存：
* 应用层缓存： 存储系统自身管理的内存区域，比如InnoDB的Buffer Pool。
* Page Cache：操作系统内核管理的通用文件缓存。

### 零拷贝
传统的文件传输（如从磁盘读文件发网络）过程繁琐：
1. 磁盘文件读到内核缓冲区 (Page Cache)。
2. 内核缓冲区数据拷贝到用户缓冲区（应用进程内存）。
3. 用户缓冲区数据拷贝到内核的 Socket 缓冲区。
4. Socket 缓冲区数据通过网卡发出去。

实现零拷贝的技术主要有：
* mmap + write： 使用内存映射 (mmap)，让应用程序的用户空间地址直接指向内核的 Page Cache，这样应用程序读取文件数据就像访问内存一样，省去了“内核缓冲区->用户缓冲区”的拷贝。当调用 write时，数据会从 Page Cache 直接拷贝到 Socket 缓冲区。
* sendfile： 系统调用直接在内核中完成数据从文件描述符（对应 Page Cache）到 Socket 描述符的传输。完全避免了数据进入用户空间。

### 锁竞争
高并发下，锁是保证数据一致性的必要手段，但锁竞争会严重降低系统吞吐量，增加延迟。高效的系统都在努力减少锁的范围、降低锁的粒度、缩短锁的持有时间，甚至在某些路径上避免锁。

常用减少锁竞争的策略：
* 无锁数据结构：CAS (Compare-And-Swap) 等原子操作实现的无锁队列、计数器等。
* 读写分离：读操作通常不修改数据，可以并发执行。使用读写锁 (ReadWriteLock) 允许多个读并发，只在写时互斥。
* 分区/分片：将数据分成多个独立区间（Shard/Partition/Queue），每个区间有自己的锁。这样并发请求只要落在不同区间，就能并行处理，互不干扰。
* 单线程模型+非阻塞 I/O：一个线程处理所有任务（包括网络 I/O 和命令执行），通过非阻塞 I/O 多路复用 (如 epoll, kqueue) 处理海量连接，彻底避免了线程上下文切换和锁竞争，适用于 CPU 非瓶颈的场景。

### 批处理
网络 I/O、磁盘 I/O、系统调用都是有开销的。批处理的核心思想就是将多个小的操作请求打包成一个大的请求进行处理。
* 大幅减少网络往返次数 (RTT)：对于网络通信，一次发送 N 条消息比发送 N 次单条消息快得多。
* 减少磁盘 I/O 次数：操作系统和磁盘对顺序的大块写入更友好。批量写入可以将多次小 I/O 合并成少量大 I/O。
* 摊薄固定开销：每次操作（如建立网络连接、系统调用、磁盘寻道/旋转延迟）都有固定开销，批量处理可以摊薄这些开销。

### 数据压缩
压缩就是用 CPU 时间换取网络带宽和磁盘 I/O 带宽，一般来讲，压缩耗费的CPU时间，要少于压缩后传输或者存储所节省的时间，因此，收益是正的。
* 减少网络传输量：对于分布式系统，尤其是跨机房传输，压缩能显著降低网络带宽消耗和传输延迟。
* 减少磁盘 I/O 量：压缩后的数据写入磁盘更快，读取时需要传输的数据量也更少（虽然需要解压）。
* 节省存储空间：这是显而易见的。

## 索引优化
为了实现高效查询，基本上每个存储都会有自己的索引结构，当然，查询的方式不同，对应的索引结构也有所不同。常见的索引结构有：B+树、哈希表、顺序表、LSM树等。这节课我们就结合前面讲到的存储系统（MySQL、Redis、Kafka / RocketMQ、 NoSQL DB），讲一讲这些索引的基本原理、特性和应用。

### B+树
B+树的关键特性
* 多叉层级少：它不像二叉树那样“瘦高”，B+树每个节点能存很多“键”和指向下一级节点的指针，这使得树的高度通常很低（3-4层就能存海量数据），查询数据时磁盘 I/O 次数少。
* 性能稳定：所有数据记录都存储在最底层的叶子节点里。非叶子节点只存“键”和指针。这保证了查询任何一条记录，路径长度都一样（性能稳定）。
* 叶子节点有序：所有叶子节点用指针按顺序连成一个有序双向链表，对于范围查询（WHERE age BETWEEN 20 AND 30）和排序查询（ORDER BY）非常友好，顺着链表读取就行，效率极高。
* 平衡性：插入删除数据时，树会通过分裂、合并节点自动保持平衡，保证查询效率稳定。

我们知道，MySQL需要支持点查（按照键精确查找某个记录）、范围查和排序，并且，支持高性能写入、修改操作，B+树非常完美的支持了这些功能需求，因此，MySQL的索引结构使用B+树来实现。当然，它也有问题，就是数据是随机写磁盘，相对于顺序写，磁盘写入性能不高，这也是为什么会先redo log（顺序写，上一节讲到了）再异步构建索引和存储数据的原因。

### 哈希表
哈希表虽然不支持B+树那么多丰富的查询（点查、范围查、排序查），但是，它结构简单且点查效率非常高（时间复杂度是O(1) ），特别适合只支持简单查询的Key-Value数据库，比如Redis。Redis使用一个全局的大的哈希表来给所有的数据构建索引。当然，为了满足更多的查询功能，Redis给Value建立了二级索引，不同类型的Value（Set、SortedSet、HSet等）使用不同的索引结构。比如，SortedSet就支持按照value中某个属性的值范围来查询。

### 顺序表
对于一个有序的数组，我们可以使用二分查找加速查询（查询时间复杂度是O（logn），对数级是非常高效的，具体可以看《数据结构与算法之美》）。实际上，Kafka和RocketMQ就是使用这种顺序表结构来存储索引的，方便消费者可能从任意位置（偏移量 Offset）读取消息。

### LSM树
LSM树（Log-Structured Merge-Tree）专为极致「写入」吞吐量而生，是 NoSQL 数据库（如 LevelDB, RocksDB, Cassandra, HBase）的核心存储引擎，在写入密集型场景（如时序数据、日志、物联网）中应用极为广泛。

传统 B+ 树等结构在更新数据时，需要先找到数据所在的磁盘位置（随机读），然后原地修改（随机写）。而磁盘的随机 I/O 性能（尤其是机械硬盘）远低于顺序 I/O 性能（几个数量级的差距）。既然磁盘顺序写这么快，那为什么不让所有的写操作都变成顺序追加呢？

当然可以，我们可以将存储数据改为存储操作（有点类似WAL），记录一个数据的所有操作：写入、更新、删除。比如写入a的值为5，我们把这个操作（a=5）在文件中顺序记录下来，后面又更新了a的值为7，我们把新版本的数据（a=7）也记录下来，而不是去修改老的记录。最后我们又执行了删除操作，我们把删除操作（DEL a）也记录下来。这样所有的操作都是顺序写入文件的，写入性能非常高！当然，查询性能就降低了，当我们要查询a的值时，我们需要将所有的对a的操作都查询出来，然后综合出一个最新值。

## 主从复制
主从复制有两个核心概念，一个是主节点，一个是从节点。主节点，也称为Master或者Leader，处理所有的写操作。从节点，也成为Slave、Follower、Replica，主节的写操作会复制到从节点进行重放，最终从节点跟主节点具有一致的数据。从节点可以只负责数据冗余，也可以负责读操作，视具体需求来定。主从复制架构可以是一主一从，也可以是一主多从。主从的复制模式有多种：异步、半同步、同步。它们最核心的区别就在于主节点在响应客户端写请求时，需要等待多少个从节点确认收到数据，这直接决定了数据一致性的强度、系统的性能和可用性。
* 异步复制：这是最常见、性能最好的模式，但也是数据可靠性要求最“宽松”的一种。主节点处理完客户端的写操作（在本地提交事务并写入Binlog/WAL），立刻就响应客户端“写入成功”。它不会等待任何从节点确认是否已经接收并应用了这个变更。复制数据的传输和应用是在后台异步进行的。优点非常明显：写操作的响应延迟最低，主节点的吞吐量最高，因为写操作不会被慢速的从节点拖累。缺点也很突出：如果主节点在变更成功发送到从节点之前就突然宕机了，即使客户端已经收到了成功响应，这个已“提交”的变更也可能会永久丢失（因为还没来得及复制到从节点上），导致数据不一致。
* 半同步复制：这种模式是异步和同步之间的一种折中，试图在性能和一定的数据安全性之间取得平衡。主节点处理完写操作并写入Binlog/WAL后，不会立刻响应客户端。它会等待至少一个从节点（可以配置）确认已经成功接收到了这个变更事件（注意：通常只是确认接收并写入从节点的Relay Log，不一定是完全应用成功），然后主节点才会给客户端返回“写入成功”的响应。相比异步复制，它显著降低了主节点故障时数据丢失的风险（只要有一个从节点确认收到了变更，这个数据大概率就能保住）。缺点是，写操作的延迟会明显增加，因为它需要等待网络往返和从节点的ACK确认。
* 同步复制：这是数据可靠性要求最高的模式。主节点处理完写操作后，必须等待所有配置的从节点（或者一个法定数量的从节点）都确认不仅收到了变更，而且已经成功在本地应用（提交）了这个变更，之后才会给客户端返回“写入成功”的响应。优点是提供了最强的一致性保证。只要主节点返回成功，数据肯定在所有参与的节点上都持久化了，主节点宕机不会导致已确认的数据丢失。缺点是，写操作的延迟和性能代价是三种模式中最高的，因为要等待所有配置的从节点完成操作；任何一个从节点响应慢或故障，都会导致整个写操作被阻塞，严重影响主节点的可用性和吞吐量。

主从复制的应用场景
* 高可用与灾难恢复：当主节点不幸宕机时，可以（通常是手动或者借助工具自动）将一个从节点提升为新的主节点。这样服务就能比较快的恢复，大大缩短了不可用时间
* 读写分离与读扩展：大部分系统是读多写少的，主从复制实现读写分离，主节点负责写操作，从节点负责读操作，对于读多写少的场景，尤其是哪些不要求绝对实时最新数据的场景，通过部署多个从节点，就能把大量的读请求分摊开，显著降低主节点的负担，提升整个系统的读能力
* 降低主节点负载：承接上一点
* 数据备份：在主从复制架构中，从节点本身就是一个准实时的热备份（主节点不停机备份）。虽然它通常不能完全替代定期的全量备份，但它提供了一个非常近的恢复点，用于恢复误操作或者快速重建节点非常方便。而且，我们可以直接在从节点上做备份操作，完全不影响主节点的性能。
* 地理分布与就近访问：我们可以把从节点部署在离用户更近的地方（比如不同地域的机房）。用户查询数据时，可以直接访问当地的从节点，大大降低了网络延迟，提升了用户体验。这在全球化服务中尤其重要，但也需要注意延迟导致的数据一致性问题。

技术挑战
* 复制延迟：在设计业务逻辑时，必须考虑这种延迟的存在。对实时性要求极高的读操作，可能还得去主节点读。或者通过监控告警系统，时刻关注主从数据复制的延迟时间，正常为毫秒级别，一旦达到秒级别就立刻告警，并介入处理。
* 主节点单点写瓶颈：虽然读可以扩展了，但所有的写操作还是压在单一主节点上。如果写请求量巨大，主节点依然可能成为性能瓶颈，这是就需要更高级的技术，比如分库分表，把数据分散到多个独立的主节点上去，每个主节点负责自己那一部分数据的读写
* 故障切换的复杂性：如果主节点挂了，我们可以手动选择一个从节点为主节点，并重新配置主从复制关系，快速恢复服务，但是，这需要监控系统敏锐的发现主节点宕机，并且运维人员随时待命及时处理，否则，服务的不可用时间就会加长。其实，为了更快速地响应故障，现在多数数据存储系统，比如MySQL、Redis、RocketMQ，都支持自动故障转移。自动故障转移底层依赖共识算法，需要多机部署（至少3台），部署更加复杂。
* 数据的不一致性：在主从架构下，尤其是在有复制延迟的情况下，我们通常只能保证数据的最终一致性。即如果没有新的写操作发生，经过一段时间后，所有从节点最终都会和主节点数据一致。但在延迟窗口期内，不一致是客观存在的。强一致性（所有节点瞬间一致）在主从模式下很难实现，且代价高昂。

## 故障转移
为了更快速的响应故障，现在多数数据存储系统，比如 MySQL、 Redis、RocketMQ 都支持故障转移，自动故障转移的价值在于：它让系统在硬件故障、网络抖动、软件异常时，能像人体免疫系统一样自我修复，保障业务持续运行。

真正落地自动故障转移，对于有状态的存储系统来说（相对于无状态的服务），并不是一件容易的事情，其中的技术挑战有这样一些：
* 数据不一致风险：在多数主从架构中，为了保证性能，一般都会采用异步或者半同步的方式，主节点写入成功即返回，从节点可能还未复制数据或者还未完全落盘。若此时主节点宕机且数据丢失，即使切换到从节点，这部分未同步的数据也会永久丢失。
* 脑裂问题风险：如何精准地判断节点故障？如果从节点因为网络分区误认为主节点宕机，然后推举自己成为新的主节点，或者多个从节点之间存在网络分区，两个分区选举出各自的主节点，这样就出现了两个“大脑”，这种脑裂问题该如何解决。
* 平衡不可用和频繁选主：如果把故障检测时间设得太短（比如1秒），网络稍微抖动就可能触发选主。想象一下，系统在1分钟内反复切换主节点，客户端不断收到读写错误（Leader选举的过程写会失败或者阻塞，读有可能也不可用），这种“癫痫式切换” 很致命。但如果把检测时间设得太长（比如30秒），主节点真宕机时，业务要忍受半分钟不可用。
* 客户端无缝重连：当有新的leader产生的时候，客户端如何快速的感应到？除此之外，因为网络分区问题，旧主节点可能还活着（错误的被认为宕机了），客户端仍向其发请求。这些写入怎么处理？怎么判断哪个Leader才是真正的新的Leader？

从上面的这些挑战，你应该也已经发现，自动故障转移不仅仅涉及Leader的选举问题，还有一个更重要更复杂的问题那就是数据的一致性保证，不要因为网络分区、故障转移，导致数据出现丢失或者重复，以及不一致。其实，这些问题在共识算法中统统都有解决。

现在更多使用 Raft 共识算法。

## 数据分片
系统刚上线时，一台 MySQL 服务器，配个 Redis 单实例，再加个 RocketMQ 单节点，可能轻轻松松就跑起来了。用户量小、数据少，单机撑得住，再不行顶多就再加个主从复制、读写分离，把读的压力分担给从库，主库专心写，对于大多数读多写少的场景，基本上也就够用了。

但是，业务一旦跑起来，性能压力随之而来，用户表过了亿，缓存飙到几百 GB，消息队列每天堆积上亿条消息。这时候你会发现，单台机器再强，也有它的物理极限：CPU 会满、内存会爆、磁盘 IO 会堵死。靠升级硬件（垂直扩展）不仅成本飙升，效果也越来越差。

这种情况下，“水平扩展”就成了必然选择。无状态的计算服务水平扩展相对容易，但对存储系统来说，则要复杂得多。 其核心技术就是数据分片（Sharding），即把数据拆开，分散放到多台普通的、相对廉价的服务器上。

不同存储系统的功能特性和应用场景不同，对应实现分片的方式也各不相同。这节课我们就拿前面反复提到的、常用的 **MySQL、Redis、RocketMQ** 为例，讲解一下存储系统的水平扩展方式：数据分片。

分片策略
* 按范围分片：比如按用户 ID 范围、按时间分片
* 按哈希分片：为了解决分布不均的问题，哈希分片更常用，缺点是范围查询变得困难，同时一旦确定了分片总数 N，后续想要增加分片就比较麻烦，因为 N 变了随之 hash(key)%N 的结果变了，需要大规模迁移数据
* 按业务分片：可以理解成有一个“总控中心”（分片映射表），记录着每个分片键（或者分片键的某个范围 / 哈希值）对应到哪个具体的Shard。应用在读写数据前，先查一下这个映射表：“哦，用户ID=123456 的数据在 Shard5 上”，然后再去操作 Shard5。这种方式非常灵活，分片规则可以很复杂（比如结合业务属性），扩容时调整映射关系相对容易（修改映射表就行）。但问题也很明显，这个“总控中心”本身成了单点和性能瓶颈。它必须非常可靠（不能挂），而且每次操作都要查它，在高并发下压力巨大。通常我们会把这个映射表缓存起来，或者做成分布式服务。

技术挑战
* 跨分片查询：上面提到了，像范围分片还好，哈希分片做范围查询就得查所有分片再聚合，这非常耗时耗资源。对于需要排序、分页、聚合统计（SUM, COUNT, AVG）的操作，难度和开销都成倍增加。很多时候，我们需要在应用层做大量额外的合并处理工作。解决方案一般是适当冗余数据，建立宽表（把原本需要通过 JOIN 关联查询才能获取到的、业务上经常一起访问的、分散在多张表里的字段，预先合并（冗余）到一张表里），或使用离线查询引擎（比如ElasticSearch）来应对这些复杂查询需求。
* 跨分片事务：在单库上保证 ACID 特性的事务，到了分片环境就变得极其复杂。一个事务可能涉及更新多个分片上的数据。要保证所有分片要么都成功，要么都失败（原子性），需要引入像两阶段提交（2PC）这样的协议。而 2PC 性能差、容易阻塞，还可能存在协调者单点故障问题。很多分布式数据库或者业务系统，会尽量避免跨分片事务，或采用最终一致性方案。
* 分片键选择：分片键选得不好，前面说的热点问题、查询效率低、事务复杂等问题会放大。理想的分片键应该能让数据均匀分布（避免热点），同时又能满足核心业务的高频查询（比如经常按用户ID查，那用户ID做分片键，点查就很快）。
* 数据库扩容：业务在增长，数据在增加，最初的分片数量有可能就不够了，需要加机器、加分片。在哈希分片下，增加分片意味着哈希取模的分母 N 变了，大部分数据的归属分片都会变，这就需要进行大规模的数据迁移。这个过程要保证线上服务不停机、数据不丢失，是个技术活。提前分配足够的分片或者使用一致性哈希算法，可以避免或减少扩容带来的大量数据迁移。
* 全局唯一 ID：在单库里，用自增主键很简单。分片后，多个分片都在生成ID，如何保证全局唯一且趋势递增（或者至少不冲突）？这需要专门的分布式 ID 生成方案（雪花算法 Snowflake、Leaf 等）。
* 运维复杂度：单机或主从架构，需要管理的机器是有限的，当数据分片之后，我们可能需要管理几十、甚至上百个分片，每个分片可能还有自己的副本。监控、备份、恢复、升级、故障排查的难度和成本都大大增加。

单机 MySQL 扛不住时，**读写分离（主从复制）是第一道防线**。但这只解决了“读”的压力，“写”的瓶颈和单机存储上限还在。真到了数据量千万级、亿级，或者并发写入高得吓人的时候，就得做水平分片了，即把一张大表的行，按某种规则分散存到多个数据库实例里。

MySQL 本身没自带分布式基因，并且支持JOIN等复杂的SQL查询，相对于天然支持分片的Redis（Redis Cluster）和RocketMQ（RocketMQ Queue），分片带来的技术挑战更大，因此，我们MySQL的数据分片放到单独的下一篇文章中详细讲解。

Redis 3.0 推出了 Redis Cluster，它是Redis 官方内置的分布式解决方案，相对于早期客户端手动挡分片，这算是自动挡分片。

消息队列要应对海量消息堆积和高并发写入，同样离不开分片。在 RocketMQ 和 Kafka 这类分布式消息系统中，“分片”的概念通常被称为 分区（Partition） 或 队列（Queue）。我们拿 RocketMQ 举例讲解。Producer 发消息时，默认按照轮询策略（Round-Robin）依次将消息发到该 Topic 下的所有队列，尽可能均匀分布。当然，你也可以在Producer Client中自定义MessageQueueSelector，比如根据消息的某个业务属性（如订单ID）哈希取模，让同一订单的消息严格按顺序进入同一个队列。我们在创建 Topic 时，指定它在哪些 Broker上创建队列，以及每个 Broker 上创建几个队列。这些队列就是该 Topic 的分片，分布在不同的 Broker 上以分摊写入和存储压力。

## 分库分表
由于 MySQL 原生并不支持分片，并且需要 JOIN 等复杂的查询操作，因此分片面临很多技术挑战。

分库分表的核心目标就是解决单一数据库实例在数据量和并发访问压力上的瓶颈。

我们先来看分库。**分库就是指将不同的表放到不同的数据库实例中**，甚至是部署到不同的物理服务器上。这样可以解决单个数据库实例的性能瓶颈（连接数、并发、IO、内存、CPU等）。当然，对于微服务架构，为了隔离数据，有时候也会将不同微服务下的表放到独立的数据库中。其实，分库很简单，只涉及部署架构的区别，不是讲解的重点。

我们重点来看分表。分表的核心思想就是：将一个逻辑上的大表，按照某种规则（路由策略），拆分成多个小表。这些小表可以存储在同一数据库的不同位置，也可以分布在不同数据库实例甚至不同数据库服务器上（分库分表结合）。其实，分表和分库的终极目的是一致的，只不过针对的优化对象不同而已，一个是整个数据库表，一个是单独的大表或者热表（访问量比较大且性能有瓶颈的）。

为什么要进行分库分表
* 数据量爆炸：单表数据突破千万甚至上亿级别。MySQL单表在数据量极大时（尤其是复杂查询、索引维护等），性能会显著下降。
* I/O瓶颈：所有读写都集中在一个物理文件上，磁盘IO成为瓶颈。
* 锁竞争加剧：高并发写入时，行锁竞争激烈，导致写入延迟。
* 维护困难：备份恢复、ALTER TABLE操作耗时极长，风险极高。
* 资源限制：单台数据库服务器的CPU、内存、磁盘容量终究有限。

用表中的哪个键作为分表的键是非常重要的，它决定了数据如何分布在不同的表中，以及之后的查询的便利性。

一般的选择原则是
* 查询高频：该字段应该出现在你最重要的、最高频的查询条件中（WHERE子句）。例如，user_id、order_id、tenant_id（多租户）、region_code（地域）。
* 数据均匀：该字段的值应该尽可能保证数据能均匀地分布到各个分表中，避免数据倾斜（某个分表数据量巨大，其他很小）。像status（只有几个状态值）这类字段做分片键通常会导致严重倾斜。
* 业务相关性：通常选择核心业务实体ID（用户ID、订单ID）。

常见的分表用的键有：用户ID、订单ID、店铺ID、租户ID、地理位置ID等业务键。当然，有时候我们也会使用时间来分表，比如create_time，这种情况一般比较特殊，比如老的数据可以归档，否则，就会导致数据严重倾斜。

分表的路由策略指的是如何决定哪些数据存储到哪个表中。对于某些有自然分区特性并且取值有限的业务分表键，比如地理位置、租户ID、订单类型、创建时间，我们可以直接用他们的取值来划分表。

对于其他分表键，我们可以基于哈希的方法来路由。对分表键（或者分表键计算出来的哈希值）跟分表总数进行取模，来决定一个数据存储在哪个表中。也就是：分表序号 = hash(sharding_key) % 分表总数

ShardingSphere是开源的分库分表中间件，使用非常广泛，也比较简单。实际上，如果深入了解底层原理并想锻炼自己的编程能力，我们也可以自己去实现分表的逻辑。因为，分表的挑战往往不在于分表本身，分表的代码实现很简单，技术挑战在于选择合适的分表键、如何应对分表之后的复杂查询、以及分表之后的分布式事务等问题。

如果手动自己实现，方法有很多，最简单的方法是使用MyBatis的Provider封装SQL，在Provider中根据分表键、分表路由算法，计算目标表名，然后拼接SQL。当然，为了让分表逻辑不侵入到各个Mapper中，我们也可以将这部分逻辑放到MyBatis的Interceptor中实现。

但是，如果我们要实现复杂的查询或者统计，比如找出TOP10数据，可能需要联合多个表查询，这样也自己去实现的话，就比较麻烦，而且过度侵入业务代码，维护成本也比较高。因此，使用ShardingSphere中间件还是很有必要的。在应用和数据库之间引入一个中间层。应用像操作单库单表一样写SQL，中间件负责拦截SQL，根据配置的分片规则解析、改写、路由到正确的物理分表执行，并合并结果返回给应用。

分表后带来的其他挑战
* 分布式主键生成：单表自增ID在分表环境下会重复。这个时候就要用到我们后面要讲到的分布式ID生成算法了，比如雪花算法、UUID、基于号段模式的发号算法、美团的Leaf等等。
* 跨分表查询：ORDER BY ... LIMIT、GROUP BY、SUM/COUNT/AVG、JOIN（涉及多个分表）等操作变得复杂低效。
* 分布式事务：如果一个业务需要操作多个分表（在不同的库里），如何保证事务一致性呢？分布式事务是一大类问题，解决方案很多，比如2PC&3PC、SAGA、TCC、消息事务、本地消息等等。
* 扩容和数据迁移：如果分表初期的容量规划不合理或者业务增长，需要增加分表时，就会涉及到数据的迁移。

跨分表查询常用的解决方案有：
* 中间件支持： hardingSphere等中间件能部分处理（如ORDER BY ... LIMIT会先在各分片排序取Top N，再在内存合并排序取最终Top N）。SUM/COUNT能合并结果。但对复杂JOIN支持有限或性能较差。
* 冗余/宽表：在业务允许的情况下，适当冗余数据或建立宽表（所谓宽表就是把原本需要通过 JOIN 关联查询才能获取到的、业务上经常一起访问的、分散在多张表里的字段，预先合并（冗余）到一张表里）来避免跨分片JOIN。
* 离线查询：对于无法避免的跨分片查询（如管理后台），明确告知性能代价，或者走独立的离线分析库（如数仓、ElasticSearch）。
* 分页：深度分页（LIMIT 100000, 10）性能极差，尽量使用WHERE id > last_id LIMIT 10（基于游标的分页）代替LIMIT offset, size，并要求带上分片键。

刚刚我们讲了分表之后带来的各种技术挑战，所以，只有单表数据量特别大才会考虑分表或者提前分表问题。那么，具体来讲，如果一个表的行数在未来3年（或者更长，比如5年，看自己项目的需求）内有可能达到500万 ~ 2000万行，甚至以上，就要考虑提前分表。具体每张表最大允许存储多少行的数据，这个最终还实打实的看性能，在当前业务数据操作的情况下的性能表现。

怎么预估未来N年的某个业务表的数据总量呢? 我们给出了一个计算公式.
```code
未来N年总数据量 = 当前行数 + (日均增量 × 365 × N)
```

根据预估的未来N年的总数据量，通过以下公式计算分表数，单表容量上限：建议控制在 500万~2000万行或磁盘50GB以内。
```code
分表数 = 未来N年总数据量 / 单表容量上限
```

一般来说，我们会选择大于分表需求值的最小2的N次方值为最终的分表数，因此，大于10的最小2的N次方是16，最终，订单表分16张表。为什么要这么做呢？因为这样做路由计算效率最高。

基于哈希的路由算法，有取模操作，而CPU执行取模指令需要 20-40 个时钟周期，远高于加法/位运算。
```java
// Java取模运算
int tableIndex = userId.hashCode() % tableCount;
```

如果表数tableCount是2的N次方，那么，我们就可以把取模操作转换为位运算，如下所示，仅需1个时钟周期，性能提升 20倍以上。其实，这个计算技巧在《Java编程之美》中讲HashMap的哈希算法的时候已经提到过。
```java
// 位运算替代取模 (tableCount=16)
int tableIndex = userId.hashCode() & (tableCount-1);
```

当数据量增加，确实需要扩容，增加更多的表时，我们一般会将表数量扩容为原来的两倍。为什么这么做？其实在《Java编程之美》的HashMap动态扩容中也讲到了。

这么做的好处是：最小化迁移数据量。具体来讲，tableCount增加之后，只需要迁移一半的数据。
```
假设原分表数 N=8（二进制 1000），扩容后 M=16（二进制 10000）
原分片号 = hash(user_id) & 0x00000007  // 取低3位 (0111)
新分片号 = hash(user_id) & 0x0000000F  // 取低4位 (1111)
```

如果原数据从右往左数的第4位是0，则保持在原表不动，如果是1，则迁移到新表中。而原数据中第4位是0的，跟第4位是1的，按照概率应该是各占一半。因此，只有一半的数据需要迁移。

当然，为了避免数据大量迁移，我们还可以使用一致性哈希算法，或者在分表初期直接创建1024张表，一次性给够，这样就不会有后续的扩容需求了。当然，这也会增加表管理的成本。

对于MySQL来说，单机数据库在数据量、读写性能和并发连接方面存在天然的不足，面对互联网海量场景极易触及性能极限。此时，**我们应优先采用读写分离、Redis缓存等更轻量级的优化方案，而分库分表应该是作为应对巨量数据的最终手段**。